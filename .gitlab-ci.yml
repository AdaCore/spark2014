# In this CI, we should only work in reaction to a Merge Request
workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: always

variables:
   # The common part of the URL for cloning from within a CI
   GIT_CLONE_BASE: https://gitlab-ci-token:${CI_JOB_TOKEN}@${CI_SERVER_HOST}:${CI_SERVER_PORT}

   PACKAGE_BASE_NAME: spark2014.tar.gz
   PACKAGE_ABSOLUTE_NAME: $CI_PROJECT_DIR/$PACKAGE_BASE_NAME

stages:
  - build
  - test

###############
# Common bits #
###############

.basic-setup: &setup_repos
    # If the package exists, move it to /tmp so as not to influence "anod vcs"
    - if [ -f $PACKAGE_ABSOLUTE_NAME ] ; then mv $PACKAGE_ABSOLUTE_NAME /tmp ; fi

    # Temporary: clone the specific branch of ci-fragments
    # TODO: remove the next 4 lines when this version of ci-fragments is available in the image
    - cd /tmp
    - git clone $GIT_CLONE_BASE/eng/it/ci-fragments -b topic/generic_ci_phase_2 --depth 1
    - export PATH=/tmp/ci-fragments:$PATH
    - cd -
    # Use generic_anod_ci here.
    - generic_anod_ci $GENERIC_ANOD_CI_OPTIONS
    - cat /tmp/ci_env.sh
    - . /tmp/ci_env.sh

    # Tune to use our build & test plan
    - anod tune --plan $CI_PROJECT_DIR/plans/ci.plan

    # Go to the sandbox dir
    - cd $ANOD_DEFAULT_SANDBOX_DIR

.deploy_package_and_touch_fingerprints: &deploy_package
    # Unpack the package
    - tar zxf /tmp/$PACKAGE_BASE_NAME -C /

    # Tell anod that the package has already been built
    - mkdir -p fingerprints
    - touch fingerprints/x86_64-linux.spark2014-cov.build.json.assume-unchanged
    - touch fingerprints/x86_64-linux.spark2014-cov.install.json.assume-unchanged
    # Attempt to avoid downloading all the deps of the spark2014-cov build
    - touch fingerprints/x86_64-linux.spark2014-doc.build.json.assume-unchanged
    - touch fingerprints/x86_64-linux.spark2014-doc.install.json.assume-unchanged
    - touch fingerprints/x86_64-linux.spark2014-core-cov.build.json.assume-unchanged
    - touch fingerprints/x86_64-linux.spark2014-core-cov.install.json.assume-unchanged
    - for pack in colibri why3 stable-gnat cvc4 cvc5 alt-ergo gnsa libgpr-current gnatcoll-core-current xmlada-current ; do
        touch fingerprints/x86_64-linux.$pack.download_bin.json.assume-unchanged;
        touch fingerprints/x86_64-linux.$pack.install.json.assume-unchanged;
      done ;


.spark2014_test:
  services:
     - image:e3
     - cpu:8
     - mem:16
  stage: test
  script:
    # Move the package out of the way, so it does not influence "anod vcs"
    - mv $PACKAGE_ABSOLUTE_NAME /tmp

    # Setup the "anod vcs as appropriate"
    - *setup_repos

    # Do not rebuild spark2014-doc
    - anod install spark2014-doc --latest

    - *deploy_package

    # set caching location
    - mkdir -p $CI_PROJECT_DIR/gnatprove_cache
    - export GNATPROVE_CACHE="file:$CI_PROJECT_DIR/gnatprove_cache"
    # Test using anod
    - anod run $ANOD_ENTRY_POINT

    # Process the results
    - e3-testsuite-report
       --failure-exit-code 1
       --xunit-output $CI_PROJECT_DIR/xunit_output.xml
       x86_64-linux/$ANOD_BUILDSPACE/results/new/ || FAILED=true
    - cp -r $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE/results/new/ $CI_PROJECT_DIR/testsuite-results
    - cp -r $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE/src/dhtml-report/ $CI_PROJECT_DIR/coverage

    - if [ ! -z ${FAILED+x} ]; then echo "There was at least one testcase failure" && exit 1; fi

  artifacts:
     when: always
     paths:
        - xunit_output.xml
        - testsuite-results
        - coverage/cobertura/cobertura.xml
     reports:
       junit: xunit_output.xml
       coverage_report:
          coverage_format: cobertura
          path: coverage/cobertura/cobertura.xml
  cache:
    - key: alwaysthesame
      paths:
        - gnatprove_cache

#########
# Build #
#########

build:
  services:
     - image:e3
     - cpu:8
     - mem:16
  stage: build
  script:
    - GENERIC_ANOD_CI_OPTIONS="--add-dep eng/spark/sparklib
                               --add-dep eng/spark/spark-internal-testsuite
                               --add-dep eng/spark/why3"
    - *setup_repos

    # Build using anod
    - anod run build

    # Create the package
    - SB_WITHOUT_LEADING_SLASH=`echo $ANOD_DEFAULT_SANDBOX_DIR | cut -b2-`
    - tar czf $PACKAGE_ABSOLUTE_NAME -C /
        $SB_WITHOUT_LEADING_SLASH/x86_64-linux/spark2014-cov/install

  artifacts:
    paths:
      - $PACKAGE_BASE_NAME

########
# Test #
########

spark2014:
  extends: .spark2014_test
  when: always
  variables:
    ANOD_ENTRY_POINT: test
    ANOD_BUILDSPACE: spark2014-test-cov-cache

spark2014_large:
  extends: .spark2014_test
  when: manual
  variables:
    ANOD_ENTRY_POINT: test_large
    ANOD_BUILDSPACE: spark2014-test-cov-large-cache

#################
# Test of ACATS #
#################

acats:
  services:
     - image:e3
     - cpu:8
     - mem:16
  stage: test
  script:
    # Setup the sanbox
    # Add acats explicitly for this build.
    - GENERIC_ANOD_CI_OPTIONS="--add-dep eng/toolchain/acats"
    - *setup_repos

    # Deploy the installed package
    - *deploy_package

    # Test using anod
    - anod run test_acats

    # Process the results
    - e3-testsuite-report
       --failure-exit-code 1
       --xunit-output $CI_PROJECT_DIR/xunit_output.xml
       x86_64-linux/acats-4-gnatprove-baseline-test/results/new/ || FAILED=true

    - if [ ! -z ${FAILED+x} ]; then echo "There was at least one testcase failure" && exit 1; fi

  artifacts:
     paths:
        - xunit_output.xml
     reports:
       junit: xunit_output.xml

################
# Build of Doc #
################

build_docs:
  stage: build
  services:
     - image:e3
  rules:
    - changes:
      - docs/**/*
      when: always
  artifacts:
    when:
      always
    paths:
      - spark/pdf/spark2014_rm.pdf
      - spark/pdf/spark2014_ug.pdf
      - spark/html/lrm
      - spark/html/ug
  script:
    # Setup the "anod vcs as appropriate"
    - *setup_repos

    # Build using anod
    - anod build spark2014-doc
    - cp -r $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/spark2014-doc/install/share/doc/spark $CI_PROJECT_DIR
