include:
  - project: eng/codepeer/ci-registry
    file: /gnatsas-on-spark.yml
  - component: $CI_SERVER_FQDN/eng/gitlab-templates/check-issue@~latest
    inputs:
      stage: check
  - component: $CI_SERVER_FQDN/eng/gitlab-templates/pre-commit@~latest
    inputs:
      stage: check

# In this CI, pipelines are triggered when pushing to a Merge Request and when
# merging a MR to a protected branch (only for the gnatsas job).
workflow:
  rules:
    # 1. Skip the pipeline if the MR has the 'skip-ci' label
    - if: $CI_MERGE_REQUEST_LABELS =~ /skip-ci/
      when: never
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" || ($CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_REF_PROTECTED == "true") || $CI_PIPELINE_SOURCE == "schedule"
      when: always
    # Run on pipelines created by selecting "Run pipeline" in the GitLab UI,
    # from the projectâ€™s "Build > Pipelines" section.
    - if: $CI_PIPELINE_SOURCE == "web"
    - when: never

variables:
   PACKAGE_BASE_NAME: spark2014.tar.gz
   PACKAGE_ABSOLUTE_NAME: $CI_PROJECT_DIR/$PACKAGE_BASE_NAME
   PACKAGE_EXPORT: /tmp/spark2014-export
   ANOD_BUILD_QUALIFIERS: assertions,coverage

stages:
  - build
  - test
  - check

###############
# Common bits #
###############

.basic-setup: &setup_repos
    # If the package exists, move it to /tmp so as not to influence "anod vcs"
    - if [ -f $PACKAGE_ABSOLUTE_NAME ] ; then mv $PACKAGE_ABSOLUTE_NAME /tmp ; fi

    # Use generic_anod_ci here.
    - generic_anod_ci $GENERIC_ANOD_CI_OPTIONS
    - cat /tmp/ci_env.sh
    - . /tmp/ci_env.sh

    # Tune to use our build & test plan
    - anod tune --plan $CI_PROJECT_DIR/plans/ci.plan

    # Go to the sandbox dir
    - cd $ANOD_DEFAULT_SANDBOX_DIR

    # We must parse testsuite results with the same version of e3-testsuite
    # that produced them. So we install e3-testsuite using the same sandbox as
    # testing.
    - anod install e3-for-testing
    - eval $(anod printenv e3-for-testing)

.deploy_package_and_touch_fingerprints: &deploy_package
    # Deploying the package is a two-step process. First we unpack the tarball,
    # by restoring its location at packing time in $PACKAGE_EXPORT.
    # Then we copy the components into the sandbox via anod-copy-components.
    - tar zxf /tmp/$PACKAGE_BASE_NAME -C /
    - anod-copy-components $PACKAGE_EXPORT $ANOD_DEFAULT_SANDBOX_DIR

.disk_monitor:
  before_script:
    - df -hT --exclude-type=tmpfs --exclude-type=devtmpfs
  after_script:
    - df -hT --exclude-type=tmpfs --exclude-type=devtmpfs


.spark2014_test:
  extends: .disk_monitor
  services:
     - image:pe-base
     - cpu:16
     - disk:60
  stage: test
  interruptible: true
  script:
    # script to load helper functions for log sections
    - source scripts/ci_predef.sh

    # add internal testsuite for which we want to use most recent
    # sources for testing
    - GENERIC_ANOD_CI_OPTIONS="--add-dep eng/spark/spark-internal-testsuite"

    # Setup the "anod vcs as appropriate"
    - *setup_repos

    # remove gnat from vcs if present for testing; this is allowed to fail when
    # gnat is not present
    - anod vcs --remove gnat || true

    - *deploy_package

    # place cache in proper location; avoid project dir to avoid slowdown of source packaging
    - mkdir -p $CI_PROJECT_DIR/gnatprove_cache
    - mv $CI_PROJECT_DIR/gnatprove_cache /tmp
    - export GNATPROVE_CACHE="file:/tmp/gnatprove_cache"
    # set location of sources for coverage
    - ANOD_BUILDSPACE_SOURCES=`anod eval spark2014 build_space_name --primitive build --qualifier=coverage,assertions`
    - export COVERAGE_ROOT_DIR=$ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE_SOURCES/src
    - export COVERAGE_SOURCE_DIR=$CI_PROJECT_DIR
    # Test using anod
    - anod run $ANOD_ENTRY_POINT

    # Process the results
    - ANOD_BUILDSPACE=`anod eval spark2014 build_space_name --primitive test --qualifier=$ANOD_QUALIFIERS`
    - testsuite_reports || testsuite_exit_code=$?
    - mv $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE/results/new/ $CI_PROJECT_DIR/testsuite-results
    - mv $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE/src/cobertura-report/ $CI_PROJECT_DIR/coverage

    # Process test file to generate timing test (skip for large tests)
    - |
      if [[ "$ANOD_ENTRY_POINT" != "test_large" ]]; then
        (cd $CI_PROJECT_DIR ; python scripts/check_test_times.py --threshold=180 xunit-*.xml xunit-times.xml) || timing_exit_code=$?
      fi

    # Reestablish cache
    - mv /tmp/gnatprove_cache $CI_PROJECT_DIR/gnatprove_cache

    # Coverage HTML report
    - section_start "coverage_report" "Generate Coverage HTML report"
    - export HTMLTARGETDIR=$CI_COMMIT_BRANCH
    - if [[ $CI_PIPELINE_SOURCE == "merge_request_event" ]]; then export HTMLTARGETDIR=MRs/$CI_MERGE_REQUEST_IID ; fi
    - if [[ $CI_PIPELINE_SOURCE == "schedule" ]]; then export HTMLTARGETDIR=weekly ; fi
    - publish-pages $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/$ANOD_BUILDSPACE/src/html-report --target-subdir $HTMLTARGETDIR --expires 30
    - section_end "coverage_report"

    # Determine final exit code based on testsuite and timing results:
    # - If testsuite_reports failed: fail with its exit code
    # - If check_test_times.py failed with code other than 0 or 1: fail with its exit code
    # - If check_test_times.py returned 1 (timing threshold exceeded): exit with code 89
    #   exit code 89 is special-cased in allow_failure to indicate a warning
    # - Otherwise: success (exit code 0)
    - |
      if [[ -n "$testsuite_exit_code" && "$testsuite_exit_code" != "0" ]]; then
        # testsuite_reports failed, fail the job
        exit $testsuite_exit_code
      elif [[ -n "$timing_exit_code" && "$timing_exit_code" != "0" && "$timing_exit_code" != "1" ]]; then
        # check_test_times.py failed with exit code other than 0 or 1, fail the job
        exit $timing_exit_code
      elif [[ -n "$timing_exit_code" && "$timing_exit_code" == "1" ]]; then
        # check_test_times.py returned 1 (timing threshold exceeded), use special exit code for warning
        exit 89
      fi

  artifacts:
     when: always
     paths:
        - xunit-*.xml
        - testsuite-results
        - coverage/cobertura.xml
     reports:
       junit: xunit-*.xml
       coverage_report:
          coverage_format: cobertura
          path: coverage/cobertura.xml
  cache:
    - key: alwaysthesame
      paths:
        - gnatprove_cache

#########
# Build #
#########

build:
  extends: .disk_monitor
  services:
     - image:pe-base
     - cpu:8
     - disk:60
  stage: build
  interruptible: true
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" || $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - when: never
  script:
    - GENERIC_ANOD_CI_OPTIONS="--add-dep eng/spark/sparklib
                               --add-dep eng/spark/spark-internal-testsuite
                               --add-dep eng/toolchain/gnat
                               --add-dep eng/spark/why3"
    - if [[ $CI_PIPELINE_SOURCE == "schedule" ]]; then GENERIC_ANOD_CI_OPTIONS="$GENERIC_ANOD_CI_OPTIONS --continuous-builder-mode"; fi
    - *setup_repos
    - export ANOD_ENABLE_DOC=false

    # Build using anod
    - anod run build

    # Create the package
    - SB_WITHOUT_LEADING_SLASH=`echo $ANOD_DEFAULT_SANDBOX_DIR | cut -b2-`
    - anod-copy-components --standalone-export $ANOD_DEFAULT_SANDBOX_DIR $PACKAGE_EXPORT
    - tar czf $PACKAGE_ABSOLUTE_NAME -C/ $PACKAGE_EXPORT

  artifacts:
    paths:
      - $PACKAGE_BASE_NAME

########
# Test #
########

spark2014:
  extends: .spark2014_test
  rules:
    - if: $CI_MERGE_REQUEST_LABELS =~ /no-cache/
      when: never
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: on_success
    - when: never
  allow_failure:
    exit_codes: [89]
  variables:
    ANOD_ENTRY_POINT: test
    ANOD_QUALIFIERS: $ANOD_BUILD_QUALIFIERS,cleanup-mode=none,cache

spark2014_large:
  extends: .spark2014_test
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: manual
    - when: never
  allow_failure: true
  variables:
    ANOD_ENTRY_POINT: test_large
    ANOD_QUALIFIERS: $ANOD_BUILD_QUALIFIERS,only_large,cleanup-mode=none,cache

spark2014_nocache:
  extends: .spark2014_test
  rules:
    - if: $CI_MERGE_REQUEST_LABELS =~ /no-cache/
      when: on_success
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: manual
      allow_failure: true
    - when: never
  cache: []
  variables:
    ANOD_ENTRY_POINT: test_nocache
    ANOD_QUALIFIERS: $ANOD_BUILD_QUALIFIERS,cleanup-mode=none

###############################
# Tests of ACATS and SPARKlib #
###############################

.other_test:
  extends: .disk_monitor
  services:
     - image:pe-base
     - cpu:16
  stage: test
  interruptible: true
  script:
    - *setup_repos
    - *deploy_package
    - anod run $ANOD_TARGET
    - testsuite_reports
  variables:
    ANOD_QUALIFIERS: $ANOD_BUILD_QUALIFIERS

  artifacts:
     paths:
        - xunit-*.xml
        - __results*
     reports:
       junit: xunit-*.xml

acats:
  extends: .other_test
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: always
    - when: never
  variables:
    GENERIC_ANOD_CI_OPTIONS: "--add-dep eng/toolchain/acats"
    ANOD_TARGET: test_acats

sparklib:
  extends: .other_test
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: always
    - when: never
  variables:
    GENERIC_ANOD_CI_OPTIONS: "--add-dep eng/spark/sparklib"
    ANOD_TARGET: test_sparklib
    ANOD_QUALIFIERS: $ANOD_BUILD_QUALIFIERS,large

################
# Build of Doc #
################

build_docs:
  extends: .disk_monitor
  stage: build
  interruptible: true
  services:
     - image:pe-base
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      changes:
        - docs/**/*
      when: always
    - when: never
  artifacts:
    when:
      always
    paths:
      - spark/pdf/spark2014_rm.pdf
      - spark/pdf/spark2014_ug.pdf
      - spark/html/lrm
      - spark/html/ug
  script:
    # Setup the "anod vcs as appropriate"
    - *setup_repos

    # Build using anod
    - anod build spark2014-doc
    - cp -r $ANOD_DEFAULT_SANDBOX_DIR/x86_64-linux/spark2014-doc/install/share/doc/spark $CI_PROJECT_DIR
    - export HTMLTARGETDIR=$CI_COMMIT_BRANCH
    - if [[ $CI_PIPELINE_SOURCE = "merge_request_event" ]]; then export HTMLTARGETDIR=MRs/$CI_MERGE_REQUEST_IID ; fi
    - publish-pages $CI_PROJECT_DIR/spark/html/lrm --target-subdir doc/lrm/$HTMLTARGETDIR --expires 30
    - publish-pages $CI_PROJECT_DIR/spark/html/ug --target-subdir doc/ug/$HTMLTARGETDIR --expires 30

pre-commit:
  interruptible: true
  services:
     - image:pe-base
     #Memory is bound to CPU and we need more than the 1Gig that 1 cpu provides
     - cpu:4
  before_script:
    - GENERIC_ANOD_CI_OPTIONS="--add-dep eng/toolchain/gnat"
    - generic_anod_ci $GENERIC_ANOD_CI_OPTIONS
    - cat /tmp/ci_env.sh
    - . /tmp/ci_env.sh
    - cd $ANOD_DEFAULT_SANDBOX_DIR
    - SPECPATH=$(anod tune 2>&1 | awk '/Path:/ {p=$NF} /Specs:/ {if ($NF == "anod)") print p "/vcs/anod-auto"; else print $NF}')
    - echo $SPECPATH
    - GNATFORMAT_DATE=`sed -n 's/.*gnatformat_version = "\(.*\)".*/\1/p' $SPECPATH/sparkdev.anod`
    - echo $GNATFORMAT_DATE
    - anod install gnat --latest
    - anod install libgpr2 --latest
    - anod install vss-text -Qedge --latest
    - anod install vss-extra -Qedge --latest
    - anod install gnatformat -Qdate=$GNATFORMAT_DATE
    - anod printenv gnat > /tmp/precommit_env.sh
    - anod printenv libgpr2 >> /tmp/precommit_env.sh
    - anod printenv vss-text -Qedge >> /tmp/precommit_env.sh
    - anod printenv vss-extra -Qedge >> /tmp/precommit_env.sh
    - anod printenv gnatformat -Qdate=$GNATFORMAT_DATE >> /tmp/precommit_env.sh
    - cat /tmp/precommit_env.sh
    - . /tmp/precommit_env.sh
    - gnatformat --version
    - mkdir /tmp/sarif-ada
    - e3-cathod source sarif-ada-src --setup $ANOD_SETUP --latest --install /tmp/sarif-ada
    - export GPR_PROJECT_PATH=/tmp/sarif-ada:$GPR_PROJECT_PATH
    - cd $CI_PROJECT_DIR
    - ln -s $ACI_SUBPROJECTS_ROOT/gnat gnat2why/gnat_src
    - make -C gnat2why setup
