Meeting at June 19th, 9h30

Tool architecture
=================

* Ed: missing on feature list: component libraries
  Rod: do we want to prove the containers themselves, at least absence of RTE?
  Yannick: maybe, but not now
* Trevor: there is a problem with information flow with the larger subset of Ada
  Yannick: just improve the algorithm
  Trevor: do we want to change things right now, while preparing a new
     product? Also, subsets are a good thing
  Yannick: it's the right time!
  Arno: Subsets are a separate feature, the core technology should support
     all of it
  Rod: info flow algo does not deal with declarative part at all today;
     this must change
* Trevor: users must be educated to deal with new features
  Yannick: browsing info flow in IDE will be perfect for that

* what about loop stability, ineffective statement?
   - general agreement is that they are nice to have, but not high
     priority
   - what does it actually catch compared to simple-minded warnings of
     the GNAT compiler? Many cases probably covered by that -> should be
     tested
   - CodePeer is probably much better than SPARK here

* Rod summary: static analysis for a certain subset of Ada, and the
  rest + conditional/array-precise flow should be done by VC
  Yannick: yes, "the rest" amounts to partial initialization of arrays
  Yannick: user can trigger the use of VCs for info flow by using
     trivial conditions:
         Derives => (Y => (if True then X))
     if there is some subtle reason that Y only derives from Y

* Rod: Barnes book and CarrÃ© paper do not contain all details about
  the info flow paper, lots of details (volatile variables etc)

* Rod/Arno: having the conventional info/data flow for the first release
  would already be good
  Yannick explains how conditional info/data flow can work using VC
  generation
  Trevor: we also need automatically generated info/data flow, for the
  generative approach
  Rod: in the future, we want to track record fields
  Yannick: this is necessary to fully track modes of variables ("out"
  for records)
  Trevor: complication with abstraction ("own") variables
  Rod/Trevor: abstraction mechanisms for state are essential to do any
  proof, otherwise explosion of complexity of VCs
  Yannick: maybe a new proof system has a different set of limitations,
  let's not anticipate too much

  Summary:
    * enhanced info/data flow not on the critical path for the
      first release, conventional one is required
    * good IDE support is important (browsing, slicing)
    * componentwise analysis for later

* proof
  features recognized as important:
   * absence of RTE
   * properties + partial correctness
   * OOP + LSP
   * OOP requires indefinite containers
   * interaction with the tools, IDE support
   * fine-grain granularity of (re-)verification
   * state abstraction
   * counter examples, detailed explanations, path

  later, but don't wait too long:
   * termination
   * subtype predicates, type invariants
   * loop invariant generation

   wait and see:
   * Model based stuff
   * metrics
   * concurrency
   * controlled types
   * exceptions

   out of scope (for now):
   * timing analysis
   * stack usage, memory usage -> gnatstack
   * dead code detection -> symbolic execution, KSU tools

* Yannick: Praxis folks, please play with gnatprove

Existing users, Legacy support
==============================

Legacy users:
   * using SPARK for a long time now
   * some of them have code that does not move any more
   * some constantly write new code

possible approaches:
   1 one time translator required
      - for our own test cases
      - for clients, manual adjustments possibly needed
      - very rough transition
      - what about comments, hide clauses?
   1' complete, reliable translator could be used as transparent tool
      - is the best solution, but by far the most difficult
      - smooth transition
   2 read both syntaxes in the GNAT frontend
      - a lot of work to support SPARK annotations
   3 generate SPARK 2014 binding from --# in specs
      - old code stays in SPARK
      - new code in SPARK 2014 can use the specs of the SPARK code
      - old SPARK code cannot call SPARK 2014 code
      - we would ship old and new tools
      - some driver tool could automatically choose appropriate toolset
      - not enough for testsuite translation
   5 no transition path
      - much of the money is with new customers (?)
      - upset existing customers?
      - forget about the testsuite?
      - we need some official transition path (marketing), even if it
        doesn't work very well

* are the SPARK Pro customers actually going to go the transition path?

general agreement that solution (3) is best for the product

* But what about the existing SPARK testsuite?
   - Trevor: testsuite very valuable
   - agreement that most part of the testsuite should be translated
   - Arno: corner cases of old technology might not be as useful for new
           technology
   - solution (1) - one time translator with manual aid
   - TQL4 qualification through tracability of the testsuite?
   - presentation of AdaCore (GNAT and Hi-Lite) testing strategy and
     Praxis (SPARK) testing strategy
   - want to use the QM (Qualifying Machine)
   - would need to write down requirements (one page with 20 high-level
     requirements vs 100 pages of low-level requirements)
   - separate test sets for development work and tool qualification
   - AI Stuart: write a verification/qualification/testing plan
   - tool quality document for customers

Tool design
============

* Yannick draws architecture of gnatprove
* Trevor: report.log file useful; this would contain:
      * versions of all tools
      * command line options
      * results of run
* problem of discovery mode vs enforcing mode, and
  generative vs constructive approach, different SPARK constraints
* have a small (3-4) number of "profiles" such as discovery mode,
  enforcing globals mode, enforcing Alfa mode
* in addition, allow the user to choose coding standard rules, e.g.
  forbid multiple exits, recursion, ...
  this can be implemented using gnatcheck
* provide two  basic sets of restrictions: full SPARK 2014, and
  something closer to today's SPARK
* some restrictions need to be checked by gnat2why
* Trevor: restriction only for the spec needed
   use case: information flow specified on public subprograms,
   discovered for internal subprograms

Additions to existing architecture - and work to be done
--------------------------------------------------------

1 planning documents (development, verification, qualification)
   + testing strategy (AP)
1' SPARK 2014 language design (PA)
2 support for SPARK 2014 aspects in the GNAT frontend (gnat2why) (Ap)
3 binding generator for SPARK specs (P)
   - --# specs to SPARK 2014 aspects
   - based on the examiner frontend
4 internal training (AP)

free order of the following tasks:

5 "profile" mechanism in gnat2why (effects computation/checking, Alfa
  filter/checking) (Pa)
6 information flow generating/checking in gnat2why after the alfa filter
  (Pa)
7 support for OOP + discriminant records in Alfa (gnat2why Why3 backend)
  (Ap)
8 container library (Ap)
9 browsing/slicing of information flow (support in gnatprove/gnat2why
  and IDE) (A)
10 counterexamples support (Riposte or Alt-Ergo?) (AP)
11 translation testsuite (P)

probably more towards the end:

12: training material, documentation, marketing material

A - Adacore
P - Praxis

capital letter: leading the effort
lowercase letter: supporting the effort

Schedule
--------

AdaCore: aim at end of 2013 to have a product, or almost

SPARK 11 will be released (target: october 2012) with support of generic packages


Schedule for the features of SPARK 2014
---------------------------------------

1 + 1': 1st version sept. 2012
4: november 2012
2: dec 2012
3: dec 2012

everything else is in 2013

external communication not before 2013

assigning names to tasks
------------------------

1': Trevor + Steve
1: Rod + Arno
2: Yannick + Steve
3: Trevor + ?

Additional things discussed at the last minute
----------------------------------------------

* scalibility of gnatprove:
   - to be tested on large code
   - implementation of fast WP
   - implementation of cut points

* backup plan:
   - Why to FDL to use simplifier, riposte



























