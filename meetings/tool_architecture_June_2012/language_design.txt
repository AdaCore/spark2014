Rod: support for existing SPARK language
Johannes: this is already scheduled for the tool architecture meeting

mathematical vs executable semantics
------------------------------------

Rod: customers have the option in SPARK of compiling without checks, because
  all proofs have been done statically
Robert: we have many customers that want belt-and-braces
  we have customers that don't do proofs and nonetheless remove checks at
  runtime
Angela: vote for 6
Trevor: many SPARK contracts not executable
Robert: impact on compilation is good.
  Ada language does not require overflow checks.
  certifiable bignum library may not be so difficult.
  it may be interesting to have a mode that detects intermediate computations
  that may overflow base type.
possible different cases when choosing 6:
  default: extended machine integers
  switch: switch back to current behavior
  switch: use bignum
Rod: Rolls-Royce worries about overflows and performance
Robert: compiler already has most of the code to do overflow check
        (-gnato), that could be used for extended machine integers
Robert: what about abstraction (function retuning an unbounded integer)?
Yannick: we decided that the user can use bignum
Steve: absence of referential transparency with solution 6 (code is using
  smaller machine integers than assertions)
Robert: what is the largest machine integers?
Arno: answer is 64 bits
Rod: we have customers doing all combination of modes between amount of proof
  (Robert: also for amount of testing) and keeping assertions at runtime
Johannes: non-executable semantics still needed (it's a different problem)
   Arno: it's not even a problem, just don't execute it!
   Johannes: but need to tell the tools which ones are supposed to be executed
Rod: do we have a smooth path for newcomers to SPARK?
  Current SPARK semantics is consistent with all Ada compilers?
  Customers like Secunet have a path for high-level assertions?
  We have something to cover this ground of users.
Angela: current solution 6 is much better than original 3 in Hi-Lite.

AI: bignum library with certification in mind
    Robert opens a TN for the compiler
AI: Yannick write summary and write tasks for people
AI: Robert makes -gnato the default

Conclusion:
   * everybody agreed on solution 6, summarized as:
        default: executable semantics with extended machine integers
           when needed
        switch: switch back to current behavior
        switch: use bignum when needed
        switch to disable overflow checks

potential impact of aspects on no-dead-code requirements of DO-178B/C
---------------------------------------------------------------------
(vs. annotations)

Trevor: what about contracts/functions that are not executed because the user
  is compiling without assertions enabled?
Arno: This is deactivated code, fine with DO-178C.
Trevor: what about code for test cases?
Yannick: This is not code, not taken into account.

General conclusion is that this does not pose any problem.

validation process for new constructs
-------------------------------------

Yannick: Current process of discussing/agreeing in TN before giving the green
  light for implementation.
  Need something slightly more formal for SPARK 2014, to have someone from
  AdaCore and Praxis give the green light.
Rod: Need light process with core team between AdaCore and Praxis to agree.
  Praxis has used both live discussions for smaller issues + design documents
  for bigger issues.
Trevor: what about language design issues?
Yannick: discuss on TN, then propose in SPARK 2014 design document.
Arno: At AdaCore, we use comments to document design (like tool architecture
  for SPARK 2014)
Rod: Propose a small group of language/technology leaders.
Yannick: proposed process is
  - discuss in TN on spark2014-discuss@lists.open-do.org
  - when the discussion concerns a language feature, put the result in
    the LRM doc under SPARK 2014 git repo
  - when the discussion concerns the design of an analysis/implementation:
     reach agreement, only then start implementing in the tools, put
     documentation directly in the code

maintaining the ability to perform modular/constructive analysis
----------------------------------------------------------------

Trevor: Many features in SPARK allow modular verification.
  Hi-Lite works on generic instances, with choices made by the compiler.
  There might be problems recovering information after the compilation frontend
  pass.
Arno: We control the compiler frontend, we can do whatever we want. Hi-Lite
  expansion is already tailored for formal verification instead of execution.
Trevor: SPARK today is top-down, do analysis before writing any code,
        can we do that with SPARK 2014?
we all agree that SPARK 2014 as discussed in Bath is ready for that use case
minor pitfall: files must be compilable while SPARK can analyse
  uncompilable files using hide clauses -> need for stub bodies etc

verifying instances of generics or generics themselves
------------------------------------------------------

Johannes: Hi-Lite verifies generic instances.
  Current Examiner verifies generic themselves
  We could support both.
Trevor: one point is that we cannot analyze code without implementating
  generic bodies.
Rod: I am going to work on verification of generic packages soon.
Arno: Do we want to do that? (to be discussed later)

Summary: We should do both. General consensus was that we should first
provide proof of the instances, because it's so easy and does not put
restrictions on the code; provide proof of the generic later

need for mathematical types? (real/unbounded arrays/etc?)
---------------------------------------------------------

Yannick: Do we want specification for real? for logical sets/maps, etc like in
  JML? This is a request from Patrice Chalin.
Rod: In the current SPARK tools, contracts talks about floating-point but
  tools understand real arithmetic.
Rod: customers interested in absence of runtime errors + precision
Yannick: What about a way to specify the ideal mathematical counterpart of
  a function returning a floating-point or fixed-point?
Ed: We don't want to have to implement all numerical research in SPARK 2014.
Rod: We don't know any SPARK user using fixed-point.
Yannick: We could support floating-point in our toolchain.
Angela: Research interested in real specifications.
Rod: Worth thinking more before extending the language.

AI: Praxis to contact Rolls-Royce to discuss interest in real specifications.
AI: Yannick to follow integration of Gappa in Alt-Ergo to experiment in our
  toolset.

Yannick: What is the need for abstract containers?
Rod: We have the formal containers that we can use.
Johannes: Yes, but we don't have infinite containers then.
Yannick: And we must then add ghost variables and computations to have a value
  for these executable containers. Not the same as the model containers that
  Patrice is interested in.
Rod: For now, we will provide logic types/functions/axioms for user to
  experiment.

AI: Rod must look at the formal container library.

Summary: we will provide support for proofs about floating point
(possibly using the Alt-Ergo + Gappa integration), proof about numerical
precision "at some later point", if at all. There was agreement that
there is no need for built-in mathematical data structures, and that
logic types/functions/axioms are good enough for now.

need for more complex data refinement?
--------------------------------------

Trevor: If we can define the abstract view, we don't have a good way to
  refine that down.
Yannick: This is purely on the logical side.
Rod: specify the retrieve relation for the correspondence.

AI: Trevor to complete the language design doc with actual proposal.
