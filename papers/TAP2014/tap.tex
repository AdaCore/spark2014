\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{zed-csp}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\DO}{\textsc{do-178}\xspace}
\newcommand{\DOB}{\textsc{do-178b}\xspace}
\newcommand{\DOC}{\textsc{do-178c}\xspace}
\newcommand{\hilite}{Hi-Lite\xspace}
\newcommand{\gnatprove}{GNATprove\xspace}
\newcommand{\oldspark}{SPARK~2005\xspace}
\newcommand{\newspark}{SPARK~2014\xspace}
\newcommand{\spark}{SPARK\xspace}
\newcommand{\ada}{Ada\xspace}
\newcommand{\adatwtw}{Ada~2012\xspace}
\newcommand{\vectorcast}{VectorCast\xspace}
\newcommand{\projectx}{Project~X\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\ie}{\textit{i.e.,}\xspace}
\newcommand{\adhoc}{\textit{ad hoc}\xspace}
\newcommand{\Eg}{\textit{E.g.,}\xspace}
\newcommand{\eg}{\textit{e.g.,}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\aka}{a.k.a.\xspace}
\newcommand{\resp}{resp.\xspace}

%TODO paragraphs about recursion, termination?

\lstdefinestyle{tinystyle} {basicstyle=\scriptsize\tt,
  keywordstyle=\bf, commentstyle=\rmfamily\it, escapeinside={(*}{*)}}
\lstset{style=tinystyle, numberbychapter=false, captionpos=b}

\lstdefinelanguage{SPARK}{ language = [95]Ada, morekeywords = {pre,
    post, assert, assume, check, derives, hide, global, inherit, from,
    own, initializes, main_program, input, output, in_out,
    refined_pre, refined_post, some, depends}, comment=[l][commentstyle]{--\ },
  showstringspaces=false }

\lstset{language=SPARK}
\sloppy

\pagenumbering{arabic}
\pagestyle{plain}

\begin{document}
\title{Explicit Assumptions - A Prenup for Marrying Static and Dynamic Program Verification}

\author{Johannes Kanig\inst{2} \and Rod Chapman\inst{1} \and Cyrille Comar\inst{2} \and Jer√¥me Guitton\inst{2} \and \\ Yannick Moy\inst{2} \and Emyr Rees\inst{1}}
\institute{Altran UK, 22 St Lawrence Street, Bath BA1 1AN (United Kingdom)\\
          \texttt{\{rod.chapman, emyr.rees\}@altran.com}
            \and
           AdaCore, 46 rue d'Amsterdam, F-75009 Paris (France)\\
          \texttt{\{comar, guitton, kanig, moy\}@adacore.com}
}

\maketitle

\begin{abstract}
Formal modular verification of software is based on assume-guarantee
reasoning, where each software module is shown to provide some guarantees
under certain assumptions and an overall argument linking results for
individual modules justifies the correctness of the approach.
However, formal verification is almost never applied to the entire code,
posing a potential soundness risk if some assumptions are not verified. In
this paper, we show how this problem was addressed in an industrial
project using the \spark formal verification technology, developed at Altran UK.
Based on this and similar experiences, we propose a partial
automation of this process, using the notion of explicit assumptions.
This partial automation may have the role of an enabler for formal
verification, allowing the application of the technology to isolated modules
of a code base while simultaneously controlling the risk of invalid
assumptions. We demonstrate a possible application of this
concept for the fine-grain integration of formal verification and testing
of Ada programs.
\end{abstract}

\paragraph{Keywords}
Formal methods, Program Verification, Test and Proof, Assumptions
\section{Introduction}

Formal modular verification of software is based on assume-guarantee
reasoning, where each software module is shown to provide some guarantees
under certain assumptions, and an overall argument linking results for
individual modules justifies the correctness of the approach. Typically, the
assumptions for the analysis of one module are part of the guarantees which
are provided by the analysis of other modules. The framework for
assume-guarantee reasoning should be carefully designed to avoid possible
unsoundness in this circular justification. For software, a prevalent
framework for assume-guarantee reasoning is Hoare logic, where
subprograms\footnote{In this paper, we use the term
{\em subprogram} to designate procedures and functions, and reserve the more
common term of {\em function} to subprograms with a return value.} are taken
as the software modules, and subprogram contracts (precondition and
postcondition) define the assumptions and guarantees. Formal verification
tools based on Hoare logic analyze a subprogram without looking at the
implementation of other subprograms, but only at their contract.

Although verification is done modularly, it is seldom the case that the
results of verification are also presented modularly. It is tempting to only
show which components have been verified (the guarantees), omitting the
assumptions on which these results depend. This is indeed what many tools
do, including the \spark tools co-developed by Altran UK and AdaCore.  In
theory, the correctness of the approach would depend, among other things, on
formal verification being applied to all parts of the software, which is never
the case for industrial projects. Even when considered desirable to maximize
formal verification, there are various reasons for not applying
it to all components: too difficult, too costly, outside the
scope of the method or tool, \etc In practice, expertise in the formal
verification method and tool is required to manually justify that the implicit
assumptions made by the tool are valid.

The care with which this manual analysis must be carried out is an incentive
for system designers to minimize boundaries between formally verified modules
and modules that are verified by other means. For example, this can be achieved
by formally verifying the entire code except some difficult-to-verify driver
code, or by formally verifying only a very critical core component of the
system.  However, such a monolithic approach is hindering a wider adoption of
formal methods.
Modules that are not formally verified are usually verified using other
methods, often by testing. If combining verification results of \eg proof and
test was easy, projects could freely choose the verification method to
apply to a given component, based on tool capabilities and verification
objectives.  We propose to facilitate the effective combination of modular
formal verification and other methods for the verification of critical software
by extending the application of assume-guarantee reasoning to these other methods.

\subsection{\spark}
\label{sec:spark}

\spark is a subset of the Ada programming language targeted at safety-
and security-critical applications. \spark builds on the strengths of
Ada for creating highly reliable and long-lived software. \spark
restrictions ensure that the behavior of a \spark program is
unambiguously defined, and simple enough that formal verification
tools can perform an automatic diagnosis of conformance between a
program specification and its implementation. The \spark language and
toolset for formal verification has been applied over many years to
on-board aircraft systems, control systems, cryptographic systems, and
rail systems~\cite{sparkbook2012,oneill2012}.

In the versions of \spark up to \oldspark, specifications are written as
special annotations in comments. Since version \newspark~\cite{sparkERTS2014},
specifications are written as special Ada constructs attached to
declarations. In particular, various contracts can be attached to subprograms:
data flow contracts (introduced by \texttt{global}), information flow contracts,
and functional contracts (preconditions and postconditions, introduced
respectively by \texttt{pre} and \texttt{post}). An important difference
between \oldspark and \newspark is that functional contracts are executable in
\newspark, which greatly facilitates the combination between test and proof
(see Section~\ref{sec:test-and-proof}). The definition of the language subset
is motivated by the simplicity and feasability of formal analysis and the need
for an unambiguous semantics. Tools are available that provide flow analysis
and proof of \spark programs.

Flow analysis checks correct access to data in the program: correct access to
global variables (as specified in data and information flow contracts) and
correct access to initialized data. Proof is used to demonstrate that the
program is free from run-time errors such as arithmetic overflow, buffer
overflow and division-by-zero, and that the functional contracts are correctly
implemented.

The different analyses support each other - for example, proof assumes that
data flow analysis has been run without errors, which ensures that all
variables are initialized to a well-defined value before use, that no
side-effects appear in expressions and function calls, and that variables are
not aliased. The latter point is partly achieved by excluding access (pointer)
types from the language, and completed by a simple static analysis. For the
purposes of this paper, we consider the \spark analysis as a whole in
Section~\ref{sec:test-and-proof} and will discuss interaction between the
different analyses in Section~\ref{sec:claims-and-assumptions-in-spark}.

\subsection{Related Work}

Neither the idea of explicit assumptions nor the
idea of combining different types of analyses on a project are new. However,
the focus of this line of research has been to show how different verification
techniques can collaborate {\em on the same code} and support each other's
assumptions. Examples are the explicit assumptions of Christakis
\etal~\cite{mullerexplicit}, the combination of analyses~\cite{framaccombining}
in Frama-C~\cite{framac}, the EVE tool for
Eiffel~\cite{tschannen2011usable} and the work of Ahrendt
\etal~\cite{ahrendt2012unified}. In contrast, we focus on the combination of
verification results for {\em different} modules. Another line of research is
the Evidential Tool Bus (ETB~\cite{cruanes2013tool}), which concentrates
on how to build a safe infrastructure for combining verification results from different sources and
tracking the different claims and supporting evidence. An ETB could be used as
the backbone for the framework that we describe in this paper.

\subsection{Outline}

In Section~\ref{sec:traditional}, we describe how the problem of heterogeneous
verification was addressed in an industrial project using the \spark formal
verification technology, developed at Altran UK, using an ad-hoc
methodology. In Section~\ref{sec:tool}, we propose a framework for combining
the results of different verification methods that can be partly automated, and
thus lends itself to a more liberal combination of verification methods. In
Sections~\ref{sec:test-and-proof}
and~\ref{sec:claims-and-assumptions-in-spark}, we present our experiments to
combine at coarse-grain and fine-grain levels proof and test on Ada programs,
using the \spark technology that we develop.

\section{Assumptions Management in a Large Safety-Critical Project}
\label{sec:traditional}

\projectx\footnote{This is not its actual name, which we cannot mention.} is a
large, mission-critical, distributed application developed by Altran UK, and
now in operational service.  The software consists of several programs that
execute concurrently on a network of servers and user workstations.  The latter
machines include a user-interface that is based on the X11/Motif UI framework.

Almost all of the software for \projectx is written in \oldspark
and is subject to extensive formal verification with the \oldspark
toolset.  Two other languages are used, though:

\begin{itemize}
\item Ada (not \spark subset, but still subject to a
project coding standard) is used where \spark units
need to call operating-system or compiler-defined run-time
libraries.

\item C code is used to form a layer between
the \spark code and the underlying X11/Motif libraries,
which is implemented in C.
\end{itemize}

One program, called the UI Engine, is a multi-task
\spark program that uses the RavenSPARK subset of Ada's
tasking features~\cite{sparkbook2012}.
This mitigates many common problems with concurrent programming, such
as deadlock and priority inversion. The C code is only ever called
from a single task of the main \spark program - a major
simplification which prevents interference between the
implementation languages, because the C code does not have global side
effects. Also, in this way the C code does not need to worry about reentrance.
The UI engine component is 87kloc (logical lines of code),
comprising of 61kloc \spark and 26kloc MISRA C.

\subsection{The Requirements Satisfaction Argument}

The fitness-for-purpose of \projectx is justified at the
top-level by a ``Requirements Satisfaction Argument''. This is
essentially structured as a tree of goals, justifications, assumptions,
and evidence, expressed in the Goal Structured Notation (GSN).

A large section of the GSN is devoted to non-interference arguments
that form the core of the safety argument for \projectx.  Part of that
non-interference argument includes detailed justifications for the
integration of software written in multiple languages, and the
prevention of defects that could arise. The leaves of the GSN typically
refer to verification evidence (\eg test specifications, results,
provenance of COTS components, static analysis results and so on)
or standards (such as the coding standards for \spark and C used by the project).

\subsection{From \spark to C (and Back Again)}

\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
Assumption & How verified\\
\hline
Parameter types match & AUTO\\
Variables initialized & MISRA\\
Outputs in expected subtype & REVIEW, TEST\\
No side effects & MISRA\\
No aliasing & MISRA, REVIEW\\
Data flow contract respected & REVIEW\\
No thread/task interaction & REVIEW\\
No dynamic allocation & MISRA\\
Functional contract respected & REVIEW, TEST\\
Absence of run-time errors & TEST\\
\end{tabular}
\end{center}
\caption{\spark to C assumptions and verification}
\label{sparkctable}
\end{table}

In \projectx, \spark code calls C code to implement various user-interface
elements. Beyond that point, the formal analyses offered by the \spark
toolset are not available, so we cannot rely on these analyses to prove
the assumptions made to analyze the \spark code.
Instead, the project manages an explicit list
of assumptions that must be enforced across each such boundary.
Essentially, the \spark code assumes that the called C function
is ``well-behaved'' according to a set of implicit project-wide rules
(\eg the function terminates, and parameters are passed using the
expected types and mechanism) and the explicit \spark contract
(\eg precondition and postcondition) applied to the \spark specification
of that function.

Each of these assumptions is verified on the C code through
a combination of one or more of:

\begin{itemize}
   \item AUTO. Automated code generation. In particular, the Ada and C
type declarations that are used to communicate across the language
boundary are automatically generated from a single description.
   \item MISRA. Automated static analysis using the MISRA C:2004 rules~\cite{misrac}.
   \item REVIEW. Checklist-driven manual code review. In particular, parameter
passing mechanisms and types are carefully reviewed to ensure they match
across such a language boundary.
   \item TEST. Specific unit test objectives.
\end{itemize}

The set of MISRA rules enforced and the review checklist items
were chosen to cover the assumptions needed to support the
verification of the \spark units. The project maintains a detailed
analysis of every MISRA rule and how its use meets the assumptions
required by the \spark analysis. A small number of MISRA rules are
not used by the project, or deviations may be justified
on a case-by-case basis.  Again, detailed records are maintained
to make sure that these deviations do not undermine the analysis
of the \spark code. Table~\ref{sparkctable} shows how each major assumption
made by the \spark code is verified in the C code.

\subsection{Example}

\begin{figure}[p]
\begin{subfigure}{\linewidth}
\lstinputlisting[language=SPARK]{frag_spec.code}
\caption{The \spark specification of \texttt{Set\_Off\_Button}}
\label{fig:fragspec}
\end{subfigure}
\begin{subfigure}{\linewidth}
  \lstinputlisting[language=C]{frag_h.code}
\caption{The C specification of \texttt{TB\_Set\_Off\_Button\_SC}}
\label{fig:fragh}
\end{subfigure}
\begin{subfigure}{\linewidth}
% SPARK Body
\lstinputlisting[language=SPARK]{frag_body.code}
\caption{The \spark body of \texttt{Set\_Off\_Button}}
\label{fig:fragbody}
\end{subfigure}
\caption{Excerpt of mixed SPARK/C code in \projectx.}
\end{figure}
\begin{figure}[p]
\ContinuedFloat
\begin{subfigure}{\linewidth}
\lstinputlisting[language=C]{frag_c.code}
\caption{The C implementation of \texttt{TB\_Set\_Off\_Button\_SC}}
\label{fig:fragc}
\end{subfigure}
\caption{ (continued) Excerpt of mixed SPARK/C code in \projectx.}
\end{figure}

This section shows an example of how \spark code interfaces to a UI function
that is written in C. The \spark specification is given
Listing~\ref{fig:fragspec}.  Note the data flow contract introduced by \texttt{global}. This specifies the
frame condition of the procedure - stating
exactly the set of objects that may be referenced and/or updated by the
procecure and (implicitly) defining that \emph{no other} objects are used.  In
this case, we see that the procedure may read and update two objects in
package Shutdown, both of which record the need to terminate the system in
response to a fatal error.

The \spark implementation is given in Listing~\ref{fig:fragbody}.  Ada's pragma Import
here specifies Convention ``C'' for the nested procedure - this instructs the
compiler to pass parameters as would be expected in C, according to a set of
rules given in the Ada Reference Manual.

The corresponding C header is provided in Listing~\ref{fig:fragh}.  Note the
careful use of naming conventions here to ease the task of both generating and
reviewing the Ada/C interface.  Finally, the C implementation is given in
Listing~\ref{fig:fragc}.  Note that the coding is overtly defensive, dealing with
the possibility of a memory corruption leading to the default branch of the
switch statement being executed.  The ``PRQA'' comments are instructions to
the MISRA analysis tool to suppress particular warnings.  All of these
comments are collated and verified as part of the satisfaction argument.

Consider one particular verification objective for this code: the output parameter
\texttt{Error} on the \spark declaration of the imported procedure \texttt{TB\_Set\_Off\_Button}.
In \spark, output parameters \emph{must} be defined by the called procedure in
all cases. This ensures that \texttt{Error} is properly initialized when calling
\texttt{Log\_And\_Handle\_Error} inside the body of \texttt{Set\_Off\_Button}.
If the body of \texttt{TB\_Set\_Off\_Button} were written in \spark, then the flow analysis engine would
verify this obligation, but since the body is in C, additional steps are needed.
In this case, an explicit review checklist item requires C function parameters
that correspond to \spark output parameters to be unconditionally initialized - hence the call to \texttt{CF\_Set\_OK} that initializes \texttt{Error} at the top of the function
body, for cases which do not result in an error.

\subsection{Summary}

This approach has proven reliable in practice, owing to
judicious architectural design, a strong desire to
minimize the volume of C code (although 26kloc does
still feel a little too large for comfort in an 87kloc
application), and strict adherence to design and
coding disciplines through automated analysis,
review checklists and focussed testing.

The main drawback is the time, expense and paperwork
required to maintain the satisfaction argument and its
supporting evidence across a long-lived project, which
has absorbed several major UI re-designs in its lifetime.

\section{Tool Assisted Assumptions Management}
\label{sec:tool}

Reading the previous section, the reader may ask the questions of how we came up
with the left column of Table~\ref{sparkctable} and how all subprograms at the
interface have been identified. This is in fact the result of expert
knowledge of the \spark technology as well as specificities of
the project. We want to present here a more systematic way to achieve the same goal.

The work done on \projectx to develop Table~\ref{sparkctable} and to apply it
at the boundary between formal verification and other methods can be broken
down into three steps:
\begin{itemize}
   \item listing all assumptions of formal verification,
   \item verifying the non-formally-verified modules using some other method,
     so that the previous assumptions are verified, and
   \item checking that all assumptions have been taken care of.
\end{itemize}

It is clear that for the first and the last step, tool support is possible and
welcome, and this is the topic of this paper. We propose to enhance formal
verification tools to not only output verification results, but also the
assumptions these results rely on - a more detailed version of the
left column of Table~\ref{sparkctable}. As non-formal methods may rely on
assumptions as well, we may also require that these other methods explicitly list
all their assumptions and guarantees when applied to a module. These should be
precise enough to avoid holes in the justification, or subtly different
interpretations of the properties in different methods. As an extreme example,
``module M is correct'' is not at the appropriate level of precision. For
formal methods, this requires an explicit enumeration of the usually implicit
assumptions made for the verification of a component, like non-aliasing of
subprogram parameters, non-interference of subprogram, validity of the data
accessed, \etc For informal methods, this requires defining methodological
assumptions and guarantees that the method relies upon.

Formally, each verification activity is a process whose
output is a list of Horn clauses, that is, implications of the form
\begin{equation*}
   A_1 \wedge A_2 \wedge \dots \wedge A_n \rightarrow\ { \textit C}
\end{equation*}
where $C$ is a claim\footnote{We prefer to use the word ``claim'' here over
``guarantee'', as it more clearly conveys the idea that until the
corresponding assumptions are verified, no guarantee can be given.}, and the
$A_i$ are assumptions. The exact form of claims and assumptions differs for
each method and tool. The next sections will provide examples based on \spark.

Compared to the manual process in the previous section, the advantage is that
the ``checklist'' of verification activities at the boundary between
verification methods is simply provided by each tool, and does not require
that users possess this level of expertise about the tool. The assumptions still need to be verified of course
- that is the right column of Table~\ref{sparkctable}.

If explicit assumptions are gathered for all verification activities on
the project, we can simply consider together all Horn clauses, and simple
queries such as
\begin{itemize}
   \item Is claim $C$ completely verified?
   \item On which unverified assumptions is claim $C$ based on?
   \item Which verification activities are left to do to verify claim $C$?
   \item If assumption $A$ turns out to be invalid, which claims does this impact?
\end{itemize}
can be answered simply by analyzing these Horn clauses.

Other forms of explicit assumptions are possible. Christakis
\etal~\cite{mullerexplicit} and Correnson and Signoles~\cite{framaccombining}
describe assumptions and claims as formulas at program points, which
is more precise than our approach.
The drawback is the complexity to generate and exploit these assumptions,
as their formulation implies the use of a weakest precondition calculus in the former work,
or trace semantics in the latter work, in order to interpret the meaning of
a formula at a program point.
Also, simply formulating these formulas already requires choosing a memory model.
Instead, we prefer to see claims and assumptions as well-defined,
parameterized properties uniquely identified by a tag. For example, where Christakis \etal use the
formula $c \neq d$ to express non-aliasing of two parameters at subprogram
entry, we prefer to write it as the property ${\textit Nonaliased}(c,d)$ where
the names $c$ and $d$ are given in a format which allows their unique
identification, and ${\textit Nonaliased}$ is a tag which is
unambiguously defined to mean that two variables do not share any memory.
Similarly, to denote an assumption on the precondition of some subprogram $p$,
we would write ${\textit Pre}(p)$ instead of the formula which constitutes the
precondition.

%% Yannick: I commented this as it talks about Ada and \spark, while this section should be language and tool agnostic
%% In \adatwtw and \spark, preconditions and postconditions are part of the language
%% and have well-defined semantics, so the predicate ${\textit Pre}$ is meaningful.

This choice, together with the choice of Horn clauses as data format, makes it
much simpler to feed assumptions to existing tools such as the
ETB~\cite{cruanes2013tool} and opens the door to tool assisted assumptions
management.

\section{Coarse-Grain Assumptions Management}
\label{sec:test-and-proof}

We have described in some previous work~\cite{hiliteERTS2012} a coarse-grain
application of the framework described in the previous section, to combine the
results of test and proof on Ada programs (proof being restricted to \spark
subprograms) in the context of certification of avionics software following
the \DOC~\cite{do178c} certification standard.
In this context, tests
with MC/DC coverage~\cite{ar0118} or proofs are two acceptable methods to verify a
module, where modules here are subprograms. We can reexpress the
goal of verifying a subprogram $P$ using Horn clauses as described in the previous section:

\begin{eqnarray*}
   {\textit Tests\_Passed}(P) \wedge {\textit MCDC\_Covered}(P) \rightarrow\ {\textit Verified(P)}\\
   {\textit Contract\_Proved}(P) \wedge {\textit No\_Runtime\_Errors}(P) \rightarrow\ {\textit Verified(P)}
\end{eqnarray*}

Note that assumptions made during proof are still implicit in the Horn clauses
above. Assumptions related to functional contracts, like the guarantee that
called subprograms respect their postcondition, or that the subprogram proved
is only called in a context where its precondition holds, are discharged either
by the proof of the callees/callers, or by executing the corresponding
contracts during the test of the callees/callers. Thus, it is essential for the
combination that functional contracts are executable. Other assumptions related
to non-aliasing of parameters, or validity of variables, are discharged by
having the compiler instrument the tested programs to check the
assumptions. Finally, the assumptions that cannot be tested are guaranteed by
the combination of a coding standard and a static analysis on the whole program.
The coding standard forbids in particular calls
through subprogram pointers, so that the call-graph is statically known.
The static analysis forbids aliasing between parameters of a call and global
variables that appear in the data flow contract for the called subprogram.

We built a prototype tool in Python implementing this approach, allowing users
to specify the generic Horn clauses above in some special syntax.  This
monolithic specially crafted approach for \spark was not completely satisfying,
as it was not easily extensible or customizable by users. This is why we
switched to a finer-grain approach where assumptions are explicit.

\section{Fine-Grain Assumptions Management}
\label{sec:claims-and-assumptions-in-spark}

We consider now the combination of verification results for individual
subprograms whose declaration is in \spark. As described in
Section~\ref{sec:spark}, various contracts can be attached to such subprograms:
data flow contracts, information flow contracts, and functional contracts
(preconditions and postconditions).

\subsection{Claims and Assumptions}
\label{sec:claims-and-assumptions-in-spark-sub}

We provide a detailed definition of claims and assumptions for \spark. We
assume subprograms are uniquely identified, for example by using their name
and the source location of the declaration. We also assume that {\em calls} to
subprograms are uniquely identified, again using \eg the source location of
the call. We use capital letters such as $P$ for subprograms, and write $P\at$
to indicate a specific call to a subprogram.

It should be noted that some fundamental assumptions, \eg correctness of the
verification tool, compiler and hardware, are out of scope of this framework
and are not taken into account here.

\spark formal verification tools may be used to ensure that the following
claims are satisfied by a subprogram $P$ or a call to $P$:
\begin{itemize}
   \item ${\textit Effects}(P)$ - the subprogram $P$ only reads
   input variables and writes output variables according to its data flow contract.
   \item ${\textit Init}(P)$ - the subprogram $P$ is only called when all
   its input parameters, and all the input variables in its data flow contract,
   are initialized.
   \item ${\textit Init}(P\at)$ - in this specific calling context of $P$,
   all its input parameters, and all the input variables in its data flow contract,
   are initialized.
   \item ${\textit Nonaliasing}(P)$ - the subprogram $P$ is not called with
   parameters which would create aliasing.
   \item ${\textit Nonaliasing}(P\at)$ - in this specific calling context of $P$,
   the values of parameters do not create aliasing.
   \item ${\textit AoRTE}(P)$ - the subprogram $P$ is free of run-time errors.
   \item ${\textit Contract}(P)$ - the subprogram $P$ respects its contract,
   that is, the precondition is sufficient to guarantee the postcondition.
   \item ${\textit Pre}(P)$ - the subprogram $P$ is only called in a context
   that respects its precondition.
   \item ${\textit Pre}(P\at)$ - in this specific calling context of $P$,
   its precondition is respected.
   \item ${\textit Term}(P)$ - the subprogram $P$ terminates.
\end{itemize}

The output of the \spark tools can then be described as follows. Given a
subprogram $P$ which contains the calls $R_i\at$, if flow
analysis is applied without errors, then the following set of Horn clauses holds:
\begin{align}
   {\textit Effects}(R_i) \wedge {\textit Init}(P) \wedge {}
   {\textit Nonaliasing}(P) \longrightarrow &{\textit Effects}(P) \wedge {} \nonumber \\
   & {\textit Init}(R_i\at) \wedge {} \nonumber \\
   & {\textit Nonaliasing}(R_i\at)
\label{eq:flow}
\end{align}
and if proof is applied without unproved properties being reported,
then the following set of Horn clauses holds:
\begin{align}
   {\textit Effects}(R_i) \wedge {\textit Init}(P) \wedge {}
   {\textit Nonaliasing}(P) \wedge {} \qquad {} & \nonumber \\
   {\textit AoRTE}(R_i) \wedge {\textit Contract}(R_i)
   \wedge {\textit Pre}(P) \longrightarrow &
   {\textit AoRTE}(P) \wedge {} \nonumber \\
\label{eq:proofpre}
   & {\textit Pre}(R_i\at) \\
   {\textit Effects}(R_i) \wedge {\textit Init}(P) \wedge {}
   {\textit Nonaliasing}(P) \wedge {} \qquad {} & \nonumber \\
   {\textit Contract}(R_i) \longrightarrow & {\textit Contract}(P)
\label{eq:proofcontract}
\end{align}

For the sake of succinctness, we have taken the liberty to merge Horn clauses
with different conclusions, but identical premises - this is only a shortcut
for the equivalent expansion using only Horn clauses.  The result of
successful flow analysis of subprogram $P$, as expressed in
Formula~\ref{eq:flow}, is that, assuming $P$'s callees respect their data flow
contract, and assuming $P$ is always called on initialized inputs and
non-aliased inputs/outputs, then $P$ respects its data flow contract, and
calls inside $P$ are done in a context which does not introduce uninitialized
inputs or aliasing for the callee. For proof, there are in fact two different
sets of results and assumptions. The first one, expressed in
Formula~\ref{eq:proofpre}, is that, assuming $P$'s callees respect their data
flow contract and their pre/post contract, and they do not raise run-time
errors, and assuming $P$ is always called on initialized inputs and
non-aliased inputs/outputs, in a context where its precondition holds, then
$P$ does not raise run-time errors, and calls inside $P$ are done in a context
where their precondition holds. The second one, expressed in
Formula~\ref{eq:proofcontract}, is that assuming $P$'s callees respect their
data flow contract and their pre/post contract, and assuming $P$ is always
called on initialized inputs and non-aliased inputs/outputs, then $P$ also
respects its pre/post contract.

Note that the precondition of $P$ is {\em not} an assumption of
Formula~\ref{eq:proofcontract}, because the tag ${\textit Contract}$ already
includes the precondition. In this manner, as can be easily seen,
Formula~\ref{eq:proofcontract} propagates assumptions about contracts {\em
down} the call graph, while Formula~\ref{eq:proofpre} propagates the
assumptions on preconditions {\em up} the call graph.

Note that ${\textit Pre}$, ${\textit Init}$ and ${\textit
Nonaliasing}$ applied to a subprogram are a bit special: they only appear as assumptions and not as
claims. They are in fact assumptions on the calling context, and the only
way to discharge them is to verify that they hold for all calling contexts. As a consequence,
a non-modular analysis is needed here to identify all calls to a subprogram,
so we add Horn clauses of the form:
\begin{equation}
{ \textit tag}(\overline{P\at}) \longrightarrow {\textit tag}(P)
\label{eq:closure}
\end{equation}
where ${\textit tag}$ is any of ${\textit Pre}$, ${\textit Init}$ and ${\textit
Nonaliasing}$ and $\overline{P\at}$ are all calls to a given subprogram $P$.
An important special case is that, for main subprograms (which are not called
by any other subprogram), we obtain the immediate guarantees of the form:
\begin{equation}
 {\textit tag}(P)
\label{eq:main}
\end{equation}

By combining Formulas~\ref{eq:flow}, \ref{eq:closure}, and~\ref{eq:main}, it
can be checked easily that, if formal verification is applied to the entire
program\footnote{We are assuming absence of recursion in the program.
Recursion requires a more advanced treatment.}, then ${\textit Effects}(P)$,
${\textit Init}(P)$ and ${\textit Nonaliasing}(P)$ hold for every subprogram
$P$. Similarly, by combining Formulas~\ref{eq:proofpre} to~\ref{eq:main}
together with the guarantees just obtained, it can be checked easily that, if
formal verification is applied to the entire program, then ${\textit
AoRTE}(P)$, ${\textit Pre}(P)$ and ${\textit Contract}(P)$ hold for every
subprogram $P$. As we did not check termination here, this corresponds exactly
to partial correctness of the program.

But, as we argued earlier, it is almost never the case that formal verification
is applied to a complete program. In that very common case, it is not
immediately clear what guarantees the application of formal verification
gives. In particular, a user is probably interested in knowing that no run-time
errors can be raised, which corresponds in our formalization to ${\textit
  AoRTE}(P)$ and ${\textit Pre}(P\at)$ and that the subprogram contracts are respected, which corresponds in our formalization
to ${\textit Effects}(P)$ and ${\textit Contract}(P)$. With our formulation of
formal verification results as Horn clauses, we can precisely compute on which
unverified assumptions these claims depend.

\paragraph{Termination.} We have not discussed termination (represented by the
tag ${\textit Term}$) yet. In fact, termination is {\em not} an assumption of
Formulas~\ref{eq:proofpre} and ~\ref{eq:proofcontract}, because the properties
claimed there are formulated in terms of partial correctness. For example, the
most precise formalization of ${\textit Contract}$ is: if the precondition
holds, and {\em if} the control flow of the program reaches the end of the
subprogram, then the postcondition holds. Assuming absence of recursion, the
\spark tools can in fact establish termination by adding the following set of
Horn clauses for each subprogram: \begin{equation*} {\textit Term}(R_i) \wedge
{\textit Term}(L_k) \longrightarrow {\textit Term}(P) \label{eq:term}
\end{equation*} where the $R_i$ are the subprograms called and the $L_k$ are
the loops occurring in the subprogram $P$. Termination of loops can be
established in \spark by two means: \texttt{for}-loops terminate by
construction in Ada, and more general loops can be annotated with a variant,
wich allows to prove termination of the loop.

\subsection{Discharging Assumptions}

\begin{table}[t]
   \begin{center}
   \begin{tabular*}{\textwidth}{p{0.3\textwidth}|p{0.7\textwidth}}
   \hline
   assumption & verification strategy \\
   \hline
   \multicolumn{2}{l}{assumption on call} \\
   \hline
   ${\textit Init}(P\at)$ & coding standard, run-time initialization checking  \\
   ${\textit Nonaliasing}(P\at)$  & static analysis, run-time non-aliasing checking, review \\
   ${\textit Pre}(P\at)$  & unit testing with assertions enabled \\
   \hline
   \multicolumn{2}{l}{assumption on subprogram} \\
   \hline
   ${\textit Effects}(P)$ & static analysis, review, coding standard \\
   ${\textit AoRTE}(P)$ & unit testing with run-time checks enabled \\
   ${\textit Contract}(P)$ & unit testing with assertions enabled\\
   ${\textit Term}(P)$ & unit testing, review \\
   \end{tabular*}
   \end{center}
   \caption{Assumptions and possible verification strategies}
   \label{table:assumptions}
\end{table}

As visible from Formulas~\ref{eq:flow} to \ref{eq:proofcontract}, the \spark tools
provide claims for the formal verification of one subprogram that discharge
assumptions for the formal verification of another subprogram. It remains to
see how to discharge assumptions at the boundary between formally verified and
non-formally verified code.

Table~\ref{table:assumptions} summarizes the assumptions of \spark and presents
possible verification strategies when the \spark tools cannot be applied to the
code on which the assumption is issued.  The possibility in Ada to perform
exhaustive run-time checking allows applying unit testing for verifying the
absence of run-time errors.  The possibility to also execute functional
contracts is only available with \newspark, not \oldspark, and it allows
applying unit testing for verifying functional contracts.

Assumptions on the calling context are a bit more difficult to verify.
Table~\ref{table:assumptions} does not contain entries for
assumptions on the calling context, so the first step is to find out all
callers. Once all call points are identified, one needs to verify that each
call verifies the assumptions that have been made for the verification of the
called subprogram. This poses another interesting challenge: How to verify
by testing that, \eg the precondition of a call deep inside the tested
subprogram holds?
How can one be sure that enough testing was applied?
We are not answering these questions in this paper, but raising the
issue.

Finally, \spark lets the user insert assumptions inside the program for both flow
analysis and proof. A typical example is a counter whose
incrementation could overflow in theory, but never does in practice because
it would require that the system runs for longer that its longest foreseen running time.
In that case, the user can insert a suitable code assumption before the counter
is incremented:
\lstinputlisting[language=SPARK]{counter.code}
Such assumptions can also be part of the output of the tools, so that a review of all
remaining assumptions can assess their validity.

\subsection{A Concrete Example}

In this section, we exercise the assumptions mechanism on the example provided
in Section~\ref{sec:traditional}. Let us assume that we apply the \spark tools
only to \texttt{Set\_Off\_Button}, and in the remainder of this section we
assume that flow analysis and proof have been applied successfully. We
therefore obtain the following verification results:
\begin{eqnarray*}
   {\textit Effects}(R_i) \wedge {\textit Init}(P)
    & \longrightarrow &{\textit Effects}(P) \wedge {\textit Init}(R_i\at)\\
   {\textit Effects}(R_i) \wedge {\textit Init}(P) \wedge
   {\textit AoRTE}(R_i)
   & \longrightarrow &
   {\textit AoRTE}(P)
\end{eqnarray*}
where $P$ is \texttt{Set\_Off\_Button} and the $R_i$ are
the subprograms called by \texttt{Set\_Off\_Button}:
 \texttt{To\_C\_Bool},
 \texttt{Background\_Color\_T},
 \texttt{TB\_Set\_Off\_Button}, and
 \texttt{Log\_And\_Handle\_If\_Error}.

Note that the above statement is somewhat shorter than the general one because
the tags ${\textit Pre}$ and ${\textit Contract}$ do not apply (no
preconditions or postconditions appear in the example), just as the tag
${\textit Nonaliasing}$. In fact, it is impossible for aliasing to occur in
the example, partly due to the types of parameters that cannot alias in some
calls, and partly because scalar input parameters are passed by copy in Ada,
and thus cannot alias with anything.  The other claims that \spark could
provide do not apply, because \texttt{Set\_Off\_Button} doesn't have a
postcondition, and the called subprograms do not have preconditions.

There are three assumptions in the above Horn clauses, for which we can find
both which verification was actually performed in \projectx, in
Table~\ref{sparkctable}, and to which general verification strategy this
corresponds, in Table~\ref{table:assumptions}, as summarized in
Table~\ref{table:concrete-example}:

\begin{table}[h]
\begin{center}
\begin{tabular}{l|l|l}
Assumption & How verified in \projectx & Verification strategy applied\\
\hline
${\textit Effects}(R_i)$ & MISRA  & coding standard\\
${\textit Init}(P)$      & MISRA & coding standard\\
${\textit AoRTE}(R_i)$   & TEST  & unit testing\\
\end{tabular}
\end{center}
\caption{Discharging assumptions by other methods in a concrete example}
\label{table:concrete-example}
\end{table}

In fact, most assumptions from Table~\ref{sparkctable} also appear in
Table~\ref{table:assumptions}. Those that do not appear are
project-specific. For example, the use of \spark tools does not prevent the use
of dynamic allocation in other parts of the program, in general. It happens to
be a requirement of the project described in this paper.

\section{Conclusion}

We have presented the current state of the art in industrial software when
applying formal verification on part of the code only. We reused the notion of
explicit assumptions, which has already been present in other works, but used
differently and for different purposes, to show how to render formal
verification truly modular by proper tool support.  We have experimented with
a coarse-grain variant of explicit assumptions to realize the combination of
proof and test, and have presented a more fine-grain model.

\paragraph{Future Work.} Our immediate plan is to implement explicit
assumptions in the \spark technology, using the evidential tool bus as the
back-end for assumptions management. More work is required to make the
application of the framework truly usable. For example, all the presented
tags simply have a subprogram name or call as argument. To increase precision,
it would be better to also include tags with variable arguments, \eg a tag
such as ${\textit Nonaliasing}(x,y)$. Such support is not very different from
what we describe here, but much more complex to write down, and the
non-modular analysis to match call guarantees with calling context assumptions
requires more work.

Our ultimate goal is to provide support for assumptions management and a
smooth combination of test and proof in a future version of the commercial
\spark tools.

% \paragraph{Acknowledgements.} We thank the anonymous reviewers for their
% remarks and especially reviewer 1 who suggested a possible way to avoid the
% non-modular analysis in Section~\ref{sec:claims-and-assumptions-in-spark-sub}.

\bibliographystyle{abbrv}
\bibliography{tap}

\end{document}
