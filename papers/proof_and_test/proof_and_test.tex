\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{zed-csp}

\newcommand{\DO}{\textsc{do-178}\xspace}
\newcommand{\DOB}{\textsc{do-178b}\xspace}
\newcommand{\DOC}{\textsc{do-178c}\xspace}
\newcommand{\hilite}{Hi-Lite\xspace}
\newcommand{\gnatprove}{GNATprove\xspace}
\newcommand{\oldspark}{SPARK~2005\xspace}
\newcommand{\newspark}{SPARK~2014\xspace}
\newcommand{\spark}{SPARK\xspace}
\newcommand{\ada}{Ada\xspace}
\newcommand{\adatwtw}{Ada~2012\xspace}
\newcommand{\vectorcast}{VectorCast\xspace}

\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\ie}{\textit{i.e.,}\xspace}
\newcommand{\adhoc}{\textit{ad hoc}\xspace}
\newcommand{\Eg}{\textit{E.g.,}\xspace}
\newcommand{\eg}{\textit{e.g.,}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\aka}{a.k.a.\xspace}
\newcommand{\resp}{resp.\xspace}

\lstdefinestyle{tinystyle} {basicstyle=\scriptsize\tt,
  keywordstyle=\bf, commentstyle=\rmfamily\it, escapeinside={(*}{*)}}
\lstset{style=tinystyle}

\lstdefinelanguage{SPARK}{ language = [95]Ada, morekeywords = {pre,
    post, assert, assume, check, derives, hide, global, inherit, from,
    own, initializes, main_program, input, output, in_out,
    refined_pre, refined_post, some, depends}, comment=[l][commentstyle]{--\ },
  showstringspaces=false }

\lstset{language=SPARK}

\begin{document}

\title{Optimising Verification Effort with \newspark}

\author{Pavlos Efstathopoulos, Andrew Hawthorn}
\institute{ALTRAN, 22 St Lawrence Street, Bath BA1 1AN (United Kingdom)}

\maketitle

\begin{abstract}
This paper considers the problems of subprogram specification,
implementation and verification in the context of SPARK 2014.  We
touch lightly on specification and implementation and focus on how to
combine formal verification and test efficiently.

The method of specification and implementation are covered briefly as
the methods used affect how formal methods can be applied. We explain
how different depths of formal analysis can be used but focus on the
case where full functional specifications are provided.

We then consider a number of scenarios where proof, test or a
combination of both can be used to verify the functional behaviour of
software. For each scenario, we identify the most effective method of
verification and demonstrate it using the SPARK 2014 toolset and or a
commercial unit testing tool.

We conclude by summarising our views on the most effective combination
of proof and test and any limitations in current tooling.

\end{abstract}

\paragraph{Keywords}
Formal methods, Verification and validation, Certification,
Dependability, DO-178C

\section{Introduction}
The introduction of executable contracts in Ada 2012 brings a new
dimension to the debate over which is most efficient: proof or
test. The Hi-Lite project was designed to demonstrate this extension
and coverered the question of how proven subprograms can reliably call
tested subprograms and visa versa \cite{hiliteERTS2012}. This paper
looks at the issue from an industrial perspective to try to identify
where it is most efficient to prove or test subprograms.

Test is clearly the most preferred method of verifying software at the
moment but the regulated industries have been trying to use formal
methods where they can ever since software was first written. The
acceptance of formal methods seems to be increasing, for example, the
latest version of the aerospace standard \DO provides explicit
guidance on the use of formal methods.

Given the general view that test is the best method of verification,
we introduce here a number of scenarios where proof can complement and
replace test with either minimal additional effort or less effort than
using test alone.

The majority of the scenarios defined here rely on the use of Ada 2012
contracts to specify the functionality of a subprogram, we therefore
spend a little time understanding how expressive they are and what
additional cost (if any) is attributable to their use.


\section{Spectrum of formal verification in \newspark}

Formal verification can be applied to varying levels to help reduce
the number of bugs introduced and maximise the number of bugs found.

\newspark provides many features to support such program analysis,
this section only introduces the features that directly support the
combination of proof and test.

\newspark is the next incarnation of \oldspark \cite{sparkbook2012}
that is effectively a complete re-write of the language and the
toolset. One of the main drivers behind this update is \adatwtw - the
executable contracts and generic aspect definitions being particularly
useful.

The features of \newspark that are particularly relevant to the proof
and test combination are:
\begin{itemize}
\item language subsetting;
\item flow analysis;
\item the use of theorem provers to demonstrate absence of run-time
  errors;
\item partial and full specification using pre- and post-conditions;
  and
\item the use of global contracts.
\end{itemize}

\subsection{\newspark language subset}
The \newspark language subset is designed with the same philosophy as
\oldspark but because of advances in technology far more features of
\ada can be practically formally reasoned about. The \newspark
language subset will therefore be a lot larger than \oldspark.

The key language features that are not in the subset are:

\begin{itemize}
\item no access types (pointers);
\item expressions (hence functions) must not have side-effects;
\item no aliasing (but renaming is permitted);
\item no gotos;
\item no controlled types;
\item no tagged types (for now...);
\item no tasking (for now...); and
\item no exceptions.
\end{itemize}

These restrictions are necessary for proof but also help test by
removing classes of faults that are difficult to find during
testing. For example, aliasing of globals may not be spotted at unit
test because the aliasing does not have an effect and may not be
spotted at system test because the alisasing has such a subtle effect.

For this reason, the authors recommend that the \newspark language
subset is used irrespective of whether or not proof or test is being
used to verify a subprogram.

\subsection{\newspark Flow analysis}
Flow analysis is the term we use for identifying uninitialized and
unused variables and ineffective statements.  It is important for
proof and test because the proof relies on all variables being
initialised and test results can be skewed in the presence of
uninitialised variables. Coverage analysis can also benefit from flow
analysis because it can identify ineffective statements.

For these reasons, the authors recommend that flow analysis is carried
out on all subprograms irrespective of whether or not proof or test is
being used to verify a subprogram.

\subsection{Proof of absence of run-time errors}

Proof of absence of run-time errors verifies that no exceptions will
be raised. This covers the following Ada checks:

\begin{itemize}
\item overflow check;
\item range check;
\item index check;
\item division check;
\item discriminant check and
\item length check.
\end{itemize}

See the Ada Language Reference Manual for the precise definitions.

For these reasons, the authors recommend that proof of absenct of
run-time errors is carried out on all subprograms irrespective of
whether or not proof or test is being used to verify a subprogram.

\subsection{\adatwtw Pre- and postconditions}

\ada \emph{Pre} and \emph{Post}-conditions can be used for a variety
of purposes. At their simplest they can be used to support proof of
absence of run-time errors (\eg by constraining the ranges of inputs
and outputs). They can also be used to specify specific properties of
a program (\eg a safety property is always true). They can also be
used to fully specify the behaviour of a subprogram (\ie the effect of
the subprogram is fully defined in the postcondition).

The following example is of a subprogram which has its behaviour fully
specified.

\begin{lstlisting}[language=SPARK]
A_Counter : Integer;

procedure Inc_A_Counter (N : in Integer)
   with Pre  => N >= 0,
        Post => A_Counter = A_Counter'Old + N;
\end{lstlisting}

The executable nature of the pre- and postconditions means that we
have an in-built method of checking that the result of a subprogram
call meets its specification. This is an extremely powerful feature
that can be used both informally and formally.

In \newspark these contracts can be refined in the body but these
refinements are less interesting from a proof and test perspective.

\subsection{\newspark Global contract}

\newspark introduces a new \ada aspect \emph{Global} which is used to
specify the global variables and their modes that a subprogram may
use. The Global aspect can be used without Pre and Post to define a
very basic contract \eg a Global aspect defines which globals a
subprogram will modify and which will be unmodified.

The Global aspect is important from a proof and test perspective
because it can be used to demonstrate ``non-interference''. Where
``non-interference'' means that a subprogram only updates the
parameters and globals that are defined in its contract.

The following example extends the first by defining the globals for
Inc\_A\_Counter. The \newspark tools check that the subprogram
complies with its Global annotation so in this case they guarantee
that Inc\_A\_Counter does not read or write to B\_Counter.

\begin{lstlisting}[language=SPARK]
A_Counter : Integer;
B_Counter : Integer;

procedure Inc_A_Counter (N : in Integer)
   with Global => (Output => A_Counter),
        Pre    => N >= 0,
        Post   => A_Counter = A_Counter'Old + N;
\end{lstlisting}


\section{Expressiveness of Ada contracts}

In this section we consider how easy it is to represent specifications
that would normally be written in \emph{Z} in Ada expressions. We
start with Z because it is a popular specification language for
software systems that need to use formal methods. In the first subsection
we provide an example Z specification and convert it to Ada.
In the second subsection we report results from an analysis of the
Z specification in an industrial case study.

\subsection{Example Z to Ada translation}

The following Z schema has been translated to demonstrate how some of
the most used constructs in formal specifications are represented in
\adatwtw contracts:

\begin{schema}{ValidEnrol}
Enrol\\ \where

        issuerCerts \cap \{ CAIdCert \} \neq \emptyset

\also

\\ \forall cert : issuerCerts @

           cert.isValidatedBy \neq Nil

\\ \t1 \land (\exists issuerCert : issuerCerts @

        issuerCert \in {CAIdCert}

\\ \t2 \land The \ cert.isValidatedBy = issuerCert.subjectPubK

\\ \t2 \land cert.id.issuer = issuerCert.subject )

\end{schema}

\begin{lstlisting}[language=SPARK]
function ValidEnrol (CAIdCert : in ID) return Boolean
   with Global => (Input => issuerCerts),
        Pre    => (for some issuerCert in issuerCerts => cert = CAIdCert),
        Post   => ValidEnrol'Result = (for all cert in issuerCerts =>
                    (cert.isValidatedBy /= NullCert
                     and (for some issuerCert in issuerCerts =>
                            (issuerCert = CAIDCert
                             and cert.isValidatedBy = issuerCert.subjectPubK
                             and cert.id.issuer = issuerCert.subject))));
\end{lstlisting}

This example is taken from the Tokeneer project which is available for
public download from the AdaCore website. It demonstrates how close Z
specifications are to executable code and indeed the above example
could be written as an \adatwtw expression function which would then
not need a body.

This is a good example of where writing the specification in a
particular way can reduce the development effort and/or support rapid
prototyping. With the function written as it is, a body needs to be
written but if efficiency is not an issue or a prototype is required
it could be written as an expression function and then no more
development effort is required.

From this perspective, writing expression functions should be the
preferred method for specifying the software. However, this removes
the semantic gap between the specification and the code. In some
contexts this may be considered very useful but it could lead to
specification faults being missed. This is an interesting topic but
will not be developed further here.

\subsection{Industrial case study analysis}

Tokeneer \cite{tokeneer} is an industrial demonstration project carried
out by the NSA that was fully specified in Z. Here we consider how
easily the Tokeneer Z specification could be represented in Ada
specifications.

There are a total of 196 schemas and definitions in the Z specification 
(excluding type definitions). Of these 90\% represent functional definitions,
the remaining primarily restric access to state. Of the 90\% schemas
and definitions that represent functional definitions, 80\% can be 
represented in Ada pre- and postconditions with virtually no translation
effort. The remaining 20\% can easily be translated by:

\begin{itemize}
  \item using the Ada Container ``Hashed\_Sets'';
  \item defining a bi-conditional (<->) expression function;
  \item using `Update instead of the relational override operator;
  \item defining inverse functions where required; and
  \item adding other basic expression functions.
\end{itemize}

Whilst carrying out the analysis the following notes were made:
\begin{itemize}
   \item For Z specifications the design is not taken into account
         but for specifications written in Ada pre- and post- conditions
         the software architecture (down to at least the high-level
         package specifications) must be defined. Given detailed 
         specifications have to make some design decisions 
         (\eg splitting up system state) this formalisation of
         design decisions will help simplify the mapping to the 
         implementation.
         
   \item 
\end{itemize}

ToDe: We then consider the practical expressiveness of \newspark pre-
and post-conditions in comparison with Z using Tokeneer as an example
application. There are 157 schemas in Tokeneer.


\section{Scenario identification}

We now consider a number of scenarios where proof and test may be
used to demonstrate code compliance with a formal
specification on a single subprogram.  The Hi-Lite project has already
demonstrated \cite{hiliteERTS2012} how proven and tested subprograms 
can be combined so we focus on how a combination of proof and test
can used to efficiently verify a single program.

The following table presents valid combinations of proof and test
for individual subprograms. The rows define combinations of proof
and test and the columns define the level of formal specification
(using Ada pre- and post-conditions). Each cell in the table is
labelled with a letter indicating whether or not the combination
of specification level and proof/test combination is required (R),
optional (O) or not applicable (NA).

\begin{center}
\begin{tabular}{l | c | c | c}
    & Fully specify & Partially specify & Can't specify \\
  \hline
  Only prove & R & NA & NA \\
  Only test & O & O & R \\
  Prove some and test some & O & O & NA \\
  Test to debug contracts & O & O & NA \\
\end{tabular}
\end{center}

In all cases, test uses the executable semantics of pre- and 
postconditions to check the program partially meets its specification.
If the behaviour is fully specified, then the pre- and postconditions 
can be used without additional test output checking - thus saving more.

The following list covers all of the interesting combinations from 
the table above.
Unless explicitly stated
otherwise, each scenario assumes a full specification is provided in
pre-/post-conditions and proof initially fails.  
We assume the user has used the \newspark subset,
carried out flow analysis and completed proof of absence of run-time
errors.


\begin {enumerate}
\item \emph{Only prove} - try to fully prove
  the code by strengthening preconditions and adding assertions until
  full proof is achieved. No testing is involved in this scenario.

\item \emph{Only test} - there are at least two reasons why a user
   may resort to only testing a subprogram:

   \begin{itemize}
      \item the proof may be too difficult to perform because the 
            structure of the code is very different from the structure
            of the specification or

      \item contracts can not be written in Ada 2012, for example 
            because they represent an interface to an external device
   \end{itemize}


\item \emph{Prove some, test some} - attempt to prove the code, fail
  and resort to testing. At this point the user has two different
  options:

  \begin{itemize}
  \item Perform exclusively testing (prove no properties).

  \item Prove only a subset of the properties of the code and test the
    remaining. For instance, prove freedom of run-time exceptions and
    test functional behaviour-related contracts.
  \end{itemize}

\item \emph{Test to debug contracts} - make a first unsuccessful
  attempt to prove the code and afterwards utilize testing to identify
  potential issues with the contracts or the implementation. The tests
  might provide hints as to how the user can alter the code to render
  it provable. In this scenario, testing reveals the actions that need
  be performed to achieve full proof. [Angela's solution to the
  ?VSTTE? competition.]

\item \emph{Prove some, test everything} - a standard may require
  all functionality to be tested. 
  However, as an additional verification activity, performing proof could
  improve the safety case and provide more confidence in the code. In
  this scenario the user would try to prove as much as possible.
\end{enumerate}

When more than one units have to be analysed, any combination of the
above could potentially occur. Naming but a few such examples, it
would be possible to have:
\begin{itemize}
\item a subprogram Q for which a full formal specification cannot be
  provided. Consequently Q is tested. Subprogram P, which has to be
  proven, calls Q. A partial specification is therefore added to Q to
  assist with the proof of P.

\item a subprogram that has only been tested is utilized in the proof
  of one of its callers.
\end{itemize}



\section{Scenario evaluation}

\subsection{Only prove}
In this section, we will demonstrate how three functions can be fully
verified without resorting to testing.

The entire
process that was followed will be presented. The chosen functions have
precise mathematical definitions and consequently it was hypothesised
that fully proving them should be possible. The functions being 
verified are:
\begin{enumerate}
\item Function \emph{Is\_Prime} takes a natural number as its single
  parameter and returns true if that number is a prime and false if it
  is not. A natural number (\ie $1, 2, 3, 4, \dots$) is called a prime
  if it has exactly two positive divisors, $1$ and the number itself.

\item Function \emph{Are\_Coprime} takes two natural numbers as
  parameters and returns true if these numbers are coprime and false
  if they are not. Two naturals X and Y are called coprime if the only
  natural that evenly divides both of them is $1$.

\item Function \emph{Factorial} takes a natural number as its
  parameter and returns this number's factorial. The factorial of a
  natural number is the product of that number and all natural numbers
  below it (\Eg $5! = 5 * 4 * 3 * 2 * 1$).
\end{enumerate}

\subsubsection{Generating and proving the code}
As a first attempt, the following code was written:
\lstinputlisting[language=Ada, numbers=left, title=Initial Spec]
{Code/proven/Initial_Attempt/proven.ads}
\lstinputlisting[language=Ada, numbers=left, title=Initial Body]
{Code/proven/Initial_Attempt/proven.adb}

Functions \emph{Is\_Prime} and \emph{Are\_Coprime} were proven
straight away while the following messages that relate to function
\emph{Factorial} were issued:

proven.adb:47:19: overflow check not proved

proven.adb:47:19: range check not proved

\noindent
To resolve these, it must be shown that:
\begin{enumerate}
\item the value returned by the expression $N * Factorial (N - 1)$ is
  within the range $Natural'First .. Natural'Last$.

\item the result of $N - 1$ is within the range $Natural'First
  .. Natural'Last$.
\end{enumerate}
Unfortunately, property 1 does not hold for the current code. If
parameter $N$ exceeds a certain value, then the result of
\emph{Factorial} will not fit within a Natural and an overflow
exception will be raised at runtime.

In order to overcome these issues, a precondition, an assumption and a
new subtype had to be introduced to the code. The precondition creates
an obligation, for all callers of \emph{Factorial}, that $N \leq
10$. The assumption informs the tools that $Factorial (N - 1) \leq
362880$, since $(10 - 1)! = 9! = 362880$. Finally, the subtype
constrains the value returned by the function to the range $1
.. 362880$. After strengthening the code, all verification conditions
are discharged.

\lstinputlisting[language=SPARK, title=Strengthened Spec, firstline=13,
  lastline=21]{Code/proven/Strengthened/proven.ads}
\lstinputlisting[language=SPARK, title=Strengthened Body, firstline=35,
  lastline=46]{Code/proven/Strengthened/proven.adb}

\subsubsection{Notable observations}
The initial assumption that was made was that the selected functions
would be proof-friendly due to their mathematically well-defined
nature. This was indeed the case. However, recursive definitions (such
as the definition of function \emph{Factorial}) can 
complicate the proving process.

\subsection{Only test}
Proving a unit may not be possible because either the contract can not
easily be specified in terms of Ada pre- and post-conditions or 
because it is difficult to derive the implementation from the 
specification.  This section provides an example where
testing was deemed to be more efficient.

\subsubsection{Mergesort}
The code that follows is an Ada implementation of the top-down version
of a well known sorting algorithm called mergesort.
\lstinputlisting[language=SPARK, title=Mergesort's
  Spec]{Code/tested/mergesort.ads} \lstinputlisting[language=SPARK,
  title=Mergesort's Body]{Code/tested/mergesort.adb}

\subsubsection{\vectorcast tool and testing results}
We used \vectorcast to try to verify the software against the 
specification for Mergesort.

\vectorcast reads the Ada source code and
assists in the generation of a test harness and provides a convenient
interface for defining test input data and expected results.
The effort to define the test harness and test cases was just a few
hours but we expect that for users who have been on a \vectorcast
training course and are familiar with the tool 
would be able to put together
the test harness and full test suite for mergesort within 30 minutes.

If \vectorcast had support for Ada 2012 Pre- and Postconditions then 
it would have been possible to decide not to define the expected results
and use the success or failure of the execution of the postcondition
to represent the result of the test. This would have reduced 
testing effort.

Even though the tests that were added were very basic (a total of 3
pairs of inputs and their expected outputs were written), 100\%
statement coverage and 96\% branch coverage was achieved. All tests
yielded the expected results.


\subsubsection{Examples where proof should not be used}
There are some cases where proof is difficult to achieve even when
a formal specificaton has been defined. This is unusual in the 
applications we have developed but should be considered a valid
reason for deciding not to prove a subprogram. Some examples are:

\begin{itemize}
   \item Contract or code contains non-linear expressions, for example 
         Result = X**Y (although Factorial defined above is a 
         counter-example);
   \item Code structure is significantly differenet from specification
         (binary sort is an example where proof is possible but
          should only be tackled by advanced users)
\end{itemize}


\subsection{Prove some and test some}
Having an accurate contract and a flawlessly annotated implementation,
unfortunately, does not always guarantee that all properties of the
code will be fully verified.  Theorem provers are the best option
currently available for performing automated reasoning and even though
they improve on a daily basis they are still far from perfect.  This
section presents two examples and shows two different courses of
action that a user can follow when proof is not (directly) achievable.

\subsubsection{Fall back to testing}
Current theorem provers are great with linear arithmetic but struggle
with non-linear arithmetic as shown in this example.
\lstinputlisting[language=SPARK, title=Spec Of Example
  One]{Code/proof_failed/example_one/example_one.ads}
\lstinputlisting[language=SPARK, title=Body Of Example
  One]{Code/proof_failed/example_one/example_one.adb} The given code
should be provable with its current annotations but the tools are not
able to discharge the overflow check associated with line
10. Restructuring the code and/or the annotations could potentially
enable the tools to achieve full proof. However, for this example, due
to the simplicity of the code, performing a manual review and writing
some basic tests was deemed to be more cost-effective.

\subsubsection{Perform a partial proof}
Proof is easily applied to leaf procedures and functions but can be
more tricky for higher level procedures. In the following example all
of the functions declared in the specification were proven
automatically. Test was used to demonstrate the top-level Sort
procedure.
\lstinputlisting[language=SPARK, title=Spec Of Example
Two]{Code/proof_failed/example_two/sorting.ads}
\lstinputlisting[language=SPARK, title=Body Of Example
Two]{Code/proof_failed/example_two/sorting.adb}

\subsubsection{Points of interest arising from attempting to verify}
When the tools are unable to discharge a verification condition and
the user is confident that the existing annotations should be
rendering it provable, trying the following might help:
\begin{itemize}
\item Increase the time spent by the prover on each verification
  condition.
\item Restructure the code and/or annotations to facilitate proof.
\end{itemize}

Predicting if a prover will be able to discharge a certain
verification condition is not always possible. Results might
occasionally even differ between provers. None the less, there are
certain areas where provers are known to perform well (linear
arithmetic) and other areas where automated proof is harder to
achieve (non-linear arithmetics). The more accustomed a user becomes
to the tools, the easier it becomes to predict what will be
automatically provable. Performing proof when it is expected to be
easy and testing the remainder of the code is theorised to be the most
time-efficient code-validating method.

\subsection{Test to debug contracts}
Unit test can be used to debug contracts, in some cases, unit tests
may identify invalid pre- or postconditions more quickly than review.

Todo: Expand this section.

\subsubsection{Example code}
\subsubsection{Describe approach to verification}
\subsubsection{Report results of verification}
\subsubsection{Report any points of interest arising from attempting to verify}

\subsection{Contracts can not be written}

There are a number of situations, particularly at the boundary of a
system, where full functional specifications can not be written. In
this case partial specifications may be required - these provide the
view of the outside world required for the formal model. In this case,
a combination of partial proof and test is ideal although pure test
would be sufficient.

An example of this is a logging procedure. We may want to prove that higher 
level functions call the logging procedure and update a log when an
error occurs but the proof may not care what data is written. In this case
we would write a partial postcondition on the logging function and write
a test to ensure the correct data is written to the file.

\section{Summary}

The introduction of \newspark heralds a leap towards the ultimate goal
of making the use of formal methods more cost effective than
traditional methods of software development. The ability to easily
switch between test and proof and the automation of proof tasks
provides the potential to identify code faults much earlier and
thereby dramatically reduce verification effort. There are some
tooling limitations that mean this goal is not yet met but many of
these have simple engineering solutions.

\subsection{How can proof and test most effectively be combined?}
Todo: Complete this section. Where proof is free executing contracts
to help debugging them

\subsection{Summary of limitations of current tooling}

Todo: Complete this section.

Todo: NB each of these points should be introduced earlier in this
doc.

What additional tooling support is required to make it easier/more
generally applicable.

Unit testing: At least Ada 2012 support required in unit testing
tools. Unit test tools should support the execution of contracts and
allow the result of a postcondition execution in place of "expected
outputs". Unit test tools should support turning on and off execution
of specific contracts (to support formal testing)

Proof: non-linear proof floating point proof

Proof and test: Need for assumption management.

\bibliographystyle{plain}
\bibliography{proof_and_test}

\end{document}
