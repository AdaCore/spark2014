\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\usepackage[table]{xcolor}
\newcommand{\CodeSymbol}[1]{\textcolor{Bittersweet}{#1}}
\lstset{
   language=Ada,
   keywordstyle=\color{RedViolet}\ttfamily\bf,
   showspaces=false,
   basicstyle={\scriptsize \sffamily},
   commentstyle=\color{red}\textit,
   stringstyle=\color{MidnightBlue}\ttfamily,
   string=[b]",  % remove ' from string delimiter as it interfers with attributes
   showtabs=false,
   showstringspaces=false,
   morekeywords=[1]Pre,
   morekeywords=[1]Post,
   morekeywords=[1]Test\_Case,
   morekeywords=[1]Contract\_Cases,
   morekeywords=[1]some,
   morekeywords=[1]Old,
   morekeywords=[1]Global,
   morekeywords=[1]Depends,
   morekeywords=[1]Loop\_Invariant,
   morekeywords=[1]Loop\_Variant,
   morekeywords=[1]Loop\_Entry,
   morekeywords=[1]Increases,
   literate={(}{{\CodeSymbol{(}}}1
            {)}{{\CodeSymbol{)}}}1
            {>}{{\CodeSymbol{$>$}}}1
            {>=}{{\CodeSymbol{$\ge$}}}1
            {<}{{\CodeSymbol{$<$}}}1
            {<=}{{\CodeSymbol{$\le$}}}1
            {=}{{\CodeSymbol{$=$}}}1
            {:}{{\CodeSymbol{$:$}}}1
            {.}{{\CodeSymbol{$.$}}}1
            {;}{{\CodeSymbol{$;$}}}1
            {/=}{{\CodeSymbol{$\ne$}}}1
            {=>}{{\CodeSymbol{$\Rightarrow$}}}1
            {->}{{\CodeSymbol{$\rightarrow$}}}1
            {<->}{{\CodeSymbol{$\leftrightarrow$}}}1
}

\newcommand{\DO}{\textsc{do-178}\xspace}
\newcommand{\DOB}{\textsc{do-178b}\xspace}
\newcommand{\DOC}{\textsc{do-178c}\xspace}
\newcommand{\hilite}{Hi-Lite\xspace}
\newcommand{\openetcs}{openETCS\xspace}
\newcommand{\gnatprove}{GNATprove\xspace}
\newcommand{\oldspark}{SPARK~2005\xspace}
\newcommand{\newspark}{SPARK~2014\xspace}
\newcommand{\spark}{SPARK\xspace}
\newcommand{\ada}{Ada\xspace}
\newcommand{\adatwtw}{Ada~2012\xspace}
\newcommand{\altergo}{Alt-Ergo\xspace}

\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\adhoc}{\textit{ad hoc}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\aka}{a.k.a.\xspace}
\newcommand{\resp}{resp.\xspace}

\urlstyle{sf}

\title{Practical Formal Verification for Reliable Software\thanks{Work partly
supported by the Joint Laboratory ProofInUse (ANR-13-LAB3-0007,
\url{http://www.spark-2014.org/proofinuse})}}

\author{Claire Dross\inst{1} \and Guillaume Foliard\inst{2} \and Lionel
  Matias\inst{2} \and Stuart Matthews\inst{3} \and Jean-Marc Mota\inst{4} \and
  Yannick Moy\inst{1} \and Romain Soulat\inst{4}}

\institute{AdaCore, F-75009 Paris \and Thales Air Systems, F-91470 Limours \and
  Altran, Bath BA1 1AN, United Kingdom \and Thales Research \& Technologies,
  F-91767 Palaiseau}

\date{}

\begin{document}
\sloppy
\hbadness=9999
\maketitle

\begin{abstract}
There is a strong link between software quality and software reliability. By
decreasing the probability of imperfection in the software, we can augment its
reliability guarantees. At one extreme, software with one unknown bug is not
reliable. At the other extreme, perfect software is fully reliable. Formal
verification with SPARK has been used for years to get as close as possible to
zero-defect software. We present the well-established processes surrounding the
use of SPARK at Altran UK, as well as the deployment experiments performed at
Thales to fine-tune the gradual insertion of formal verification techniques in
existing processes.  Experience of both long-term and new users helped us
define adoption and usage guidelines for SPARK based on five levels of
increasing assurance that map well with industrial needs in practice.
\end{abstract}

% Keywords
% Certification, Formal methods, Programming by contract

\section{Introduction}

Taken literally, reliable software is the notion that we can rely on software
to perform as intended. This is also how the international standard bodies and
academic experts define it, as phrased in IEC 60050 terms applied to software:
``reliability [is the] ability to perform as required, without failure, for a
given time interval, under given conditions''. Currently, almost no software is
reliable in this sense, because the intention is usually expressed in ambiguous
natural language, and the confidence that software behaves as intended is
obtained by a combination of development discipline (to avoid introducing
errors) and partial testing of all the possible software behaviors (to detect
errors that were introduced). Hence, reliable software today is more an
aspiration when building the software than a quality of the software produced.
However, a link between software quality and reliability does exist, and it was
clarified by researcher John Rushby~\cite{RushbySEFM2009}: ``probability of
(im)perfection [..]  provides a bridge between correctness, which is the goal
of software verification (and especially formal verification), and the
probabilistic properties such as reliability that are the targets for system
level assurance.''

This interpretation of reliable software as probably perfect software has been
taken seriously in some companies like Altran UK, where specifications are
routinely expressed with the precision 3 of a formal language, and confidence
is obtained by a combination of classical techniques plus the guarantees
provided by the use of formal verification. Tools for formal verification of
software have reached a degree of automation and usability that makes them
suitable for use in commercial contexts across a large set of industries, from
the well-established - space, railway, aerospace \& defense - to industries that
more recently included software as a critical component like automotive and
medical. The main tool used at Altran UK for formal specification, programming
and formal verification of software is SPARK, a subset of the Ada programming
language targeted at safety- and security-critical applications. The use of
SPARK allows Altran UK to provide assurance that software will not crash or
behave erratically, and that critical properties are satisfied, which it
demonstrates by committing to these properties with its customers.

While the benefits obtained by formal verification at Altran UK are clearly
desirable, it may be intimidating for companies without formal verification
know-how to start on this path. Knowing that others have replicated these
benefits in other contexts is an important argument to make.  Here, we are
describing the experiments done at Thales during the year 2016, to assess the
costs and benefits of using formal verification of software using SPARK. With
little investment in training and consulting, the operational teams managed to
specify intended behavior formally with limited efforts. The teams also proved
critical properties of their software. Specifically, in one use case the code
was fully proved to be free of run-time errors (like buffer overflows and
divisions by zero) while in another use case the code was proved to follow a
specified safety automaton.

In addition, the collaboration of AdaCore and Thales resulted in a set of
guidelines~\cite{AdaCoreThalesSPARK} that should be followed for an easier
adoption of formal verification in existing projects, codebases and
processes. These guidelines are based on five levels of assurance that can be
achieved on software, in increasing order of costs and benefits.

\section{SPARK: Formal Verification Focused on Practical Use}

For his PhD defence in 1969 on ``A Program Verifier'', J. King submitted a
manuscript that started with these sentences~\cite{King1970PhD}:

\begin{quote}
This research is a first step toward developing a ``verifying compiler''. Such
a compiler, as well as doing the standard translation of a program to machine
executable form, attempts to prove that the program is ``correct''. In order to
do this a program must be annotated with propositions in a mathematical
notation which define the ``correct'' relations among the program
variables. The verifying compiler then checks for consistency between the
program and its propositions.
\end{quote}

To most programmers, this may sound like a naive dream, whose illusory nature
is exemplified by Sir Tony Hoare’s call in 2003 for researchers to tackle the
``verifying compiler'' as a Grand Challenge, more than thirty years later. Yet,
``verifying compilers'' are available today. For example, the formal
development environments Coq and Isabelle have been used to create a compiler
for C~\cite{Leroy-backend} and a microkernel~\cite{Klein2009SOSP} which are
guaranteed to be ``correct''.

The problem is that these ``verifying compilers'' are operating on proof
languages that are reserved for experts. Since King's PhD defence, there have
been numerous attempts at defining practical ``verifying compilers'' for
programming languages used in industry (first Pascal, then Ada, more recently
Java and C\#), none of which has succeeded in gaining industrial adoption. It is
difficult to prove that a program is ``correct'', and it will remain so for the
foreseeable future. As Rustan Leino, a prominent researcher in formal program
verification, put it in 2010: ``Program verification is unusable. But perhaps
not useless.''~\cite{Leino10usableauto-active}

Departing from this academic tradition, SPARK has been focused on practical
formal verification from the start. SPARK has been adopted in numerous large
industrial projects and only critical parts of the software were proved
``correct'' with respect to full functional specification. SPARK was used to
prove specific properties of interest about the software, like the absence of
run-time errors (no division by zero, no buffer overflow, etc.) and
user-specified safety or security properties.

SPARK is both a language and a toolset, supported by specific development and
verification processes. In this article, we are focusing on the latest
generation of SPARK technology, called SPARK 2014~\cite{sparkERTS2014}, in
which the specification language and the programming language have been unified
as a subset of the programming language Ada 2012. Constraints on both program
data and control can be specified using respectively type contracts (predicates
and invariants) and function contracts (preconditions and
postconditions).

The concept of program contracts was invented by the researcher C.A.R. Hoare in
1969 in the context of reasoning about programs. In the mid-1980s, another
researcher, Bertrand Meyer, introduced the modern function contract and type
invariant in the Eiffel programming language.  In its simplest formulation, a
function contract consists of two Boolean expressions: a precondition to
specify input constraints and a postcondition to specify output constraints.
Function contracts have subsequently been included in many other languages,
either as part of the language (such as CodeContracts for .NET or contracts for
SPARK) or as an annotation language (such as JML for Java or ACSL for C). Type
invariants may come in two forms, depending on whether they can be temporarily
violated (type invariants in SPARK) or not (type predicates in
SPARK). Contracts can be executed as runtime assertions, interpreted as logic
formulas by analysis tools, or both.

This design choice has far-reaching consequences. First, specifying properties
of programs is similar to programming: there is no additional language to learn
and the tools available to the programmer also work on specifications. Second,
contracts are executable, which means that they can be tested and debugged like
code. Another important design choice was to allow SPARK and Ada code to
coexist in the same files. Hybrid verification is obtained by using proof on
SPARK code and test on Ada code. This is possible because contracts can be
executed, and because test and proof use the exact same semantics for
contracts~\cite{tseChalin10}. Other formal program verification technologies
like Frama-C for C programs have made similar
choices~\cite{kosmatov:hal-01344110}.

SPARK toolset focuses on automation and usability. Generation of implicit
specifications lowers the cost of writing specifications, and generation of
loop invariants, use of multiple state-of-the-art automatic provers, possible
generation of counterexamples when proof fails, combination of static analysis
and proof, all lower the cost of proof by reducing the time and effort required
to prove that the code respects its contracts. Usability is similar to other
tools in the developer’s toolbox, mostly because formal verification can be
performed by developers while they are developing, using their personal
computers, thanks to the modularity and parallelization of the analysis.

We identify five levels of assurance that can be achieved with SPARK, which are
- in increasing order:

\begin{enumerate}
\item Stone level - valid SPARK
\item Bronze level - initialization and correct data flow
\item Silver level - absence of run-time errors (AoRTE)
\item Gold level - proof of key properties
\item Platinum level - full functional correctness
\end{enumerate}

At Stone level, strict SPARK rules are enforced on the code, which leads to
better quality and maintainability. At Bronze level, the SPARK code is
guaranteed to be free from a number of defects like reads of uninitialized
variables. At Silver level, the SPARK code is guaranteed to be free of run-time
errors. At Gold level, the SPARK code is guaranteed to respect key integrity
properties. At Platinum level, the SPARK code is guaranteed to implement a
complete specification of intended behavior. Note that each level builds on the
previous one, so that at Platinum level the guarantees given by all the lower
levels are also achieved.

\section{The Practice of Formal Verification}

Altran UK has a special relationship with the SPARK technology, being the heir
of both PVL and then Praxis, the companies which have developed SPARK since
1987~\cite{Chapman2014}. Along the years, Altran has used SPARK both directly
and in partnership with our customers - through training, support and
consulting - in a number of project domains which range across air traffic
management, airborne systems, avionics, railway control \& protection, security
and defence systems.

SPARK is used at Altran as an efficient means to both get as close as possible
to zero-defect software and as a means to address the objectives of the
relevant standards. This technical strategy has been subject to careful
evaluation of costs and benefits, in order to apply formal verification where
it brings more value to the business. At Altran UK, SPARK fits within an
overall software development philosophy known as Correctness By
Construction~\cite{Croxford2005Manifesto}. The key principles of this approach
are:

\begin{itemize}
\item to use techniques that prevent the introduction of errors (e.g. language
  subsets);
\item to maximize the ability to detect defects early (e.g. through the use of
  formal techniques);
\item to generate assurance evidence as you progress.
\end{itemize}

The detail of how SPARK is applied varies from project to project, depending on
factors which include the required integrity level, applicable standards, and
the overall verification strategy for the system (in which SPARK will play a
part amongst other technique and tools). Together, these considerations will
lead to a set of verification objectives for SPARK, which will be documented in
the technical plan at the start of a project (and which in turn support the
assurance case either implicitly or explicitly if it is a formal deliverable).

In spite of these variations, we can identify certain typical ways in which
SPARK is applied on projects which have been shown to deliver high value in
relation to the effort required. The table in Figure~\ref{fig:levels}
summarizes our experience of how best to apply the different assurance levels
possible with SPARK vs. the relative design integrity level of the software
under development.  Stone level is not represented as it is more an
intermediate level during adoption of SPARK than a target assurance level.

\begin{figure}

\begin{center}
\begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} \hline
\multicolumn{2}{|c|}{Software Integrity Level} & \multicolumn{4}{c|}{SPARK Verification Objective} \\  \hline
DAL & SIL & Bronze & Silver & Gold & Platinum \\ \hline
A   & 4   &        & \cellcolor{OliveGreen} & \cellcolor{OliveGreen} & \cellcolor{OliveGreen} \\ \hline
B   & 3   &        & \cellcolor{OliveGreen} & \cellcolor{OliveGreen} & \cellcolor{YellowGreen} \\ \hline
C   & 2   &        & \cellcolor{NavyBlue} & \cellcolor{NavyBlue} & \\ \hline
D   & 1   &        & \cellcolor{NavyBlue} & \cellcolor{TealBlue} & \\ \hline
E   & 0   & \cellcolor{Orchid} & \cellcolor{Fuchsia} & & \\ \hline
\end{tabular}
\end{center}

\caption{Technical Planning Guidelines for the Application of SPARK.  The
  colored cells denote the typical application of the objectives with the
  darker colours showing the most common use cases.}
\label{fig:levels}
\end{figure}

The way to understand this table is as both a summary of our experience on
industrial projects at Altran UK and as a starting point for how we approach
new projects. Every project at Altran will tailor its own approach. However, we
would expect new projects to fall within the typical region(s) indicated in the
table; any which did not would require justification in the planning phase.

We have chosen to represent the software integrity level using two commonly
understood scales: DAL (Design Assurance Level) is the terminology from DO-178
and SIL (Software Integrity Level) is the terminology used in DEFSTAN 00-55,
IEC 61508, EN 50128 et al. The correspondence between DAL and SIL is
necessarily informal because different standards define the levels according to
different criteria. Note also that while DAL-E is defined by DO-178 its
counterpart ``SIL-0'' is an informal but widely used term taken here to mean
software below SIL-1 but which is still well-engineered.

Our experience groups projects into three broad categories, shown by the three
colored regions in the table. Category 1, shown in green, represents our
practice at the highest levels of integrity. Within this category, Silver
(AoRTE proof) is considered the ``default level'', but may be increased to Gold
or even Platinum depending on whether key properties and functional correctness
respectively are verified by other means. Targeting Platinum (full functional
proof) becomes less likely for a SIL-3 system where verification by testing
could more easily be argued to be sufficient.

Category 2, shown in blue, captures our practice at medium levels of
integrity. Silver is still the default level, and it is very unlikely that
Platinum would be employed on systems below SIL-3.  However, proof of key
properties (Gold), should still be strongly considered. There may be some key
property where proof represents a very efficient means of verification, i.e. it
is relatively easy to prove and relatively difficult to verify by any other
means. The nature of such properties will vary from system to system, but could
include even one key safety property (``the lift will not move when the doors
are open'') or security property (``the account details cannot be accessed when
the user is not logged in''). While testing can provide some level of
confidence in such properties it can never provide a complete guarantee for any
realistically-sized system, due to the impossibility of covering all possible
states and input combinations.

Category 3, shown in purple, represents the lowest levels of integrity. Even
here, we would regard Silver as the default objective, but would consider that
this could be weakened to Bronze if we had confidence that AoRTE was being
sufficiently-well assured by other means or mitigation.

The table shows that - for all but SIL-0 software - SPARK code will as a
minimum be checked for AoRTE. Note that this level of verification implicitly
means that all SPARK code has also been shown to be free of references to
uninitialized variables and basic data flow errors.  Experience shows that the
presence of this kind of flaw - which can have far-reaching consequences - can
be immensely difficult to detect by other forms of
verification~\cite{King2000TSE}.

A key part of the software engineering process which maximizes the benefit of
SPARK is a careful delineation of the "SPARK boundary" i.e. choosing which
parts of the application software will be written in SPARK. Although the
benefits of SPARK would push towards maximizing the proportion of the software
written in SPARK, other factors are likely to affect this engineering
decision. For example, there may be pre-existing libraries to support the user
interface or other external communication protocols that we wish to use and
which are qualified by alternative means. It is not unusual even to use
different levels of SPARK verification within the same application. For example
SHOLIS~\cite{Croxford2005Manifesto} used this approach with SIL-4 parts of the
application attaining full functional proof (Platinum level) while in
lower-integrity functions (SIL-2) we verified to the level of AoRTE
(Silver). The non-interference between different sections of the code was
assured by the use of information flow analysis. More generally, consideration
has to be given to the assumptions that are made to support the verification
objectives - how these are satisfied or mitigated by other activities in the
overall V\&V strategy~\cite{kanig2014tap}.

The use of SPARK within the Correctness By Construction framework as described
above has been demonstrated to produce software with very low defect density
when compared to other high-integrity
processes~\cite{Croxford2005Manifesto}. Although the above approach is the
standard approach within Altran UK, we have continued to explore new ways in
which benefits can be gained from the use of SPARK, in particular the
possibility of so-called ``hybrid'' approaches to verification, where a mixture
of static and dynamic verification techniques are used to exploit the SPARK
contracts.

The hybrid approach that Altran is currently pioneering, called ConTestor, uses
SPARK verification at Silver level i.e. assurance of AoRTE using proof. In
addition, SPARK contracts are used to add a functional specification to the
code. Rather than verifying these contracts by proof using the SPARK tools (as
per the standard Platinum approach), they are verified dynamically by
testing. To perform these tests a fully-integrated version of the code is
compiled with the run-time checks enabled for the functional contracts. Test
cases for the integrated code are generated using constrained-random test
generation and if no exceptions are raised during execution then the code has
passed this functional test. The contracts effectively provide a test oracle
i.e. an independent calculation of the expected outcome for each test
case. However, rather than having to manually calculate the expected outcomes
per test case, the contracts are written once and provide an implicit
definition of the expected outcome for all possible test cases.

\section{The Adoption of Formal Verification}

One trait of established industrial software development processes is their
inertia in accepting new practices which could be considered as too disrupting,
either by lack of understanding and know-how or, mostly for early adopters,
because of the difficulty to assess costs and benefits. In the latter case, the
upfront adoption effort is hiding the longer term process optimisation
opportunities. In order to get a first idea of the possibilities SPARK-based
formal verification could provide at Thales, a study was planned with the aim
of producing a first set of deployment guidelines supported by real life
experiments on actual software applications.

As formal verification with SPARK is not a widespread technology in the
software industry, a prerequisite is to picture the range of its capabilities
with a simple to remember concept. This led to the definition of the five
levels of assurance previously introduced. As part of this study, AdaCore and
Thales wrote a guidance document describing how SPARK could be adopted at these
different levels. This is further conditioned by the phase of the software
development lifecycle, which has a significant impact on the definition of
activities to be performed when deploying SPARK.

Given the current state of progress of some ongoing software development
projects, four use cases were identified as potential targets for SPARK
deployment experiments, from teams working on air defence systems software.

The first use case meant to assess the effort for transitioning from Ada to
SPARK code (Stone level) using a mature software application about to be ported
onto a new execution platform. As porting the application on a new platform
using a different compiler may introduce a different behavior in case of errors
such as references to uninitialized variables, reaching the Bronze level seemed
a desirable aim. A significant refactoring effort was required in order to cope
with constructs excluded from the SPARK subset of the Ada language, the most
prominent one being pointers. We started using successfully the refactoring
solutions described in the guidance document, but did not manage to complete
refactoring in the expected time frame (5 person-days), due to the size of the
chosen code base (around 300 klocs). This is expected to be completed in the
coming year.

The second use case focused on programmer proficiency. In that use case a small
subprogram of less than 10 lines of code was given to an experienced Ada
programmer with the goal of performing validation activities, both using the
usual unitary test approach, and a contract-based approach. Based on current
tools, it took less than one hour for the experienced software test engineer to
set up a working test environment for the subprogram. On the other hand,
writing relevant contracts on that same subprogram to formally prove properties
took an order of magnitude more time for the same engineer. Interestingly, the
amount of code to implement a contract was in that case as long and complex as
the code to prove. As we learned from SPARK experts, numerical computations are
currently a difficult nut to crack for provers. As a consequence, we do not
intend to invest in Gold level verification on numerical computations in the
near future. We also learned that we should start with the lowest levels of
assurance and work upwards, as practiced in the subsequent use cases.

The third use case was designed to complement test result artifacts on
automatically generated code. A large amount of our software application source
code relating to data binary serialization and deserialization is automatically
generated. The code generator compiles data models described through a domain
specific language of our own domain specific language into Ada code. Up to now
the test strategy for the code generator was mostly based on a limited set of
regression tests and the confidence acquired over time as we deployed this
technology across many projects over the last fifteen years. However, a hard to
trigger weakness was lying dormant, which we chose to clean up using a Gold
level approach. With the support of SPARK experts, a first stage was to correct
and refactor the code (2 kloc for the runtime and 21 kloc of generated code) to
pass Stone, Bronze and Silver levels. For code written by savvy programmers
making a moderate use of specialized language features such levels are easy
targets, in our case less than half a person-day for a few hundreds lines of
code. Reaching Gold level to prove one property related to buffer overflows
required a larger effort, two person-days in that case, in order to refactor
the code for proof, interact with automatic provers through intermediate
assertions and provide the required loop invariants. Given the extra level of
confidence regarding the robustness these changes provide, we plan to deploy
them in the next release of the code generator.

The fourth use case targeted the proof of safety properties in a context where
safety standards apply. Safety properties are usually written as “nothing bad
will ever happen” and, since their scope is usually on a large part of the
code, need to be specified at the highest level of the code, almost at the
entry point. Inside a 70 kloc control commands project, we identified a few
units (7 kloc) defining a set of high level automata where those properties
could be specified.  As a first step, we reached the Stone, Bronze, and Silver
levels on this code in less than a person-day. Then, contracts were added on
subprograms implementing the automata, mostly to express the effect of calling
each automaton, also in less than a person-day. Automatic proof was obtained
without much difficulty after that, with no need for intermediate assertions,
loop invariants or specific proof switches. The lessons learned here are that
SPARK is expressive enough for typical safety automata properties, and powerful
enough for automatic proof of such properties.

From an adoption point of view, we conclude that formal verification as
implemented by SPARK 2014 and its associated toolset can be considered as a
toolbox providing various opportunities for subsetting and tailoring. This
flexibility gives the possibility to fine-tune the gradual insertion of formal
verification techniques in existing processes, while mitigating risks both on
their efficiency from a cost and planning point of view and their ability to
output software with a defect density under control.

\section{Conclusion}

Formal program verification with SPARK has been used for years at companies
like Altran UK to get as close as possible to zero-defect software. Altran UK
has developed software engineering processes to maximize the costs-benefit
ratio of using SPARK. In particular, it has defined a mapping between levels of
use of SPARK and software assurance targets (SIL/DAL), which is used by all
projects at Altran UK. Altran UK is now investing in its use of SPARK for the
future, by investigating innovative ways to generate tests from contracts, to
combine tests and proofs and to analyze code generated from Simulink.

Other companies like Thales are starting to use SPARK to obtain similar
benefits. We have presented in this article the lessons learned at Thales on
various deployment experiments at different levels of use of SPARK. As for
every promising but complex technology, the success of its deployment is
conditioned by the pace at which adopters can climb the learning curve and
identify relevant insertion points and strategies into established development
processes. While AdaCore expertise was essential in the success of these
experiments, Thales has identified typical use cases where the methodology used
could be replicated without external help. Thales is now aiming at clarifying
how SPARK can be adapted to its internal processes. The guidance document
written as a result of Thales experiments will be published for others to start
on this path.

\paragraph*{Acknowledgements.}

\bibliographystyle{plain}
\bibliography{fm2018}

\end{document}
