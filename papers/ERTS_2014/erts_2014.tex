\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\newcommand{\CodeSymbol}[1]{\textcolor{Bittersweet}{#1}}
\lstset{
   language=Ada,
   keywordstyle=\color{RedViolet}\ttfamily\bf,
   showspaces=false,
   basicstyle={\scriptsize \sffamily},
   commentstyle=\color{red}\textit,
   stringstyle=\color{MidnightBlue}\ttfamily,
   showtabs=false,
   showstringspaces=false,
   morekeywords=[1]Pre,
   morekeywords=[1]Post,
   morekeywords=[1]Test\_Case,
   morekeywords=[1]Contract\_Cases,
   morekeywords=[1]some,
   morekeywords=[1]Old,
   morekeywords=[1]Global,
   morekeywords=[1]Depends,
   morekeywords=[1]Loop\_Invariant,
   morekeywords=[1]Loop\_Variant,
   morekeywords=[1]Loop\_Entry,
   morekeywords=[1]Increases,
   literate={(}{{\CodeSymbol{(}}}1
            {)}{{\CodeSymbol{)}}}1
            {>}{{\CodeSymbol{$>$}}}1
            {>=}{{\CodeSymbol{$\ge$}}}1
            {<}{{\CodeSymbol{$<$}}}1
            {<=}{{\CodeSymbol{$\le$}}}1
            {=}{{\CodeSymbol{$=$}}}1
            {:}{{\CodeSymbol{$:$}}}1
            {.}{{\CodeSymbol{$.$}}}1
            {;}{{\CodeSymbol{$;$}}}1
            {/=}{{\CodeSymbol{$\ne$}}}1
            {=>}{{\CodeSymbol{$\Rightarrow$}}}1
            {->}{{\CodeSymbol{$\rightarrow$}}}1
            {<->}{{\CodeSymbol{$\leftrightarrow$}}}1
}

\newcommand{\DO}{\textsc{do-178}\xspace}
\newcommand{\DOB}{\textsc{do-178b}\xspace}
\newcommand{\DOC}{\textsc{do-178c}\xspace}
\newcommand{\hilite}{Hi-Lite\xspace}
\newcommand{\openetcs}{openETCS\xspace}
\newcommand{\gnatprove}{GNATprove\xspace}
\newcommand{\oldspark}{SPARK~2005\xspace}
\newcommand{\newspark}{SPARK~2014\xspace}
\newcommand{\spark}{SPARK\xspace}
\newcommand{\ada}{Ada\xspace}
\newcommand{\adatwtw}{Ada~2012\xspace}
\newcommand{\altergo}{Alt-Ergo\xspace}

\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\adhoc}{\textit{ad hoc}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\aka}{a.k.a.\xspace}
\newcommand{\resp}{resp.\xspace}

\newcommand{\SPARK}[1]{\lstinline[language=Ada,basicstyle={\footnotesize
      \sffamily},framesep=0pt]$#1$}

\begin{document}

\title{Rail, Space, Security: Three Case Studies for SPARK 2014}

\author{%
\large Claire Dross$^4$, Pavlos Efstathopoulos$^1$, David Lesens$^2$, David Mentré$^3$ and Yannick Moy$^4$\\
\normalsize 1: Altran UK Limited, 22 St Lawrence Street, Bath BA1 1AN (United Kingdom),\\
\normalsize 2: Astrium Space Transportation, 51-61 route de Verneuil F-78130 Les Mureaux (France),\\
\normalsize 3: Mitsubishi Electric R\&D Centre Europe, 1 allée de
Beaulieu, CS 10806, F-35708 Rennes (France),\\
\normalsize 4: AdaCore, 46 rue d'Amsterdam, F-75009 Paris (France)}

\date{}

\maketitle

\paragraph{Abstract}
SPARK is a subset of the Ada programming language targeted at safety
and security critical applications. \newspark is a major evolution of
the SPARK language and toolset, that integrates formal program
verification in the existing development and verification processes,
in order to decrease the cost of verification for software subject to
certification constraints. We present industrial case studies in three
different certification domains that show the benefits of using formal
verification with \newspark.

\paragraph{Keywords}
System formal development, Verification and validation,
Certification and dependability

\section{Introduction}

\newspark is a major evolution of the SPARK subset of Ada and associated formal
verification toolset with two main objectives:
%
\begin{enumerate}
\item being accessible to non-expert users, and
\item being compatible with testing.
\end{enumerate}

In this paper, we describe how \newspark achieves these objectives, based on
practical use of the language and associated formal verification tool
\gnatprove on three industrial case studies developed in the context of the
\hilite\footnote{\url{http://www.open-do.org/projects/hi-lite/}} and
\openetcs\footnote{Part of this work was funded by the DGCIS (Grant
No. 112930309) in the context of the ITEA2 project \openetcs \url{http://openetcs.org/}.} research projects. The first
case study by Mitsubishi Electric R\&D Centre Europe evaluates the use of Ada
and SPARK on a small subset of requirements for the European Train Control
Systems. The second case study by Astrium Space Transportation is an extensive
evaluation of the technology over a period of three years, consisting in the
re-development of various parts of a Flight Control and Vehicle Management
software. The third case study by Altran compares the benefits of the new
technology \wrt previous version, on the well-known Tokeneer case study in
SPARK.

\section{SPARK 2014}

%%\subsection{SPARK: Past and Present}

SPARK is a subset of the Ada programming language targeted at safety and
security critical applications. SPARK builds on the strengths of Ada for
creating highly reliable and long-lived software. SPARK restrictions ensure
that the behavior of a SPARK program is unambiguously defined, and simple
enough that formal verification tools can perform an automatic diagnostic of
conformance between a program specification and its implementation. The SPARK
language and toolset for formal verification has been applied for many years in
on-board aircraft systems, control systems, cryptographic systems, and rail
systems~\cite{sparkbook2012,oneill2012}. The new version \newspark builds on
the new specification features added in \adatwtw~\cite{ada2012rationale}, so
formal specifications are now understood by the usual development tools and can
be executed.

\subsection{Key Language Features}

The most useful feature in \newspark is the ability to specify a contract on
subprograms. Subprogram contracts were popularized in the Design-by-Contract
approach~\cite{meyer:1988:OSC} as a means to separate responsibilities in
software between a caller and a callee. The callee's \textit{precondition}
states the responsibility of its caller, while the callee's
\textit{postcondition} states the responsibility of the callee itself.  For
example, the following contract for procedure \SPARK{Swap} specifies that it
should be called with index parameters within the range of the array parameter,
and that \SPARK{Swap} will ensure on return that the corresponding values in
the array have been swapped. Attribute \SPARK{Old} in the postcondition is used
to refer to values on entry to the subprogram.

\begin{lstlisting}
procedure Swap (A : in out Arr; X, Y : Idx) with
  Pre  => X in A'Range and Y in A'Range,
  Post => A(X) = A(Y)`Old and A(Y) = A(X)`Old;
\end{lstlisting}

%% In \newspark, subprogram contracts may additionally specify global variables
%% read and written in the subprogram, as well the flow of information from inputs
%% (parameters and global variables read) to outputs (parameters and global
%% variables written). For example, the following contract for procedure
%% \SPARK{Swap} specifies that the subprogram writes global variable
%% \SPARK{Backup}, that the value of \SPARK{Backup} on return depends only on the
%% input value of \SPARK{A}, and that the value of \SPARK{A} on return depends on
%% the input values of all parameters.

%% \begin{lstlisting}
%% procedure Swap (A : in out Arr; X, Y : Idx) with
%%   Global  => (Output => Backup),
%%   Depends => (Backup => A,
%%               A     => (A, X, Y));
%% \end{lstlisting}

%% Instead of preconditions and postconditions, or in addition to them, subprogram
%% contracts may be specified by a set of disjoint and complete cases. For
%% example, the following contract for procedure \SPARK{Swap} states separate
%% sub-contracts for the cases where the elements at indexes \SPARK{X} and
%% \SPARK{Y} are equal or different. The first case specifies that, if
%% \SPARK{A(X)} equals \SPARK{A(Y)} on entry, then \SPARK{A} should not be
%% modified by the call. The second case specifies that, if \SPARK{A(X)} is
%% different from \SPARK{A(Y)} on entry, then \SPARK{A} should be modified by the
%% call.

%% \begin{lstlisting}
%% procedure Swap (A : in out Arr; X, Y : Idx) with
%%   Contract_Cases =>
%%     (A(X) = A(Y) => A = A'Old,
%%      A(X) /= A(Y) => A /= A'Old);
%% \end{lstlisting}

New expressions make it easier to express contracts. If-expressions and
case-expressions are the expression forms which correspond to the usual
if-statements and case-statements. Note that an if-expression without else-part
\SPARK{(if A then B)} expresses a logical implication of \SPARK{B} by
\SPARK{A}. Quantified expressions \SPARK{(for all X in A)} and \SPARK{(for some
  X in A)} correspond to the mathematical universal and existential
quantifications, only on a bounded domain. Expression functions define a
function with a single expression, like in functional programming languages. As
expression functions can be part of the specification of programs (contrary to
regular function bodies), they provide a powerful way to abstract complex parts
of contracts.

The second most useful feature in \newspark (after contracts) is the ability to
specify properties of loops. A loop invariant expresses the cumulated effect
of the loop up to that point. For example, the following loop invariant
expresses that the array \SPARK{A} has been zeroed out up to the current loop
index \SPARK{J}, and that the rest of the array has not been modified.
Attribute \SPARK{Loop_Entry} is used to refer to values on entry to the loop.

\begin{lstlisting}
pragma Loop_Invariant
  (for all K in A'Range =>
    (if K <= J then
       A(K) = 0
     else
       A(K) = A'Loop_Entry(K)));
\end{lstlisting}

A loop variant expresses that a quantity varies monotonically at each iteration
of the loop, which can be used to show loop termination. For example, the
following loop variant expresses that scalar variable \SPARK{J} increases at
each loop iteration.

\begin{lstlisting}
pragma Loop_Variant (Increases => J);
\end{lstlisting}

\subsection{Benefits of Executable Contracts}
\label{subsec:ExecutableContracts}

Traditionally, contracts have been interpreted quite differently depending on
whether they were used for run-time assertion checking or for formal program
verification. For run-time assertion checking, contracts have been interpreted
as assertions on entry and exit of subprograms. For formal program
verification, assertions have typically been interpreted as formulas in
classical first-order logic. This was the situation with SPARK prior to
\newspark. Practitioners have struggled with this interpretation, which was not
consistent with the run-time assertion checking semantics~\cite{tseChalin10}.

\newspark reconciles the logic semantics and executable semantics of contracts,
so users can now execute contracts, debug them like code, and test them when
formal verification is too difficult to achieve. Furthermore, by keeping the
annotation language the same as the programming language, users don't have to
learn one more language.

% Except for the Global and Depends contracts,
All the previously presented contracts and assertion
pragmas lead to run-time assertions. If a given property
is not satisfied at run time, an exception is raised with a message indicating
the failing property, for example on the procedure \SPARK{Swap}:

\begin{footnotesize}
\begin{verbatim}
failed precondition from swap.ads:4
\end{verbatim}
\end{footnotesize}

%% failed postcondition from swap.ads:5
%% contract cases overlap for subprogram swap
%% swap.ads:5 contract cases incomplete
%% failed contract case at swap.ads:7
%% Loop_Invariant failed at swap.adb:5
%% Loop_Variant failed at swap.adb:4

Another key benefit of executable contracts is that they can be used by other
tools working at the level of code. For example, the CodePeer~\cite{codepeer}
static analysis tool uses contracts and assertion pragmas to issue more precise
messages. Most notably, this allows also combining the results of formal
verification and testing, when only part of a program is formally
analyzed~\cite{hiliteERTS2012}.

\subsection{Key Tool Features}

\gnatprove is the formal verification tool that analyzes \newspark code. It
performs two different analyses:

\begin{enumerate}
\item flow analysis of the program;
\item proof of program properties.
\end{enumerate}

Flow analysis checks correct access to data in the program: correct access to
global variables
% (as specified in Global and Depends contracts)
and correct
access to initialized data. It is a fast static analysis (analysis time
typically comparable with compilation time) based on the computation of the
Program Dependence Graphs~\cite{Horwitz:1988:ISU:53990.53994}.

Proof is used to demonstrate that the program is free from run-time errors, and
that the specified contracts are correctly implemented. It internally generates
mathematical formulas for each property, that are given to the automatic prover
\altergo\footnote{\url{http://alt-ergo.ocamlpro.com/}}. If \altergo manages to
prove the formula in the given time, then the property is known to
hold. Otherwise, more work is required from the user to understand why the
property is not proved.

As proof requires interactions between the user and the tool until the
specification can be proved automatically, the efficiency and the granularity
at which the tool can be applied are critical. For efficiency, \gnatprove uses
a compilation-like model, where only parts that are impacted by a change need
to be reanalyzed, where the dependencies are both syntactic (coarse-grain) and
semantic (fine-grain). Also, the generation of formulas uses an efficient
algorithm~\cite{leino:2005:ipl} that avoids the possible combinatorial
explosion of the classical algorithm, and a small timeout of 1s is used by
default for the prover. For convenient interaction with the user during the
development of specifications, \gnatprove allows focusing on a single unit, a
single subprogram inside this unit, or even a single line inside this
subprogram.

A very useful feature of \gnatprove to investigate unproved properties is its
ability to display the paths in the program that lead to unproved
properties. This path can be displayed in
GPS\footnote{\url{http://www.adacore.com/gnatpro/toolsuite/gps/}} or in
Eclipse\footnote{\url{http://www.adacore.com/gnatpro/toolsuite/gnatbench/}},
the two Integrated Development Environments which support \newspark. The user
can also change the parameters of the tool to perform more precise proofs, at
the expense of longer analysis time.

%% \subsection{Combining Testing and Proof}

%% Formal methods are complementary to testing, and may find faults that
%% are not detected by testing, but they cannot establish verification
%% evidence for the target hardware. Therefore testing on the target is
%% still required. However, formal analysis of source code can be used to
%% show compliance with the low-level requirements. \DOC requires an
%% argument for property preservation between the source code and the
%% object code for those properties that have been verified formally at
%% the source level. Since formal program verification and testing are
%% complementary, we would like to use each method where it is most
%% efficient. For this we need to make sure that the combination is at
%% least as strong as testing alone.~\cite{hiliteERTS2012}.

\section{Train Control Systems}
\label{sec:openETCS}

% openETCS case study

The \openetcs European project
aims at making an open-sourced, open-proofs reference model of ETCS
(European Train Control System). ETCS is a radio-based train control
system aiming at unifying train signaling and control over all
European countries. Organized in several levels, ETCS can range from,
at Level 0, a simple ATP (Automatic Train Protection) system
monitoring train speed to, at Level 3, a fully featured radio-based
train control system where trains inform a Radio Block Centre about
their location and receive Movement Authorities, using cab signaling
instead of track-side signaling.

We made some experiments with \newspark to see if one could formalize
the ETCS System Requirement Specification (SRS, ERA UNISIG
SUBSET-026).

We should acknowledge that using \newspark for formalizing
\emph{system} requirements (and not only software requirements) is a
bit excessive and out of scope for the language. We made nonetheless
this formalization attempt for two reasons. Firstly, we wanted to give
a formal semantics to this system specification and \newspark
first-order logic used in contracts seemed suitable for
this task. The goal of this formalization was to formally verify some
properties at the specification level.  Secondly, some of
the content of the ETCS specifications is quite low-level, therefore
not as far from software requirements as one would expect.

\subsection{Description of the Software}

We made several experiments but due to space constraints we will only detail
one of them. The complete code is available
online.\footnote{\url{https://github.com/openETCS/model-evaluation/tree/master/model/GNATprove-MERCE}}

This example is the coding of step functions, or piecewise constant functions,
used in the \textit{Speed and distance monitoring} section (SRS §3.13). Such
functions are used to model for example speed restrictions along distance. One
of our main goal is to model the merge of two speed restrictions, taking at
each point the most restrictive (\ie smaller) one.

To encode step functions, we used the following data structure:
\begin{lstlisting}
type Num_Delimiters_Range is range 0 .. 10;

type Function_Range is new Natural;

type Delimiter_Entry is record
   Delimiter : Function_Range;
   Value     : Float;
end record;

type Delimiter_Values is array
  (Num_Delimiters_Range) of Delimiter_Entry;

type Step_Function_t is record
   Num_Delim : Num_Delimiters_Range;
   Step      : Delimiter_Values;
end record;
\end{lstlisting}

A step function of type \SPARK{Step_Function_t} can have up to 11 steps
separated by 10 delimiters, stored together with the initial value of the step
function (as first element of the array) in component \SPARK{Step}. Each
delimiter of type \SPARK{Delimiter_Entry} contains the delimiter position
(\SPARK{Delimiter}) and the associated constant value (\SPARK{Value}) for the
function. The number of delimiters used is stored in component
\SPARK{Num_Delim}.
We defined several subprograms that query or update step functions.
%% \begin{itemize}
%% \item \SPARK{Get_Value(SFun, X)} returns the value of step function \SPARK{SFun}
%%   at point \SPARK{X};
%% \item \SPARK{Minimum_Until_Point(SFun, X)} returns the minimum of the step
%%   function \SPARK{SFun} until point \SPARK{X};
%% \item \SPARK{Restrictive_Merge(SFun1, SFun2, Merge)} merges step functions
%%   \SPARK{SFun1} and \SPARK{SFun2} into step function \SPARK{Merge}.
%% \end{itemize}

\subsection{Formalization of Properties}

We wanted to check full functional correctness of this critical unit. We used
contracts to express the specifications of all subprograms.

For example, we specified that, given a valid (\ie with strictly increasing
delimiters) step function \SPARK{SFun} and a point \SPARK{X}, function
\SPARK{Minimum_Until_Point} returns a value of the step function~(2) where the
step function reaches its minimum on the given domain~(1).

\begin{lstlisting}
function Minimum_Until_Point
    (SFun : Step_Function_t; X : Function_Range)
                             return Float
with
Pre  => Is_Valid(SFun),
Post =>
  -- (1) Returned value is the minimum before X
  (for all i in
    Num_Delimiters_Range'First..SFun.Num_delim =>
     (if X >= SFun.Step(i).Delimiter then
      Minimum_Until_Point'Result
         <= SFun.Step(i).Value))
  and
  -- (2) Returned value is a value of the step
  --     function before X
  ((for some i in
     Num_Delimiters_Range'First..SFun.Num_delim =>
      (X >= SFun.Step(i).Delimiter
        and
      (Minimum_Until_Point'Result
        = SFun.Step(i).Value))));
\end{lstlisting}

The most complex subprogram we specified is \SPARK{Restrictive_Merge}.  We
specified that, given two valid step functions \SPARK{SFun1} and \SPARK{SFun2}
without too many delimiters, \SPARK{Restrictive_Merge} generates a step
function \SPARK{Merge} that is valid, that contains all the delimiters of
\SPARK{SFun1} and \SPARK{SFun2} and that, for each of those delimiters,
the value of the \SPARK{Merge} step function is the minimum over both input
step functions.

%% \begin{lstlisting}
%% procedure Restrictive_Merge
%%   (SFun1, SFun2 : in  Step_Function_t;
%%    Merge        : out Step_Function_t)
%% with
%% Pre =>
%%   Is_Valid(SFun1) and Is_Valid(SFun2)
%%     and
%%   SFun1.Num_Delim + SFun2.Num_Delim <=
%%     Num_Delimiters_Range'Last,
%% Post =>
%%   -- (1) Output is a valid step function
%%   Is_Valid(Merge)
%%     and
%%   -- (2) All SFun1 delimiters are valid in Merge
%%   (for all i in
%%      Num_Delimiters_Range'First..SFun1.Num_Delim =>
%%    (for some j in
%%      Num_Delimiters_Range'First..Merge.Num_Delim =>
%%       (Merge.Step(j).Delimiter
%%         = SFun1.Step(i).Delimiter)))
%%     and
%%   -- (3) All SFun2 delimiters are valid in Merge
%%   (for all i in
%%      Num_Delimiters_Range'First..SFun2.Num_Delim =>
%%    (for some j in
%%      Num_Delimiters_Range'First..Merge.Num_Delim =>
%%       (Merge.Step(j).Delimiter
%%         = SFun2.Step(i).Delimiter)))
%%     and
%%   -- (4) For each delimiter of Merge, its value is
%%   --     the minimum of SFun1 and SFun2
%%   (for all i in
%%      Num_Delimiters_Range'First..Merge.Num_Delim =>
%%    (Merge.Step(i).Value = Min
%%      (Get_Value(SFun1, Merge.Step(i).Delimiter),
%%       Get_Value(SFun2, Merge.Step(i).Delimiter))));
%% \end{lstlisting}

%% Note that the signature of \SPARK{Restrictive_Merge} already specifies that
%% \SPARK{SFun1} and \SPARK{SFun2} cannot be modified, as they are passed as
%% \SPARK{in} parameters.

Overall, the number of lines for contracts and assertion pragmas (in particular
loop invariants) is about the same as the number of lines of code. We could
have simplified the contracts by specifying that \SPARK{Is_Valid} is a type
invariant of \SPARK{Step_Function_t}, but this Ada feature is not yet supported
in \newspark, although it is planned for the future.

\subsection{Formal Verification Results}

The first goal of this experiment was to check if \newspark was
expressive enough to describe the objects of the requirements:
requirement text, transition tables, breaking curve equations, \etc
Overall, we were quite satisfied, as we were able to express most of
the requirements in a formal way. The very expressive data structures
of \newspark (records, arrays, enumerations, \etc) were very helpful
compared to other specification languages like B~Method~\cite{b-book}
(lacking usable record structures) or ACSL~\cite{acsl} for C programs
(lacking record with variant part or bounded integers).
We found that it lead to quite readable specifications.

The second goal was to evaluate the automatic proving capabilities of \newspark
on some parts of the specification. We were able to prove the complete absence
of run-time errors, plus the contracts of all subprograms except the one of
\SPARK{Restrictive_Merge}. In this procedure, the postcondition and the loop
invariant (added to be able to prove the postcondition) could not be
automatically proved by \altergo. The main reason is that the proof context is
too big and \altergo gets lost in all the possible quantifier
instantiations. We have checked that some parts of the loop invariant could be
automatically proved if the proof context was manually pruned of irrelevant
hypotheses. Moreover, we compiled and tested the contracts and assertion
pragmas, which increased our confidence in their correctness.

\subsection{Lessons Learned}

We were rather pleased by the expression capabilities of \newspark, making
specification and code writing rather easy and, more importantly, clearer for the
reader. The ability inherited from Ada to define new data types for specific
ranges, which are incompatible with other types, is crucial in this regard.

Another important finding is that the code should be written with proof in
mind. For example, function \SPARK{Minimum\_Until\_Point} was initially
unprovable, because an early exit in a loop lead to the formula
\begin{multline*}
(\forall~K. A(K-1)<A(K)) \wedge X<A(1) \\
\rightarrow (\forall~K. K>1 \rightarrow X<A(K))
\end{multline*}
which requires support for induction in the automatic prover, a feature still
missing from most automatic provers~\cite{leino:2012:vmcai}. As the early exit
was not necessary for correct behavior, it was sufficient to remove it to
achieve automatic proof with \altergo.

A third finding is that contracts that can be automatically proved are
not always the most natural contracts, \ie contracts a reviewer would
understand more easily. For example, for \SPARK{Restrictive_Merge}
procedure, we would have preferred to write that the resulting function
is the minimum of both input functions for all possible input
values:
\begin{lstlisting}
Post =>
  (for all i in Function_Range =>
   (Merge.Step(i).Value = Min
     (Get_Value(SFun1, Merge.Step(i).Delimiter),
      Get_Value(SFun2, Merge.Step(i).Delimiter))));
\end{lstlisting}
It would have been impossible to prove this contract automatically, so we wrote
instead that the resulting function is the minimum of both input functions for
all possible delimiter values, which is logically equivalent and proved
automatically:
\begin{lstlisting}
Post =>
  (for all i in
     Num_Delimiters_Range'First..Merge.Num_Delim =>
   (Merge.Step(i).Value = Min
     (Get_Value(SFun1, Merge.Step(i).Delimiter),
      Get_Value(SFun2, Merge.Step(i).Delimiter))));
\end{lstlisting}
Ideally, the tool should allow users to manually prove contracts that cannot be
proved automatically, like done for example in B~Method tools.

Our last finding, not entirely surprising, is that writing the correct
loop invariant for a complex loop is not an easy task, as we experienced
it for procedure \SPARK{Restrictive_Merge}. It can require several
hours of work of skilled people, trained in the proof environment and
the use of the automatic prover. In such cases, the ability to
compile and test the loop invariant is very useful to help ``debug''
the loop invariant.

Overall, we think that the programmer should be trained to exploit the
feedback provided by the proof environment, much like a programmer
exploits feedback of debuggers and tests to debug her program.
Moreover, as a complete proof of the program can be very costly, a proof
methodology must be defined to avoid spending too much time on proofs
that could be broken by future code changes.

\section{Flight Control and Vehicle Management in Space}
\label{sec:space}

This case study was carried in the project \hilite as a means to evaluate the
adequacy of formal verification for future space programs. Detailed results
have been published in \cite{lesens:2013:dasia}.

\subsection{Description of the Software}

A typical space applicative flight program is made up of two parts, which we
both considered in our case study:
Flight control (or more generally numerical command and control algorithm) and
Mission and Vehicle Management.

\subsubsection{Numerical Command and Control Algorithms}

Numerical command and control algorithms take as inputs floating point values,
perform some numerical computations (with the classical basic mathematical
operators such as additions, subtractions, multiplications, divisions, absolute
values, trigonometry or operations on vectors and arrays, \etc) and return
floating point results. Such algorithms generally have a retro-action loop, \ie
internal states.

It is generally not possible to define interesting functional contracts for
such code. Indeed, the functional contract of the equation
``X := A * Y + Cos (Z)''
is just itself (\ie it is not possible to specify in a more abstract way this
equation). Then, instead of defining functional contracts, the objective on
this kind of software is the proof of absence of run-time errors (such as
division by zero) and the correctness of variable ranges (such as, for
instance, a velocity shall always be between 0 and 25 km/s).

\newspark has been first experimented on a solar wing management software (for a spacecraft such as the ATV -- Automated Transfer Vehicle).
This piece of code uses a mathematical library whose implementation is not in
\newspark, implying that it could not be formally proved, but only tested.
However, the interface of this mathematical library is in \newspark. The
contracts defined on the mathematical library can then be used to prove the
application code. For example:

\begin{lstlisting}
function Sin (X : T_Float) return T_Float
with
  Pre  => X in -C_2Pi .. C_2Pi,
  Post => Sin'Result in -1.0 .. 1.0;
\end{lstlisting}

\subsubsection{Mission and Vehicle Management}

The Mission and Vehicle Management of a spacecraft is described by the ECSS (European Cooperation for Space Standardization) standard
{\it ECSS-E-ST-70-01C ``Space engineering -- Spacecraft on-board control
procedures''}.
This standard defines the general principles of an On Board Control Procedure
(OBCP). An OBCP is in practice represented by a simplified programming language
interpreted on-board the spacecraft. This interpreter is generally at the
highest level of criticality of the spacecraft. The implementation of this
interpreter in \newspark is table driven and relies greatly on rich features of Ada such as generic packages
(allowing an easy customization of the code)
and discriminants
(ensuring a strict typing of the code, even in case of heterogeneous
communication between components of the system).

%%\begin{itemize}
%%\item The generic packages allow an easy customization of the code:
%%
%%\begin{lstlisting}
%% generic
%%    -- the list of events
%%    type T_Event_Id is (<>);
%%\end{lstlisting}
%%\item The discriminants ensures a strict typing of the code, even in case of
%%heterogeneous communication between components of the system:
%%
%%\begin{lstlisting}
%% type T_Monitoring is
%%   (Simple, Window, Protected_Window);
%%
%% type T_Event_Status
%%   (Monitoring : T_Monitoring := Simple) is
%%    record
%%       Detection_Time : T_Float;
%%       case Monitoring is
%%       when Simple => null;
%%       when Window | Protected_Window =>
%%          Start_Window : T_Float;
%%          End_Window   : T_Float;
%%       end case;
%%    end record;
%%\end{lstlisting}
%%\end{itemize}

\subsection{Formalization of Properties}

The contracts defined on algorithmic code are mainly related to the ranges of
variables.

%%\begin{lstlisting}
%%   subtype T_Angle_180 is T_Float
%%   range -180 .. 180;
%%
%%   function Normalise (X : in T_Float)
%%   return T_Angle_180
%%   with Pre => (X >= -720.0) and
%%               (X <=  720.0);
%%\end{lstlisting}

The contracts defined on mission and vehicle management code have (among others)
the following objectives:

\begin{itemize}
\item Ensuring the permanent consistency of the software tables;
\item Ensuring the permanent consistency between the Mission and Vehicle Management function and other functionalities (such as, for instance, the solar wing management);
\item Ensuring that functional properties such as mutual exclusion between executions of some automated procedures are respected;
\item Ensuring the absence of run-time errors.
\end{itemize}

%%\begin{lstlisting}
%%   procedure Reset_Event
%%     (Event_Id :        T_Event_Id;
%%      Events   : in out T_Events)
%%   with
%%     Post =>
%%       Get_Status (Event_Id, Events) = Inactive)
%%       and then
%%       (for all Other_Id in T_Event_Id =>
%%         (if Other_Id /= Event_Id then
%%           Get_Status (Other_Id, Events) =
%%           Get_Status (Other_Id, Events'Old))));
%%\end{lstlisting}

A majority of the program could be formally analyzed (1505 out of 2054
subprograms), although some subprograms fell outside the boundary of \newspark.
Subprograms that could not be analyzed either used access types (\aka
``pointers'') that are outside the language (87 subprograms), or unchecked type
conversions that are not verified (377 subprograms).  The 377 subprograms using
unchecked type conversions are very small subprograms defined in a library for
reading external inputs and could be validated by intensive testing. The
remaining unanalyzed subprograms used features that are expected to be included
in a future version of the language: class wide types (53 subprograms), tagged
types (81 subprograms) and specific attributes (10 subprograms). These
correspond to Object Oriented Programming features, allowing dynamic binding of
subprograms depending on the type of objects at run time.

%% The following table provides for each component: the number of subprograms
%% fully in \newspark (``Ok''), the number of bodies not in \newspark (``BN''),
%% the number of specifications not in \newspark (``SN''), the number of bodies
%% not yet in \newspark (``BNY'') and the number of specifications not yet in
%% \newspark (``SNY'').

%% \vspace{5mm}

%% \begin{tabular}{|l|c|c|c|c|c|}
%% \hline
%% Component   & Ok  & BN  & SN & BNY & SNY \\
%% \hline
%% Library     &  15 &   0 &  0 &   0 &   0 \\
%% \hline
%% Algorithms  &  30 &   0 &  0 &   0 &   0 \\
%% \hline
%% Time        &   3 &   0 &  0 &   0 &   0 \\
%% \hline
%% Variable    &  85 &   0 &  0 &   0 &   0 \\
%% \hline
%% Variables   & 140 &   0 &  0 &   0 &   0 \\
%% \hline
%% Events      &  24 &   0 &  0 &   0 &   0 \\
%% \hline
%% Expressions & 331 &   0 &  0 &   0 &   0 \\
%% \hline
%% Parameters  &  62 &   0 &  0 &   0 &   0 \\
%% \hline
%% Units       &  76 &   0 &  0 &  13 &  13 \\
%% \hline
%% Sequences   & 192 &  28 & 15 &   3 &   3 \\
%% \hline
%% OBCP        & 547 & 447 & 30 &  13 &  13 \\
%% \hline
%% \end{tabular}

%% \vspace{5mm}

Subprograms that were analyzed were first specified with a precondition and a
postcondition. The following table provides for each
component: the number of lines of code (``Size''), the number of preconditions
(``Pre''), the number of postconditions (``Post'') and the duration of analysis
in seconds (``Duration''):

\vspace{5mm}

\begin{tabular}{|l|c|c|c|c|}
\hline
{\bf Component}   & {\bf Size} & {\bf Pre} & {\bf Post} & {\bf Duration} \\
\hline
Library     &  694 &  34 &   39 &   233 \\
\hline
Algorithms  &  795 &   8 &    2 &   653 \\
\hline
Time        &   13 &   0 &    2 &    10 \\
\hline
Variable    &  260 &  41 &   30 &   118 \\
\hline
Variables   &  438 &  43 &   31 &   274 \\
\hline
Events      &  249 &  16 &   17 &   371 \\
\hline
Expressions & 1253 &  93 &   79 &  1992 \\
\hline
Parameters  &   75 &   0 &    1 &   279 \\
\hline
Units       &  463 &  13 &    8 &  2921 \\
\hline
Sequences   &  276 &   5 &    5 &  7803 \\
\hline
OBCP        &  714 &  33 &   15 & 13705 \\
\hline
\end{tabular}

\subsection{Formal Verification Results}

All the contracts have been checked by dynamic testing. This phase is quite
classical, except for the fact that preconditions and postconditions were also
tested. Then, \gnatprove has been applied. The following table provides for each
type of checks, their number and the number proved:

\vspace{5mm}

\begin{tabular}{|l|c|c|c|}
\hline
{\bf Features}            & {\bf Checks} & {\bf Proved} & {\bf \%}  \\
\hline
division check      & 22     & 22     & 100 \\
\hline
overflow check      & 164    & 164    & 100 \\
\hline
precondition        & 1410   & 1400   & 99  \\
\hline
postcondition       & 369    & 344    & 93  \\
\hline
range check         & 232    & 194    & 87  \\
\hline
assertion           & 967    & 961    & 99  \\
\hline
index check         & 184    & 46     & 25  \\
\hline
discriminant check  & 2334   & 2327   & 99  \\
\hline
loop initialization & 14     & 12     & 86  \\
\hline
loop  preservation  & 14     & 14     & 100 \\
\hline
\end{tabular}

\vspace{5mm}

These results are globally satisfactory, a quite limited number of checks having not been proved.
All unproved checks were analyzed to determine why they were not proved, which
uncovered a few limitations of the tool. Some algorithmic functions
(\eg trigonometric functions) are not completely known by \gnatprove and then
can not be used in proofs. \gnatprove has some difficulties to take into account rounding. For example, the second assertion below is not proved:
\begin{lstlisting}
pragma Assert (Y in -11160.002 .. 11160.002);
Round_Y := Round_Closest (Y);
pragma Assert (Round_Y in -11160.0 .. 11160.0);
\end{lstlisting}
\gnatprove currently does not prove non linear equations involving floating-point values. For example, the last assertion below is not proved:
\begin{lstlisting}
pragma Assert (X in 0.0 .. 180.0);
pragma Assert (Y in -180.0 .. 0.0);
pragma Assert (Z in 0.0 .. 1.0);
pragma Assert (X + Y >= 0.0);
Result := X + Y * Z;
pragma Assert (Result in 0.0 .. 360.0);
\end{lstlisting}
\gnatprove is not yet able to verify index checks when accessing an array whose
dimension is defined by a record discriminant.
%%, for example:
%%\begin{lstlisting}
%%   subtype R is Integer range 1 .. 100;
%%   type T_Array is array (R range <>) of Boolean;
%%
%%   type T_Record (L : R) is record
%%      A : T_Array (1 .. L);
%%   end record;
%%
%%   function G (X : T_Record) return Boolean is
%%     (for all I in X.A'Range => X.A (I));
%%\end{lstlisting}
%%In function ``G'', the index check ``X.A (I)'' is not proved even if ``I'' is
%%defined in the range of ``X.A''
%%An improvement of \gnatprove is in progress in order to deal with such case.
The remaining non proved checks are due to a too complex subprogram.
These subprograms shall be split in several smaller subprograms to be proved.


\subsection{Lessons Learned}

A precise process was followed for the development of this case study:
(1) writing of the contracts of each subprogram,
(2) development of the body and of the tests,
(3) test of the software with executable contracts, and potentially fixes,
(4) formal proof of contracts.
The errors detected in the testing phase were in the code
in 50\% of the cases, and in the contracts in the remaining 50\%.

The first run of \gnatprove was often not sufficient to achieve 100\% automatic
proof. Investigating the failed proofs was sometimes very difficult:
\begin{itemize}
\item Errors in the code are generally quite easy to find (as it is
  the classical activity of an engineer);
\item Missing contracts or assertion pragmas were almost always very difficult to detect without
  the help of an expert in \newspark;
\item Conclusions on a tool limitation shall almost always be confirmed by
  a senior expert on \newspark.
\end{itemize}

The quality of the code has been dramatically improved thanks to both
executable contracts and formal proof.  The writing of contracts for testing
should become a natural activity of any software developer. Understanding why a
proof (which may seem sometimes really obvious) does not work may require the
involvement of an expert in \newspark. The writing of effective contracts for
formal proof requires a dedicated training and sometimes the involvement of an
expert in \newspark

The following needs are then expressed for \gnatprove in order to make possible
its efficient use in an industrial context: ability to prove generic units once
(instead of proving each instance like done currently), need to use a sound
model of floating-points instead of real numbers, some loop invariants
generated automatically.

\section{Biometric Access to a Secure Enclave}
\label{sec:tokeneer}
% Tokeneer case study

\subsection{Description of the Software}

Tokeneer\footnote{\url{www.adacore.com/sparkpro/tokeneer}} is a highly secure
biometric software system that was originally developed by Altran. The system
provides protection to secure information held on a network of workstations
situated in a physically secure enclave. The Tokeneer project was commissioned
by the US National Security Agency (NSA) to demonstrate the feasibility of
developing systems to the level of rigor required by the higher assurance
levels of the Common Criteria. The requirements of the system were captured
using the Z notation and the implementation was in \oldspark. The original
development artefacts, including all source code, are publicly available.

During this study, the source code for Tokeneer has been translated
into \newspark. The core system now consists of approximately 10,000
lines of \newspark code. There are also approximately 3,700 lines of
supporting code written in Ada which mimick the drivers to
peripherals connected to the core system.

\subsection{Converting \oldspark to \newspark}

For the majority of the code, translating \oldspark to \newspark was
very straight forward since most of the original annotations map
directly to the new ones. This section will focus on the more
interesting occasions where the conversion was non-trivial.

Often, it is convenient to introduce a function which exists
solely for the sake of proof and does not contribute at all in
the final executing program. The constructs which achieve this
functionality for \oldspark and \newspark respectively are proof
functions and ghost functions. In \oldspark, the behavior of a proof
function was defined with user rules, which are axioms expressed in a special
syntax and given to the proof system. In \newspark, this
behavior is simply given by the body of the ghost function.

The Global aspect has an additional mode \SPARK{Proof\_In} in \newspark \wrt
\oldspark. This mode is used to specify global variables of a subprogram which
appear solely within contracts and assertions. Therefore, the translation
required separating global variables of mode \SPARK{in} that were used
exclusively inside annotations (translated as \SPARK{Proof\_In} global
variables), from those that were used in code (translated as \SPARK{Input}
global variables).

Parts of the original code were specially annotated to be ignored during formal
verification, because they contained constructs which were outside the Ada
subset supported by \oldspark. As \newspark supports a bigger subset of Ada
than \oldspark, a lot of this code is now formally analyzable. The two
constructs most often encountered that fall under this category are string
concatenation and array slices.

The information that values of variables respect the constraints of their type
is available for proofs with the new toolset. This simplifies both the work of
the programmer and the code itself.  In \oldspark, it was necessary to add many
lines of assertions to repeat that the values of variables and components are
within the bounds allowed by their type, as this information was lost inside
loops.  The new tools are more sophisticated and preserve more of the context.

%% Consider the following code segment:

%% \begin{lstlisting}
%% for I in LogFileIndexT loop
%%   if LogFilesStatus(I) = Used then
%%     if UsedLogFiles.Length = 0 then
%%       UsedLogFiles.Head := LogFileIndexT'First;
%%       UsedLogFiles.LastI := LogFileIndexT'First;
%%       UsedLogFiles.Length := 1;
%%       UsedLogFiles.List(UsedLogFiles.Head) := I;
%%     else
%%       for J in LogFileIndexT
%%         range 1..UsedLogFiles.LastI
%%       loop
%%         ...
%% \end{lstlisting}

%% In \oldspark, it was necessary to add 25 lines of assertions to repeat that the
%% values of variables like \SPARK{I} and components like
%% \SPARK{UsedLogFiles.Head} are within the bounds allowed by their type, as this
%% information was lost inside loops.


%% However, at this point, it should be pointed out that since the
%% development of the Tokeneer project the \oldspark tools have greatly
%% evolved and some of the original annotations might actually now be
%% redundant.

\subsection{Formalization of Properties}

The contracts that have been added in the Tokeneer source code consist of
information flow contracts expressed as Depends aspects and functional
behavioral contracts expressed as Pre and Post aspects.

The Depends aspects (not presented previously for lack of space) facilitate
flow analysis of the code. Flow analysis detects improper initialization,
identifies ineffective assignments and ensures secure flow of information.

The Pre and Post aspects enable the prover to show that the code is
free from run-time exceptions, such as buffer overflow or
divide-by-zero and assist in proving that key security properties are
guaranteed by the implementation.

\begin{tabular}{|l|c|}
\hline
{\bf Aspect/Pragma}       & {\bf Number of occurrences}  \\
\hline
Global              & 197 \\
\hline
Refined\_Global     & 71 \\
\hline
Depends             & 202 \\
\hline
Refined\_Depends    & 40 \\
\hline
Pre                 & 28 \\
\hline
Post                & 41 \\
\hline
Assume              & 3 \\
\hline
Loop\_Invariant     & 10 \\
\hline
\end{tabular}

In order to measure the expressiveness and proving power of the
\newspark tools, a specific package, on which functional behaviour
proof had not been attempted, was selected and then fully augmented
with functional behaviour annotations. Package \SPARK{admin}, which is
the package that was augmented, contains the state of the
administrator of the system and a set of operations that
administrators can perform.

To illustrate how functional behaviour contracts were added, we will
consider subroutine \SPARK{OpIsAvailable} of package \SPARK{admin}. An
administrator can be either a \SPARK{UserOnly}, a \SPARK{Guard}, an
\SPARK{AuditManager} or a \SPARK{SecurityOfficer}. Each type of
administrator has a set of predefined operations that it is allowed to
perform. Function \SPARK{OpIsAvailable} takes as input an
administrator and a string that is read from the keyboard and
determines if this string corresponds to an operation available to the
administrator. If the operation is indeed available, then this
operation is returned, otherwise \SPARK{NullOp} is returned.

%% \begin{lstlisting}
%% function OpIsAvailable
%%   (TheAdmin : T; KeyedOp : Keyboard.DataT)
%%   return OpAndNullT;
%% --# pre IsPresent(TheAdmin);
%% --# return R =>
%% --#   (R /= NullOp ->
%% --#     (R = OverrideLock <->
%% --#       prf_rolePresent(TheAdmin) = PrivTypes.Guard));
%% \end{lstlisting}
The \oldspark postcondition served only as a test case. It
ensured that if a non null operation was returned and if that
operation was \SPARK{OverrideLock} then the administrator was of type
\SPARK{Guard}. This annotation was incomplete since it did not specify
any other kind of valid combinations of administrator types and their
corresponding operations
%%  (\eg the administrator being of type
%% \SPARK{SecurityOfficer} and the operation being
%% \SPARK{UpdateConfigData} or \SPARK{ShutdownOp})
or under which
circumstances \SPARK{NullOp} should be returned.
The \oldspark tools proved all properties associated with
the non-augmented \SPARK{admin} package but a total of 12 user rules had
to be provided. The
effort associated with proving the completed version of this
postcondition would have been significant and hence this was not attempted.

\begin{lstlisting}
function OpIsAvailable
  (TheAdmin : T; KeyedOp : Keyboard.DataT)
  return OpAndNullT;
with
  Pre  => IsPresent(TheAdmin),
  Post => (for some Op in Opt =>
             Str_Comp(KeyedOp, Op)
               and AllowedOp(TheAdmin, Op)
               and OpIsAvailable'Result = Op)
          xor OpIsAvailable'Result = NullOp;
\end{lstlisting}
This \newspark Post aspect states that we have two mutually
exclusive cases. If there exists some operation in \SPARK{Opt} that
matches the string read from the keyboard and this operation is
allowed for the current administrator then the result of function
\SPARK{OpIsAvailable} is this operation, otherwise, the result is
\SPARK{NullOp}.

\subsection{Formal Verification Results}

The original source code of Tokeneer was proven to be free of run-time
exceptions and some key security properties were proven to hold but
full functional proof was not performed on the entirety of the code.

The \newspark tools discharge all 24 verification conditions
associated with the augmented \SPARK{admin} package in a matter of
seconds.

More than 95\% of the checks that were associated with the converted
code were automatically discharged. The remaining 5\% consisted of
checks that either derived from code that was previously not
analyzable (and hence no provision was in place to assist in their
provability) or required something similar to the \oldspark user rules
to assist the prover in discharging them (this can potentially also be
achieved through utilizing a combination of pragmas Assert and
Assume). The typical cases that require additional assumptions are
those on which combinations of several non-trivial mathematical
transformations would have to be applied when performing a manual
proof.

\subsection{Lessons Learned}

Thanks to the new toolset's proving power, users are no longer
required to add intermediate cutpoints. In the Tokeneer case study,
neither pragma Assert nor pragma Assert\_And\_Cut had to be provided
to facilitate proof. However, it is hypothesised that their usage
could improve readability of the code since it would signify that a
certain property holds at a given point in the code. In addition,
should manual proof be required, the verification conditions would be
significantly smaller and hence proving them would be easier.

While augmenting package \SPARK{admin}, it was noticed that formulating
the functional behaviour annotations based on the low level Z
specifications was very intuitive. This suggests that it might be
possible for future projects to skip this step and directly provide
\newspark annotated specifications.

An interesting case of how executable semantics affect the way code
and annotations have to be written was uncovered while attempting to
prove package \SPARK{AuditLog}. Converting \oldspark annotations into
their \newspark equivalents introduced several run-time checks that
previously did not exist. The extra checks are a byproduct of the
contracts and assertions being
executable (Section~\ref{subsec:ExecutableContracts}). Some of these checks were
not automatically discharged. A more in depth investigation revealed
that an invariant was missing from the original specifications. This
invariant described the property of always using at least one log file
(\SPARK{UsedLogFile.Length >=
1})\footnote{\url{
www.open-do.org/wp-content/uploads/2013/05/Industrial_Case_Studies_Final_Report.
pdf}}.

When analyzing a single package in isolation, it was easy to
understand which verification conditions were proved and which were
not. However, when more than one files were analyzed in a single run,
due to the magnitude of the output, it became increasing complicated
to retain supervision of both individual and overall
provability. Having a tool that summarizes the results and informs
about remaining undischarged checks would greatly assist. For
instance, this would enable users to focus their efforts only on
proving packages for which 100\% automated proof was achievable and
resort to testing for the rest.

\section{Common Findings and Dissimilarities}
% We will discuss the common findings between the three case studies,
% denoting instrinsic properties of \newspark, and the dissimilarities,
% which may be explained by differences in domains, backgrounds,
% processes, \etc

Not surprisingly, the lessons learned in all three case studies partly reflect
the level of expertise in Ada and SPARK of the engineers who carried the case
studies. The first case study was a first-time experience with Ada and SPARK
from someone with extensive experience in other programming languages (C) and
formal verification tools (B~Method, Frama-C), hence the very positive feedback
on the support for rich data types. The second case study was the occasion of a
discovery of Ada 2012 and SPARK 2014 from someone with previous experience of
both Ada and SPARK, hence the emphasis on support for rich language
features. The third case study was carried in the team who developed the
previous versions of SPARK, hence the enthusiatic feedback on automatic proof
improvements.

The three case studies confirm some good properties of \newspark.
One of the principal objectives of \newspark is to offer an expressive
language for formal verification. As stated by Jonathan P. Bowen in
Ten Commandments of Formal Methods~\cite{bowen1995ten}, \emph{Thou shalt choose
an appropriate notation}. The three case studies confirm that
expressiveness is indeed perceived as an advantage of \newspark. It provides
both rich data-structures such as records and enumerations
(Section~\ref{sec:openETCS}) and support for advanced coding structures such as
generic packages and discriminants (Section~\ref{sec:space}). It is close enough
to \oldspark to allow an easy translation of annotations but also supports
constructs that were previously excluded from the language
(Section~\ref{sec:tokeneer}).

Tests remain an important means of gaining confidence in a program's
specification and implementation, even when formal methods are involved. As
stated in Bowen's ninth commandment of formal methods, \emph{Thou shalt test,
test, and test again}. Executable contracts are a key language
feature (see Section~\ref{subsec:ExecutableContracts}) of \newspark as it allows to execute
the contracts while testing. They were found to be beneficial both to increase
confidence in annotations (Section~\ref{sec:openETCS}) and to improve overall
code quality (Section~\ref{sec:space}). What is more, the constraint implied by
executability of contracts does not seem to be too important as additional
checks introduced by the verification of absence of runtime errors in contracts
on a project that had not be annotated with this constraint in mind could be
discharged (Section~\ref{sec:tokeneer}).

Another objective of \newspark was to improve the amount of proofs that could
go through automatically with respect to \oldspark. The case study in
Section~\ref{sec:tokeneer} shows that obtaining automated proofs with \newspark
requires smaller loop-invariant, as information about bounds of variables is
preserved, less user rules, as ghost functions can now have bodies, and less
user written additional assertions.

The case studies also raise some usability issues.
\newspark strives to define a subset of Ada as big as possible while remaining
amenable to formal verification. For example, pointers are excluded but a
library of containers adapted to formal specification is provided to alleviate
this restriction~\cite{dross2011correct}. The case studies show that both code
and contracts must also be adapted for formal proof to go through automatically
(Sections~\ref{sec:openETCS} and~\ref{sec:space}).

Determining why a proof doesn't succeed is a really difficult task. Even if
specific feedback can be provided by the proof environment for a failed proof
in the form of an execution trace, the results of the case-study show
that there is room for improvement in that matter. For example, the
tool could provide inputs on which the annotation is not verified.
What is more, it appears that programmers should be trained to debug their proof
and annotations (Sections~\ref{sec:openETCS} and~\ref{sec:space}).

Finely, as stated in Bowen's fourth commandment, \emph{Thou shalt have a formal
methods guru on call}. If automated formal verification of runtime errors is
achievable by an implementer, the case studies show that, as the desired
properties become more complex, their expression and their verification may
require the involvement of an expert (Sections~\ref{sec:openETCS}
and~\ref{sec:space}).

\section{Conclusion}
% We will summarize the three case studies, and present the planned future
% evolutions of \newspark.

Railway, space, and security are three domains subject to an important
certification requirement that implies costly verification processes. The three
case studies show that verification with \newspark can bring a solution to this
problem thanks to an expressive language, executable contracts, and
an improved provability. Even if there is still room for improvement in
usability, the actual tool state already allows to integrate formal proofs into
the standard developer's work-flow.


\bibliographystyle{plain}
\bibliography{erts_2014}

\end{document}


% LocalWords:  openETCS ETCS UNISIG Centre SRS
