\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\newcommand{\CodeSymbol}[1]{\textcolor{Bittersweet}{#1}}
\lstset{
   language=Ada,
   keywordstyle=\color{RedViolet}\ttfamily\bf,
   showspaces=false,
   basicstyle=\ttfamily,
   commentstyle=\color{red}\textit,
   stringstyle=\color{MidnightBlue}\ttfamily,
   showtabs=false,
   showstringspaces=false,
   morekeywords=[1]Pre,
   morekeywords=[2]Post,
   morekeywords=[3]Test\_Case,
   morekeywords=[4]Contract\_Cases,
   morekeywords=[5]some,
   morekeywords=[6]Old,
   literate={(}{{\CodeSymbol{(}}}1
            {)}{{\CodeSymbol{)}}}1
            {>}{{\CodeSymbol{$>$}}}1
            {<}{{\CodeSymbol{$<$}}}1
            {=}{{\CodeSymbol{$=$}}}1
            {:}{{\CodeSymbol{$:$}}}1
            {.}{{\CodeSymbol{$.$}}}1
            {;}{{\CodeSymbol{$;$}}}1
}

\newcommand{\DO}{\textsc{do-178}\xspace}
\newcommand{\DOB}{\textsc{do-178b}\xspace}
\newcommand{\DOC}{\textsc{do-178c}\xspace}
\newcommand{\hilite}{Hi-Lite\xspace}
\newcommand{\openetcs}{openETCS\xspace}
\newcommand{\gnatprove}{GNATprove\xspace}
\newcommand{\oldspark}{SPARK~2005\xspace}
\newcommand{\newspark}{SPARK~2014\xspace}
\newcommand{\spark}{SPARK\xspace}
\newcommand{\ada}{Ada\xspace}
\newcommand{\adatwtw}{Ada~2012\xspace}
\newcommand{\altergo}{Alt-Ergo\xspace}

\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\adhoc}{\textit{ad hoc}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\aka}{a.k.a.\xspace}
\newcommand{\resp}{resp.\xspace}

\lstset{basicstyle={\scriptsize \sffamily}}
\newcommand{\SPARK}[1]{\lstinline[language=Ada,basicstyle={\footnotesize
      \sffamily},framesep=0pt]$#1$}

\begin{document}

\title{Rail, Space, Security: Three Case Studies for SPARK 2014}

\author{%
\large Claire Dross$^4$, Pavlos Efstathopoulos$^1$, David Lesens$^2$, David Mentré$^3$ and Yannick Moy$^4$\\
\normalsize 1: Altran Praxis, 20 Manvers Street, Bath BA1 1PX (United Kingdom),\\
\normalsize 2: Astrium Space Transportation, 51-61 route de Verneuil F-78130 Les Mureaux (France),\\
\normalsize 3: Mitsubishi Electric R\&D Centre Europe, 1 allée de
Beaulieu, CS 10806, F-35708 Rennes (France),\\
\normalsize 4: AdaCore, 46 rue d'Amsterdam, F-75009 Paris (France)}

\date{}

\maketitle

\paragraph{Abstract}
SPARK is a subset of the Ada programming language targeted at safety and
security critical applications. \newspark is a major evolution of the SPARK
language and toolset, that integrates formal program verification in the
existing development and verification processes, in order to decrease the cost
of verification for software submitted to certification constraints. We present
industrial case studies in three different certification domains that show the
benefits of using formal verification with \newspark.

\paragraph{Keywords}
System formal development, Verification and validation,
Certification and dependability

\section{Introduction}

\newspark is a major evolution of the SPARK language and toolset with two main
objectives:
%
\begin{enumerate}
\item being accessible to non-expert users, and
\item being compatible with testing.
\end{enumerate}

In this paper, we describe how \newspark achieves these objectives, based on
practical use of the language and associated formal verification tool
\gnatprove on three industrial case studies developed in the context of the
\hilite and \openetcs research projects.

\section{SPARK 2014}

%%\subsection{SPARK: Past and Present}

SPARK is a programming language subset targeted at safety and security critical
applications. It is a subset of the Ada programming language, thus building on
the strengths of Ada for creating highly reliable and long-lived
software. SPARK restrictions ensure that the behavior of a SPARK program is
unambiguously defined, and simple enough that formal verification tools can
perform an automatic diagnostic of conformance between a program specification
and its implementation. The SPARK language and toolset for static verification
has been applied for many years in on-board aircraft systems, control systems,
cryptographic systems, and rail systems~\cite{sparkbook2012,oneill2012}. The
new version \newspark builds on the new specification features added in
\adatwtw~\cite{ada2012rationale}, so formal specifications are now understood
by the usual development tools and can be executed.

\subsection{Key Language Features}

The most useful feature in \newspark is the ability to specify a contract on
subprograms. Subprogram contracts were popularized in the Design-by-Contract
approach~\cite{meyer:1988:OSC} as a means to separate responsibilities in
software between a caller and a callee. The callee's \textit{precondition}
states the responsibility of its caller, while the callee's
\textit{postcondition} states the responsibility of the callee itself.  For
example, the following contract for procedure \SPARK{Swap} specifies that it
should be called with index parameters within the range of the array parameter,
and that \SPARK{Swap} will ensure on return that the corresponding values in
the array have been swapped. Attribute \SPARK{Old} in the postcondition is used
to refer to values on entry to the subprogram.

\begin{lstlisting}
procedure Swap (A : in out Arr; X, Y : Idx) with
  Pre  => X in A'Range and Y in A'Range,
  Post => A(X) = A(Y)'Old and A(Y) = A(X)'Old;
\end{lstlisting}

In \newspark, subprogram contracts may additionally specify global variables
read and written in the subprogram, as well the flow of information from inputs
(parameters and global variables reads) to outputs (parameters and global
variables written). For example, the following contract for procedure
\SPARK{Swap} specifies that the subprogram writes global variable
\SPARK{Backup}, that the value of \SPARK{Backup} on return depends only on the
input value of \SPARK{A}, and that the value of \SPARK{A} on return depends on
the input values of all parameters.

\begin{lstlisting}
procedure Swap (A : in out Arr; X, Y : Idx) with
  Global  => (Output => Backup),
  Depends => (Backup => A,
              A      => (A, X, Y));
\end{lstlisting}

Instead of preconditions and postconditions, or in addition to them, subprogram
contracts may be specified by a set of disjoint and complete cases. For
example, the following contract for procedure \SPARK{Swap} states separate
sub-contracts for the cases where the elements at indexes \SPARK{X} and
\SPARK{Y} are equal or different. The first case specifies that, if
\SPARK{A(X)} equald \SPARK{A(Y)} on entry, then \SPARK{A} should not be
modified by the call. The second case specifies that, if \SPARK{A(X)} is
different from \SPARK{A(Y)} on entry, then \SPARK{A} should be modified by the
call.

\begin{lstlisting}
procedure Swap (A : in out Arr; X, Y : Idx) with
  Contract_Cases =>
    (A(X) = A(Y)  => A = A'Old,
     A(X) /= A(Y) => A /= A'Old);
\end{lstlisting}

New expressions make it easier to express contracts. If-expressions and
case-expressions are the expression forms which correspond to the usual
if-statements and case-statements. Note that an if-expression without else-part
\SPARK{(if A then B)} expresses a logical implication of \SPARK{B} by
\SPARK{A}. Quantified expressions \SPARK{(for all X in A)} and \SPARK{(for some
  X in A)} correspond to the mathematical universal and existential
quantifications, only on a bounded domain. Expression functions define a
function with a single expression, like in functional programming languages. As
expression functions can be part of the specification of programs (contrary to
regular function bodies), they provide a powerful way to abstract complex parts
of contracts.

The second most useful feature in \newspark (after contracts) is the ability to
specify properties of loops. A loop invariant expresses the cumulated effect
of the loop up to that point. For example, the following loop invariant
expresses that the array \SPARK{A} has been zeroed out up to the current loop
index \SPARK{J}, and that the rest of the array has not been modified.
Attribute \SPARK{Loop_Entry} is used to refer to values on entry to the loop.

\begin{lstlisting}
pragma Loop_Invariant
  (for all K in A'Range =>
    (if K < J then A(K) = 0
     else A(K) = A'Loop_Entry(K)));
\end{lstlisting}

A loop variant expresses that a quantity varies monotonically at each iteration
of the loop, which can be used to show loop termination. For example, the
following loop variant expresses that scalar variable \SPARK{J} increases at
each loop iteration.

\begin{lstlisting}
pragma Loop_Variant (Increases => J);
\end{lstlisting}

\subsection{Benefits of Executable Contracts}

Traditionally, contracts have been interpreted quite differently depending on
whether used for formal program verification or for run-time assertion
checking. For formal program verification, assertions have typically been
interpreted as formulae in classical first-order logic. This was the situation
with SPARK until version \oldspark. Practitioners have struggled with this
interpretation, which was not consistent with the run-time assertion checking
semantics.\cite{tseChalin10}

\newspark reconciles the logic semantics and executable semantics of contracts,
so users can now execute contracts, debug them like code, and test them when
formal verification is too difficult to achieve. Furthermore, by keeping the
annotation language the same as the programming language, users don't have to
learn one more language.

Except for the Global and Depends contracts, all the contracts and assertion
pragmas presented previously lead to run-time assertions. If a given property
is not satisfied at run time, an exception is raised with a message indicating
the failing property, for example on the procedure \SPARK{Swap}:

\begin{verbatim}
failed precondition from swap.ads:4
failed postcondition from swap.ads:5
contract cases overlap for subprogram swap
swap.ads:5 contract cases incomplete
failed contract case at swap.ads:7
Loop_Invariant failed at swap.adb:5
Loop_Variant failed at swap.adb:4
\end{verbatim}

\subsection{Key Tool Features}

\gnatprove is the formal verification tool that analyzes \newspark code. It
performs two different analyses:

\begin{enumerate}
\item flow analysis of the program;
\item proof of program properties.
\end{enumerate}

Flow analysis checks correct access to data in the program: first, correct
access to global variables (as specified in Global and Depends contracts), and
second, correct access to initialized data. It is a fast static analysis
(typically comparable with compilation time) based on the computation of the
Program Dependence Graphs~\cite{Horwitz:1988:ISU:53990.53994}.

Proof checks that the program is free from run-time errors, and that the
implementation respects the specified contracts. It internally generates
mathematical formulas for each property, that are given to the automatic prover
\altergo. If \altergo manages to prove the formula in the given time, then the
property is known to hold. Otherwise, more work is required from the user to
understand why the property is not proved. In our experience, a very useful
feature of \gnatprove is its ability to display the paths in the program that
lead to unproved properties. This path can be displayed in GPS or in Eclipse,
the two Integrated Development Environments which support \newspark. The user
can also change the parameters of the tool to perform more precise proofs, at
the expense of longer running time.

%% \subsection{Combining Testing and Proof}

%% Formal methods are complementary to testing, and may find faults that
%% are not detected by testing, but they cannot establish verification
%% evidence for the target hardware. Therefore testing on the target is
%% still required. However, formal analysis of source code can be used to
%% show compliance with the low-level requirements. \DOC requires an
%% argument for property preservation between the source code and the
%% object code for those properties that have been verified formally at
%% the source level. Since formal program verification and testing are
%% complementary, we would like to use each method where it is most
%% efficient. For this we need to make sure that the combination is at
%% least as strong as testing alone.~\cite{hiliteERTS2012}.

\section{Train Control Systems}

% openETCS case study

The \openetcs\footnote{\url{http://openetcs.org/}} European project
aims at making an open-sourced, open-proofs reference model of ETCS
(European Train Control System). ETCS is a radio-based train control
system aiming at unifying train signaling and control over all
European countries. Organized in several levels, ETCS can range from,
at Level 0, a simple ATP (Automatic Train Protection) system
monitoring train speed to, at Level 3, a fully featured radio-based
train control system where trains inform a Radio Block Centre about
their location and receive Movement Authorities, using cab signaling
instead of track-side signaling.

We made some experiments with \newspark to see if one could formalize
the ETCS System Requirement Specification (SRS, ERA UNISIG
SUBSET-026).

We should acknowledge that using \newspark for formalizing
\emph{system} requirements (and not only software requirements) is a
bit excessive and out of scope for the language. We made nonetheless
this formalization attempt for two reasons. Firstly, we wanted to give
a formal semantics to this system specification and \newspark
first-order logic used in contracts seemed suitable for
this task. The goal of this formalization was to formally verify some
properties at the specification level.  Secondly, some of
the content of the ETCS specifications is quite low-level, therefore
not as far from software requirements as one would expect.

\subsection{Description of the Software}

We made several experiments but due to space constraints we will only
detail one of them.

This example is the coding of step functions, \aka functions
constant by interval, used in the \textit{Speed and distance monitoring}
section (SRS §3.13). Such functions are used to model for example
speed restrictions along distance. One of our main goal is to model
the merge of two speed restrictions, taking at each point the most
restrictive (\ie smaller) one.

To encode step functions, we used the following data structure:
\begin{lstlisting}
type Num_Delimiters_Range is range 0 .. 10;

type Function_Range is new Natural;

type Delimiter_Entry is record
   Delimiter : Function_Range;
   Value     : Float;
end record;

type Delimiter_Values is array
  (Num_Delimiters_Range) of Delimiter_Entry;

type Step_Function_t is record
   Num_Delim : Num_Delimiters_Range;
   Step      : Delimiter_Values;
end record;
\end{lstlisting}

A step function of type \SPARK{Step_Function_t} can have up to 11
steps (stored in array \SPARK{Step}) separated by 10 delimiters (in
\SPARK{Num_Delimiters_Range},
delimiter 0 being the initial value of the step function). Each
delimiter of type \SPARK{Delimiter_Entry} contains the delimiter
position (\SPARK{Delimiter}) and the associated constant value
(\SPARK{Value}) for the function. The currently used number of
delimiters is stored in \SPARK{Num_Delim}.

We defined several subprograms that query or update step functions:
\begin{itemize}
\item \SPARK{Get_Value(SFun, X)} returns the value of step function \SPARK{SFun}
  at point \SPARK{X};
\item \SPARK{Minimum_Until_Point(SFun, X)} returns the minimum of the step
  function \SPARK{SFun} until point \SPARK{X};
\item \SPARK{Restrictive_Merge(SFun1, SFun2, Merge)} merges step functions
  \SPARK{SFun1} and \SPARK{SFun2} into step function \SPARK{Merge}.
\end{itemize}

\subsection{Formalization of Properties}

We wanted to check full functional correctness of this critical unit. We used
contracts to express the specifications of all subprograms.

For example, in following function \SPARK{Minimum_Until_Point}, we
specified that given a valid (\ie with strictly increasing delimiters)
step function \SPARK{SFun} and a point \SPARK{X}, the returned value
(\SPARK{Minimum_Until_Point'Result}) is the minimum until point \SPARK{X} (1)
and effectively belongs to the domain of the step function (2).

\begin{lstlisting}
function Minimum_Until_Point
    (SFun : Step_Function_t; X : Function_Range)
                             return Float
with
  Pre => Is_Valid(SFun),
Post =>
-- (1) Returned value is the minimum until point X
  (for all i in
    Num_Delimiters_Range'First..SFun.Num_delim =>
     (if X >= SFun.Step(i).Delimiter then
      Minimum_Until_Point'Result
         <= SFun.Step(i).Value))
  and
-- (2) Returned value is a value of the step
--     function until point X
  ((for some i in
     Num_Delimiters_Range'First..SFun.Num_delim =>
      (X >= SFun.Step(i).Delimiter
       and
         (Minimum_Until_Point'Result
             = SFun.Step(i).Value))));
\end{lstlisting}

Our next example is more complex.
In procedure \SPARK{Restrictive_Merge} we specified that, given two
valid step functions \SPARK{SFun1} and
\SPARK{SFun2} without too many delimiters, the resulting step function is a
valid one (1), it contains all the delimiters of \SPARK{SFun1} (2) and
\SPARK{SFun2} (3) and for each of those delimiters, the value of the step
function is the minimum of both initial step functions (4).

\begin{lstlisting}
procedure Restrictive_Merge
  (SFun1, SFun2 : in  Step_Function_t;
   Merge        : out Step_Function_t)
with
Pre =>
  Is_Valid(SFun1) and Is_Valid(SFun2)
    and
  SFun1.Num_Delim + SFun2.Num_Delim <=
    Num_Delimiters_Range'Last,
Post =>
  -- (1) Output is a valid step function
  Is_Valid(Merge)
    and
  -- (2) All SFun1 delimiters are valid in Merge
  (for all i in
     Num_Delimiters_Range'First..SFun1.Num_Delim =>
   (for some j in
     Num_Delimiters_Range'First..Merge.Num_Delim =>
      (Merge.Step(j).Delimiter
        = SFun1.Step(i).Delimiter)))
    and
  -- (3) All SFun2 delimiters are valid in Merge
  (for all i in
     Num_Delimiters_Range'First..SFun2.Num_Delim =>
   (for some j in
     Num_Delimiters_Range'First..Merge.Num_Delim =>
      (Merge.Step(j).Delimiter
        = SFun2.Step(i).Delimiter)))
    and
  -- (4) For each delimiter of Merge, its value is the
  --     minimum of SFun1 and SFun2
  (for all i in
     Num_Delimiters_Range'First..Merge.Num_Delim =>
   (Merge.Step(i).Value
    = Min(Get_Value(SFun1, Merge.Step(i).Delimiter),
          Get_Value(SFun2, Merge.Step(i).Delimiter))));
\end{lstlisting}

Please also notice that through the use of \SPARK{in} and \SPARK{out}
parameters, only \SPARK{Merge} can be written to and \SPARK{SFun1} and
\SPARK{SFun2} should be kept unmodified.

As last note, our contracts could have been lightened by attaching the
\SPARK{Is_Valid} function as a type invariant to
\SPARK{Step_Function_t} data structure. This is not currently
supported in \newspark but will in future releases.

\subsection{Formal Verification Results}

The first goal of this experiment was to check if \newspark was
expressive enough to describe the objects of the requirements:
requirement text, transition tables, breaking curve equations, \etc
Overall, we were quite satisfied, as we were able to express most of
the requirements in a formal way. The very expressive data structures
of \newspark (records, arrays, enumerations, \etc) were very helpful
compared to other specification languages like B~Method\cite{b-book}
(lacking usable record structure) or Frama-C's ACSL\cite{acsl}
(lacking record with variant part or easy limited range data type
definition). We found that it lead to quite readable specifications.

The second goal was to evaluate the automatic proving capabilities of \newspark
on some parts of the specification.

On step functions, we were able to prove all subprogram
contracts except the one of \SPARK{Restrictive_Merge}. In this procedure, the
postcondition and the loop invariant could not be automatically
proved by \altergo. The main reason is that the proof context is too
big and \altergo gets lost in all possible quantification
instantiations. We have checked that some parts of the loop invariant
could be automatically proved if the proof context was manually pruned
of irrelevant hypotheses. Moreover, we compiled and tested the
contracts and formal annotations, making us more confident about their
correctness.

We should also notice that in all those subprograms, we needed about
the same number of lines of formal annotations than of executable
code.

\subsection{Lessons Learned}

As said previously, we were rather pleased by the expression
capabilities of \newspark, making specification and code writing
rather easy and, more important, clearer for the reader. The ability
to define new data types for specific ranges and incompatible with
other types is crucial in this regard.

Another important finding is that the code should be written with
proof in mind. For example, in one of the step function procedure, we
wrote a loop with an early exit. However \altergo was unable to prove
this loop because it lacks induction reasoning, even if all elements
of the induction could be easily pointed out to the prover. We had to
change the code with a loop without early exit, the proof then
becoming trivial.

A third finding is that contracts that can be automatically proved are
not the most natural contracts, \ie contracts a reviewer would
understand more easily. For example, for \SPARK{Restrictive_Merge}
procedure, we would have preferred to write that the resulting function
is the minimum of both input functions for all possible input
values. It would have been impossible to prove this contract. Other
formal approaches with formal refinement like B~Method would probably
be able to formalize such contract, at the expense of manual proofs.

Our last finding, not entirely surprising, is that writing the correct
invariant for a complex loop is not an easy task, as we experimented
it for \SPARK{Restrictive_Merge} procedure. It can necessitate several
hours of work of skilled people, trained in the proof environment and
the reaction of the automatic prover. In such case, the ability to
compile and test the loop invariant is very useful to help ``debug''
the invariant.

Overall, we think that the programmer should be trained to exploit the
feedback provided by the proof environment, much like a programmer
exploits feedback of debuggers and tests to debug his/her program.
Moreover, as complete code proof can become very costly, a proof
methodology must be defined to avoid spending to much time on proofs
that would be broken afterward due to other code changes.

\section{Flight Control and Vehicle Management in Space}

\subsection{Description of the Software}

A typical space applicative flight program is made up of two parts, which we
both considered in our case study:

\begin{itemize}
\item Flight control or more generally numerical command and control algorithm
\item Mission and Vehicle Management
\end{itemize}

\subsubsection{Numerical Command and Control Algorithms}

Numerical command and control algorithms take as inputs floating point values,
perform some numerical computations (with the classical basic mathematical
operators such as additions, subtractions, multiplications, divisions, absolute
values, trigonometry or operations on vectors and arrays, \etc) and return
floating point results. Such algorithms have generally a retroaction loop, \ie
internal states.

It is generally not possible to define interesting functional contracts for
such code. Indeed, the functional contract of the equation:

\begin{lstlisting}
   X := A * Y + Cos (Z)
\end{lstlisting}

\noindent
is just itself (\ie it is not possible to specify in a more abstract way this
equation). Then, instead of defining functional contracts, the objective on
this kind of software is the proof of absence of run-time errors (such as
division by zero) and the correctness of variable ranges (such as, for
instance, a velocity shall always be between 0 and 25 km/s).

\newspark has been first experimented on a solar wing management software (for a spacecraft such as the ATV / Automated Transfer Vehicle).
This piece of code uses a mathematical library which implementation is not in \newspark, implying that it could not be formally proved, but only tested. However, the interface of this mathematical library is in \newspark. The contracts defined on the mathematical library can then be used to prove the application code.

\paragraph{Example of contract in the mathematical library:}

\begin{lstlisting}
   function Sin32 (X : T_Float32)
   return T_Float32
   with
     Pre  => (X >= - C_2Pi32) and then
             (X <=   C_2Pi32),
     Post => (Sin32'Result >= -1.0) and then
             (Sin32'Result <=  1.0);
\end{lstlisting}

\subsubsection{Mission and Vehicle Management}

The Mission and Vehicle Management of a spacecraft is described by an ECSS (European Cooperation for Space Standardization) standard:

\begin{center}
{\bf ECSS-E-ST-70-01C Space engineering - Spacecraft on-board control procedures}
\end{center}

This standard defines the general principles of a On Board Control Procedure
(OBCP). An OBCP is in practice represented by a simplified programming language
interpreted onboard the spacecraft. This interpreter is generally at the
highest level of criticality of the spacecraft. The implementation of this
interpreter in \newspark is table driven and relies greatly on generic packages
and discriminants. The generic packages allow an easy customization of the
code, while use of discriminants ensures a strong typing of the code, even in
case of heterogeneous communication between components of the system.

%% \begin{itemize}
%% \item Generic packages


%% 	\begin{lstlisting}
%%      generic
%%         -- the list of events
%%         type T_Event_Id is (<>);

%%      package Mvm.Events is
%% 	\end{lstlisting}

%% \item Discriminant

%% 	The discriminants

%% 	\begin{lstlisting}
%%    type T_Monitoring is (No_Window, Window);

%%    type T_Event_Status
%%      (Monitoring : T_Monitoring := No_Window)
%%    is record
%%      case Monitoring is
%%      when No_Window => null;
%%      when Window    =>
%%        Start_Window : T_Float32;
%%        End_Window   : T_Float32;
%%      end case;
%%    end record;
%% 	\end{lstlisting}
%% \end{itemize}

\subsection{Formalization of Properties}

The contracts defined on algorithmic code are mainly related to the ranges of variables.

\paragraph{Example of contract in algorithmic code:}

\begin{lstlisting}
   subtype T_Angle_180 is T_Float32
   range -180 .. 180;

   function Normalise (X : in T_Float32)
   return T_Angle_180
   with Pre => (X >= -720.0) and
               (X <=  720.0);
\end{lstlisting}

The contracts defined on mission and vehicle management code have (among others) the following objectives:

\begin{itemize}
\item Ensuring the permanent consistency of the software tables
\item Ensuring the permanent consistency between the Mission and Vehicle Management function and the other functionalities (such as, for instance, the solar wing management)
\item Ensuring the respect of some functional properties such as the mutual exclusion of execution of some automated procedures
\item Ensuring the absence of run-time errors
\end{itemize}

\paragraph{Example of contract in mission and vehicle management code:}

\begin{lstlisting}
   procedure Reset_Event
     (Event_Id :        T_Event_Id;
      Events   : in out T_Events)
   with
     Post =>
       Get_Status (Event_Id, Events) = Inactive)
       and then
       (for all Other_Id in T_Event_Id =>
         (if Other_Id /= Event_Id then
           Get_Status (Other_Id, Events) =
           Get_Status (Other_Id, Events'Old))));
\end{lstlisting}

\subsection{Formal Verification Results}

All the contracts have been checked by dynamic testing. This phase is quite classical, except for the fact that the testing includes the preconditions and the postconditions defined in the software. Then, \gnatprove has been applied.

\subsubsection{Subprograms not in \newspark}

The origins of subprograms not yet in \newspark are mainly the following:

\begin{itemize}
\item unchecked conversion

The unchecked conversions are used in a library allowing reading external inputs.
All the concerned subprograms are very small and shall be validated by intensive testing because there are out of the perimeter of \hilite and of \newspark.
\item tagged type

Tagged types are related to Object Oriented Programming. The analysis of Object Oriented software is foreseen but has not yet been implemented.
\item access

Accesses are used in the software to store objects in a table.
This kind of design can not be proved by \gnatprove.
\end{itemize}

\subsubsection{Analysis of the non proved VCs}

Some algorithmic functions are not completely known by \gnatprove.

\paragraph{Example:}

\begin{lstlisting}
   function Arctan (X : T_Float32)
   return T_Float32
   with
     Post =>
       (Arctan'Result >= -C_Halfpi32) and then
       (Arctan'Result <=  C_Halfpi32);

   function Arctan (X : T_Float32)
   return T_Float32
   is (Num32.Arctan (X));
\end{lstlisting}

The postcondition is not proved by \gnatprove.
The exact behaviour of algorithmic functions depending of the implementation, this behaviour is acceptable.
Algorithmic functions and their contracts are preferably validated by intensive testing.

\gnatprove has some difficulties to take into account rounding.
In the following example, the second assertion is not proved.

\paragraph{Example:}

\begin{lstlisting}
   function Round_Closest (X : T_Float32)
   return T_Float32
   is (T_Float32'Rounding (X));

   pragma Assert ((Y >= -11160.002) and then
                  (Y <=  11160.002));
   Round_Y := Ml.Round_Closest (Y);
   pragma Assert ((Round_Y >= -11160.0) and then
                  (Round_Y <=  11160.0));
\end{lstlisting}

\gnatprove currently does not prove non linear equation.
In the following example, the last assertion is not proved.

\paragraph{Example:}

\begin{lstlisting}
   pragma Assert (X >= 0.0 and then X <= 180.0);
   pragma Assert (Y >= -180.0 and then Y <= 0.0);
   pragma Assert (Z >= 0.0 and then Z <= 1.0);
   pragma Assert (X + Y >= 0.0);
   Result := X + Y * Z;
   pragma Assert (Result >= 0.0 and then
                  Result <= 360.0);
\end{lstlisting}

\gnatprove is not yet able to verify the index of an array which dimension is defined by a type descriminant

\paragraph{Example:}

\begin{lstlisting}
   subtype R is Integer range 1 .. 100;
   type T_Array is array (R range <>) of Boolean;

   type T_Record (L : R) is
      record
         A : T_Array (1 .. L);
      end record;

   function G (X : T_Record) return Boolean is
     (for all I in X.A'Range => X.A (I));
\end{lstlisting}

In function ``G'', the index check ``X.A (I)'' is not proved even if ``I'' is defined in the range of ``X.A''
An improvement of \gnatprove is in progress in order to deal with such case.

The remaining non proved VCs are due to too complex subprograms.
These subprograms shall be split in several smaller subprograms to be proved.

\subsection{Lessons Learned}

Our initial objectives were that \newspark should fulfill the following requirements:

\begin{itemize}
\item Capability to formalize the test cases

Current assessment: Very good
\item Verification that all test procedures cover all the test cases

Current assessment: Not implemented. This implies that the formalization of the test cases is today not really interesting
\item Support to the verification / validation of the absence of run-time errors

Current assessment: Very good
\item Support to the verification / validation of functional properties

Current assessment: Good. In order to be efficient, it requires some works (definition of contracts, definition of assertions, spliting of complex subprograms in smaller ones)
\item Verification of the correctness of the access of all global variables

Current assessment: Not yet implemented
\item Verification of the absence of out of range values

Current assessment: Very good. As for functional properties, it requires some manual works
\item Internal consistency of software unit. The \hilite tools shall help ensuring that each aggregate satisfies the requirements of the software item. In other words, each caller of a subprogram shall ensures that the pre-condition of the callee is respected and that the post-condition of the callee is compatible with the caller

Current assessment: Very good
\item Capability to assess the commandability and observability. In other words, the \hilite tools shall help detecting dead code

Current assessment: No
\item Help proving that numerical protection mechanisms are correctly implemented

Current assessment: Very good
\item Prove the correctness of a generic code in a specific context

Current assessment: Very good
\end{itemize}

\section{Biometric Access to a Secure Enclave}

% Tokeneer case study

\subsection{Description of the Software}

\subsubsection{Overview}

Tokeneer is a highly secure biometric software system that was
originally developed by Altran in \oldspark. The system provides
protection to secure information held on a network of workstations
situated in a physically secure enclave. The Tokeneer project was
commissioned by the US National Security Agency (NSA) to demonstrate
the feasibility of developing systems to the level of rigour required
by the higher assurance levels of the Common Criteria. The original
development artefacts, including all source code, are publicly
available. For more details see
\url{www.adacore.com/sparkpro/tokeneer}.

During this study, the source code for Tokeneer has been translated
into \newspark. The core system now consists of approximately
\emph{10,000} lines of \newspark code (declarations and executable
lines, excluding blank lines, comments and SPARK annotations). There
are also approximately 3,700 lines of supporting code written in Ada
which mimicked the drivers to peripherals connected to the core
system.

\subsubsection{Converting \oldspark to \newspark}

For the majority of the code, translating \oldspark to \newspark was
very straight forward since most of the original annotations map
directly to the new ones. This section will focus on the more
interesting occasions where the conversion was non-trivial.

Proof functions were replaced with ghost functions. In \oldspark, the
behaviour of a proof function was often defined in a user rule. In
\newspark, this behaviour is described in the body of the ghost
function.

The \newspark Global aspect has an additional mode
(\emph{Proof\_In}). All global variables of mode \emph{in} that were
used exclusively inside annotations had to be turned into
\emph{Proof\_In} global variables in \newspark.

\newspark supports a bigger subset of Ada than \oldspark. As a
consequence, pieces of code that could previously not get analysed and
were hidden, were now analysable. Two occasions that fall under this
category and were frequently encountered involved string concatenation
and array slices.

In-type checks are added automatically by the new toolset. This
simplifies both the work of the programmer and the code
itself. Consider the following two code segments:

\begin{lstlisting}[caption=\oldspark]
for I in LogFileIndexT loop
--# assert
--# I in LogFileIndexT and
--# UsedLogFiles.Length in LogFileCountT and
--# UsedLogFiles.Length < I and
--# UsedLogFiles.LastI in LogFileIndexT and
--# (UsedLogFiles.Length > 0 ->
--#   UsedLogFiles.LastI < I) and
--# (for all N in LogFileIndexT =>
--#   (LogFileEntries(N) in FileEntryCountT)) and
--# (for all N in LogFileIndexT =>
--#   (UsedLogFiles.List(N) in LogFileIndexT));
  if LogFilesStatus(I) = Used then
    if UsedLogFiles.Length = 0 then
       -- easy case list currently empty
       UsedLogFiles.Head := LogFileIndexT'First;
       UsedLogFiles.LastI := LogFileIndexT'First;
       UsedLogFiles.Length := 1;
       UsedLogFiles.List(UsedLogFiles.Head) := I;
    else
      for J in LogFileIndexT
        range 1 .. UsedLogFiles.LastI
      loop
      --# assert
      --# I in LogFileIndexT and
      --# J in LogFileIndexT and
      --# J <= UsedLogFiles.LastI and
      --# UsedLogFiles.LastI in LogFileIndexT and
      --# UsedLogFiles.Length in LogFileCountT and
      --# UsedLogFiles.Length > 0 and
      --# UsedLogFiles.Length < I and
      --# (UsedLogFiles.Length > 0 ->
      --#   UsedLogFiles.LastI < I) and
      --# (for all N in LogFileIndexT =>
      --#   (LogFileEntries(N) in
      --#    FileEntryCountT)) and
      --# (for all N in LogFileIndexT =>
      --#   (UsedLogFiles.List(N) in
      --#    LogFileIndexT));
        ...
\end{lstlisting}

\begin{lstlisting}[caption=\newspark]
for I in LogFileIndexT loop
  if LogFilesStatus(I) = Used then
    if UsedLogFiles.Length = 0 then
      -- easy case list currently empty
      UsedLogFiles.Head := LogFileIndexT'First;
      UsedLogFiles.LastI := LogFileIndexT'First;
      UsedLogFiles.Length := 1;
      UsedLogFiles.List(UsedLogFiles.Head) := I;
    else
      for J in LogFileIndexT
        range 1..UsedLogFiles.LastI
      loop
        ...
\end{lstlisting}
Even though these segments are equivalent the difference in lines of
code is apparent. However, at this point, it should be pointed out
that since the development of the Tokeneer project the \oldspark tools
have greatly evolved and some of the original annotations might
actually now be redundant.

\subsection{Formalization of Properties}

The contracts defined in the Tokeneer source code consist of
information flow contracts (expressed as Depends aspects) and
functional behavioural contracts (expressed as Pre and Post aspects).

The Depends aspects facilitate flow analysis of the code. Flow
analysis detects improper initialization, identifies ineffective
assignments and ensures secure flow of information.

The Pre and Post aspects enable the prover to show that the code is
free from run time exceptions, such as buffer overflow or
divide-by-zero and assist in proving that key security properties are
guaranteed by the implementation.

\subsection{Formal Verification Results}

The original source code of Tokeneer was proven to be free of run time
exceptions and some key security properties were proven to hold but
full functional proof was not performed on the entirety of the code.

\subsubsection{Fully specifying and proving a package}

In order to measure the expressiveness and proving power of the
\newspark tools, a specific package, on which functional behaviour
proof had not been attempted, was selected and then fully augmented
with functional behaviour annotations. Package \emph{admin}, which is
the package that was augmented, contains the state of the
administrator of the system and a set of operations that
administrators can perform.

To illustrate how functional behaviour contracts were added, we will
consider subroutine \emph{OpIsAvailable} of package admin. An
administrator can be either a ``UserOnly'', a ``Guard'', an
``AuditManager'' or a ``SecurityOfficer''. Each type of administrator
has a set of predefined operations that it is allowed to
perform. Function \emph{OpIsAvailable} takes as input an administrator
and a string that is read from the keyboard and determines if this
string corresponds to an operation available to the administrator. If
the operation is indeed available, then this operation is returned,
otherwise \emph{NullOp} is returned.

\begin{lstlisting}[caption=\oldspark]
function OpIsAvailable(TheAdmin : T;
                       KeyedOp  : Keyboard.DataT)
                      return OpAndNullT;
--# pre IsPresent(TheAdmin);
--# return R => (R /= NullOp ->
--#                (R = OverrideLock <->
--#                 prf_rolePresent(TheAdmin) =
--#                   PrivTypes.Guard));
\end{lstlisting}
Here, the \oldspark postcondition serves only as a test case. It
ensures that if a non null operation was returned and if that
operation was \emph{OverrideLock} then the administrator was of type
\emph{Guard}. This annotation is incomplete since it does not specify
any other kind of valid combinations of administrator types and their
corresponding operations (\Eg the administrator being of type
\emph{SecurityOfficer} and the operation being \emph{UpdateConfigData}
or \emph{ShutdownOp}) or under which circumstances \emph{NullOp}
should be returned.

\begin{lstlisting}[caption=\newspark]
function OpIsAvailable(TheAdmin : T;
                       KeyedOp  : Keyboard.DataT)
                      return OpAndNullT
  with Pre  => IsPresent(TheAdmin),
       Post => (for some Op in Opt =>
                          Str_Comp(KeyedOp, Op)
                  and AllowedOp(TheAdmin, Op)
                  and OpIsAvailable'Result = Op)
               xor OpIsAvailable'Result = NullOp;
\end{lstlisting}
This \newspark \emph{Post} aspect states that we have two mutually
exclusive cases. If there exists some operation in \emph{Opt} that
matches the string read from the keyboard and this operation is
allowed for the current administrator then the result of function
\emph{OpIsAvailable} is this operation, otherwise, the result is
\emph{NullOp}.

The \newspark tools discharge all 24 verification conditions
associated with the augmented \emph{admin} package in a matter of
seconds. The \oldspark tools also prove all properties associated with
the non-augmented \emph{admin} package but a total of 12 user rules had
to be provided.

\subsection{Lessons Learned}

-Conversion of the majority of the code is straight forward.

-Flaws in specifications can be detected (see HiLite report).

-Low level Z specs look very much like the \newspark code.

-Explain why we have packages where not everything is proven and why we
have some flow warnings (\oldspark was getting away with these because
it was using rlu/prv files and the flow warnings were accepted).

-Investigate how executable semantics affect the way code has to be
written and what advantages/disadvantages they introduce

-Discover the percentage of the code that can be automatically proven;
and the combination of options/provers that achieve the best results
for this specific test case.

-Provide feedback and suggest features or changes that could improve
future versions of the \newspark toolset.

\section{Common Findings and Dissimilarities}

We will discuss the common findings between the three case studies,
denoting instrinsic properties of \newspark, and the dissimilarities,
which may be explained by differences in domains, backgrounds,
processes, \etc

\section{Conclusion}

We will summarize the three case studies, and present the planned future
evolutions of \newspark.

\section{Acknowledgment}

Part of this work was funded by the ``Direction Générale de la
compétitivité, de l'industrie et des services'' (DGCIS) (Grant
No. 112930309) in the context of the ITEA2 project \openetcs.

\bibliographystyle{plain}
\bibliography{erts_2014}

\end{document}


% LocalWords:  openETCS ETCS UNISIG Centre SRS
